[
["index.html", "Lecture Notes for Causality in Machine Learning 1 About", " Lecture Notes for Causality in Machine Learning Robert Ness 6/7/2019 1 About Northeastern University Khoury College CS 7290: Summer 2019 These are lecture notes for an ongoing course on causal inference and modeling in machine learning, taught by Dr. Robert O. Ness. These notes are a work in progress, created as the course progresses. They are created by the instructor, the course TA’s Kaushal Paneri and Sicheng Hao, and the Summer 2019 students of this course. "],
["causal-inference-overview-and-course-goals.html", "2 Causal inference overview and course goals 2.1 Course thesis 2.2 Causal modeling as an extension of generative modeling 2.3 Case studies 2.4 Don’t worry about being wrong", " 2 Causal inference overview and course goals Lecture notes prepared by Kaushal Paneiri 2.1 Course thesis The goal of this course is to learn techniques of causal inference in a way that builds on students’ existing intuition and experience with generative machine learning. Moreover, we will do so using frameworks from generative machine learning, include tools for building deep neural networks. Further, when reasoning about causal inference problems, we will bias the case studies to those seen in professional environments where data scientists and machine learning engineers build and manage in-product machine learning models. 2.1.1 Causal modeling as generative ML More specifically, this course focuses on machine learning in the following two ways. We will place causal inference firmly on a foundation of model-based generative machine learning. Our goal is to build machine learning systems that think in causal terms, such as confounding, interventions, and counterfactuals. If you peruse the causal inference literature, you will see examples similar to the A/B test example from epidemiology, econometrics, and clinical trials. This course focuses on the kinds of cases data scientists experience in professional settings, particularly in the tech industry. The focus of the tech industry is shifting towards problems where A/B test becomes more complicated and not feasible. We will cover some advanced techniques like how to deal with confounding, how to build up an online and offline learning and policy evaluation for Markov decision processes that automates testing. We will also cover a little bit of relevant literature from Game theory (Auction models) and Reinforcement learning (policy evaluation and improvement) at the end. 2.1.2 What is left out Causal inference spans many other concepts, and we won’t be able to cover all of them. Though the concepts below are essential, they are out of scope for this course. Causal discovery Causal inference with regression models and various canonical SCM models Doubly-robust estimation Interference due to network effects (important in social network tech companies like Facebook or Twitter) heterogeneous treatment effects deep architectures for causal effect inference causal time series models algorithmic information theory approaches to causal inference 2.1.3 Examples of problems in causal inference To properly contextualize our motivation, we start by understanding how causal inference developed as a field across domains, including economics, biology, social science, computer science, anthropology, epidemiology, statistics. 2.1.3.1 Estimation of causal effects The problem of finding causal effects is the primary motivation of researchers in these domains. For example, in the late 80s and 90s, doctors used to prescribe Hormonal replacement therapy to old women. Experts believed that at the lower age, women have a lower risk of heart disease than men do, but as they age, after menopause, their estrogen level decline. However, after doing a large randomized trial, where women were selected randomly and given either a placebo or estrogen, the results showed that taking estrogen increases the chance of getting heart disease. Causal inference techniques are essential because the stakes are quite high. 2.1.3.2 Counterfactual reasoning with statistics Counterfactual reasoning means observing reality, and then imagining how reality would have unfolded differently had some causal factor been different. For example, “had I broken up with my girlfriend sooner, I would be much happier today” or “had I studied harder for my SATs, I would be in a much better school today.” An example of a question from an experimental context would be “This subject took the drug, and their condition improved. What is the difference between this amount improvement and the improvement they would have seen had they taken placebo?” Counterfactual reasoning is fundamental to how we as humans reason. However, statistical methods are generally not equipped to enable this type of logic. Your counterfactual reasoning process works with data both from actual and hypothetical realities, while your statistical procedure only has access to data from actual reality. The same is true of cutting-edge machine learning. Intuition tells us that if we trained the most powerful deep learning methods to provide us with relationship advice based on our romantic successes and failers, something would be lacking in that advice since those counterfactual outcomes are missing from the training data. 2.1.3.3 The challenge of running experiments In traditional statistics, randomized experiments are the gold standard for discovering the causal effect. An example of a randomized experiment is an A/B test on a new feature in an app. We randomly assign users to two groups and let one group use the feature while the other is presented with a control comparison. We then observe some key outcome, such as conversions. As we will learn, the randomization enables us to conclude the difference between the two groups is the causal effect of the feature on the conversions, because it isolates that effect from other unknown factors that are also affecting the conversions. However, in many instances, setting up this randomization might be complicated. What if users object to not getting a feature that other users are enjoying? What if the experience of the feature and probability of conversion both depend on user-related factors, such that it is unclear how to do proper randomization? What if some users object to being the subjects of an experiment? What if it is unethical to do the experiment? 2.2 Causal modeling as an extension of generative modeling 2.2.1 Generative vs. discriminative Models Let’s focus on supervised learning for a moment. Given a target variable Y and predictor(s) X, a discriminative model learns as much about \\(P(Y| X)\\) as it needs to an optimal prediction. In contrast, generative models try to fully learn the joint distribution \\(P(X, Y)\\) underlying the data. We will discuss this more in later lectures. In simple words, these models can generate data that looks like real data. We focus on generative models because they allow us to build our theories about the data-generating process into the model itself. We will see that we naturally think of this process in causal terms. 2.2.2 Model-based ML and learning to think about the data-generating process The following is the typical checklist in training a statistical machine learning model. Split the data into training and test sets. Choose a few models from literally thousands of algorithm choices. Typically this choice is limited algorithms you are familiar with, are in vogue, or happen to be implemented in the software you have available. Manipulate the data until it fits your algorithm inputs and outputs. Evaluate the model on test data, compare to other models ( optional) If data doesn’t fit the algorithms modeling assumptions, manipulate the data until it does. (optional) If using a deep learning algorithm, search for hyperparameter settings that further optimize prediction. This process works well. However, in this workflow, the data scientist’s time is devoted to manipulating data, hyperparameters, and often, the problem definition itself until things work. An alternative approach is to think hard about the process that generated the data, and then explicitly building your assumptions about that process into a bespoke solution tailored to each new problem. This approach is model-based machine learning. Proponents like this approach because with an excellent model-based machine learning framework, you can create a bespoke solution to pretty much any problem, and don’t need to learn a vast number of machine learning algorithms and techniques. Most interestingly, with the model-based machine learning approach the data scientists shifts her time from transforming her problem to fit some standard algorithm, to thinking hard about the process that generated the problem data, and then building those assumptions into the designing of the algorithm. We’ll see in this class that when we think about the data-generating process, we think about it causally, meaning it has some ordering of causes and effects. In this class, we formalize this intuition by apply causal inference theory to model-based machine learning. 2.2.3 Note on reinforcement learning As reinforcement learning gains in popularity amongst machine learning researchers and practitioners, many may have encountered the term “model-based” for the first time in a reinforcement learning (RL) context. Model-based RL is indeed an example of model-based machine learning. Model-free RL. The agent has no model of the generating process of the data it perceives in the environment; i.e., how states and actions lead to new states and rewards. It can only learn in a Pavlovian sense, relying solely upon experience. Mode-based RL: The agent has a model of the generating process of the data it perceives in the environment. This model enables the agent to make use not only of experience but also of model-based predictions of the consequences of particular actions it has less experience performing. 2.3 Case studies 2.3.1 From linear regression to model-based machine learning The standard Gaussian linear regression model is represented as follows: \\[\\begin{align} \\epsilon &amp;\\sim \\mathcal{N}(0,1)\\\\ Y &amp;= \\beta X + \\alpha + \\epsilon \\end{align}\\] When we read this model specification, it is natural to think of it as predictors \\(X\\) generating target variable \\(Y\\). Indeed, the term generates feels a lot like causes here. Usually, we moderate this feeling by remembering that linear regression models only correlation, and we could just have easily regressed \\(X\\) on \\(Y\\). In this course, we learn how to formalize that feeling. We can turn this model into a generative model by placing a marginal distribution on X. \\[\\begin{align} \\epsilon &amp;\\sim \\mathcal{N}(0,1)\\nonumber\\\\ X &amp;\\sim P_X\\nonumber \\\\ Y &amp;= \\beta X + \\alpha + \\epsilon \\nonumber \\end{align}\\] At this point, we are already telling a data generating story where \\(Y\\) comes from \\(X\\) and \\(\\epsilon\\). Now let’s expand on that story. Suppose we observe that \\(Y\\) is measured from some instrument, and we suppose that this instrument is adding technical noise to \\(Y\\). Now the regression model becomes a noise model. \\[\\begin{align} \\epsilon &amp;\\sim \\mathcal{N}(0,1)\\nonumber\\\\ X &amp;\\sim P_X\\nonumber \\\\ Z &amp;\\sim P_Z \\nonumber\\\\ Y &amp;= \\beta X + \\alpha + \\epsilon + Z \\nonumber \\end{align}\\] 2.3.2 Binary classifier The logistic regression model has the form: \\[\\mathbb{E}[Y] = \\texttt{logit}(\\beta X + \\alpha)\\] If we read this formula, it reads as Y comes from X. Of course that is not true, this model doesn’t care whether Y comes from X or vice versa, in other words, it doesn’t care how Y is generated, it merely wants to model \\(P(Y=1|X)\\) In contrast, a naive Bayes classifier models P(X, Y) as P(X|Y)P(Y). P(X|Y) and P(Y) are estimated from the data, and then we use Bayes rule to find P(Y|X) and predict Y given X. P(X|Y)P(Y) is a representation of the data generating process that reads as “there is some unobserved Y, and then we observe X which came from Y.” There is nothing that forces us to apply naive Bayes only in problems where the generation of the prediction target generation of the features. Yet, this is precisely the kind of problem where this approach tends to get applied, such as spam detection. I argue that it P(X|Y)P(Y) aligns with a causal intuition that X comes from Y, and we avoid the inner cringe that comes from using naive Bayes when we suspect that Y comes from X. Causal modeling gives us a language to formalize this intuition. 2.3.3 Gaussian mixture model The naive Bayes classifier is an example of a latent variable model. Latent variable models come with a pre-packaged data-generating story. Another example is the Gaussian mixture model. Image Let’s recall the intuition with simple GMM with two Gaussians. We observe the data and realize that it is coming from two Gaussians with means \\(\\mu_1\\), and \\(\\mu_2\\). Let \\(Z_i\\) be a binary variable that says \\(X_i\\) belongs to one of these distributions. The data generating process is: 1. Some probabilistic process generated \\(\\mu\\). 2. Some Dirichlet distribution generated \\(\\theta\\). 3. Then for each \\(i\\) in some range 1. a discrete distribution with parameter \\(\\theta\\) generated \\(Z_i\\). 2. \\(Z_i\\) picks a \\(\\mu\\) that generates \\(X_i\\) from a Gaussian with mean \\(\\mu\\). We can represent this data generative process in code quite easily. The following pseudocode generalizes from two to k mixture components. ``` function (alphas, sigma, scale): theta = random_dirichlet(alphas)) for each mixture component k: mu[k] = random_normal(0, sigma)) for each data_point: Z[i] = random_discrete(theta)) X[i] = random_normal(mu[Z[i]], scale)) ``` Now inferring the mixture components given data using this code requires an inference algorithm. Model-based machine learning frameworks generally let you code up the model just as above and then provide implementations of algorithms that can provide inference on the model. In the next lecture, we will cover the basics of two frameworks for model-based machine learning that implement inference algorithms. The GMM and other latent variable models like the hidden Markov model, mixed membership models like LDA, linear factor models, provide an off-the-shelf data-generating story that is straightforward to cast into code. However, just as we turned regression into a noise model, we can adjust the model and code to create a bespoke solution to a unique problem. 2.3.4 Deep generative models Deep generative models are generative models that use deep neural network architectures. Examples include variational autoencoders and generative adversarial networks. Rather than make the data generation story explicit, their basic implementation compresses all generative process into a latent encoding. But nothing is forcing them to do so. In this course, we will see examples of deep generative models where we model the critical components of the data generating process explicitly, and let the latent encoding handle nuisance variables that we don’t care about. 2.4 Don’t worry about being wrong Deep learning works well because they essentially infer the optimal circuit between a given input signal and output channels. In contrast, when you reason about and represent as code the data generating process, you are inferring a program given program inputs and outputs. In machine learning, the task of automatically inferring a program is called program induction, and it is much harder than inferring a circuit. Indeed, that is an ill-specified problem, because there are numerous programs we could write to generate the same data. Algorithmic information theory tells us that the task of finding the shortest program that produces an output from a given input is an NP-hard problem. So if program induction is hard for computers, it should surprise us that it is challenging for humans too. In practice, we make use of domain knowledge and other constraints. For example, an economist building a price model might incorporate in their understanding of supply and demand. A computational biologist may use extensive databases of verified relationships between genes in modeling. Finally, we can still validate the model on the data using goodness-of-fit or predictive performance statistics. We can also use standard techniques for handling uncertainty in our models, such as ensemble methods. "],
["tutorial-probabilistic-modeling-with-bayesian-networks-and-bnlearn.html", "3 Tutorial probabilistic modeling with Bayesian networks and bnlearn 3.1 Installing bnlearn 3.2 Understanding the directed acyclic graph representation 3.3 Estimating parameters of conditional probability tables 3.4 Conditional independence in Bayesian networks 3.5 Plotting Conditional Probability Distributions", " 3 Tutorial probabilistic modeling with Bayesian networks and bnlearn Lecture notes by Sara Taheri 3.1 Installing bnlearn Open RStudio and in console type: install.packages(bnlearn) install.packages(Rgraphviz) If you experience problems installing Rgraphviz, try the following script: if (!requireNamespace(&quot;BiocManager&quot;, quietly = TRUE)) install.packages(&quot;BiocManager&quot;) BiocManager::install(&quot;Rgraphviz&quot;) Then type “bnlearn” in the window that appears and click on the install button. Do the same thing for the other package. 3.2 Understanding the directed acyclic graph representation In this part, we introduce survey data set and show how we can visualize it with bnlearn package. 3.2.1 The survey data dataset Survey data is a data set that contains information about usage of different transportation systems with a focus on cars and trains for different social groups. It includes these factors: Age (A): It is recorded as young (young) for individuals below 30 years, adult (adult) for individuals between 30 and 60 years old, and old (old) for people older than 60. Sex (S): The biological sex of individual, recorded as male (M) or female (F). Education (E): The highest level of education or training completed by the individual, recorded either high school (high) or university degree (uni). Occupation (O): It is recorded as an employee (emp) or a self employed (self) worker. Residence (R): The size of the city the individual lives in, recorded as small (small) or big (big). Travel (T): The means of transport favoured by the individual, recorded as car (car), train (train) or other (other) Travel is the target of the survey, the quantity of interest whose behaviour is under investigation. 3.2.2 Visualizing a Bayesian network We can represent the relationships between the variables in the survey data by a directed graph where each node correspond to a variable in data and each edge represents conditional dependencies between pairs of variables. In bnlearn, we can graphically represent the relationships between variables in survey data like this: # empty graph library(bnlearn) ## Warning: package &#39;bnlearn&#39; was built under R version 3.5.2 ## ## Attaching package: &#39;bnlearn&#39; ## The following objects are masked from &#39;package:BiocGenerics&#39;: ## ## path, score ## The following object is masked from &#39;package:stats&#39;: ## ## sigma dag &lt;- empty.graph(nodes = c(&quot;A&quot;,&quot;S&quot;,&quot;E&quot;,&quot;O&quot;,&quot;R&quot;,&quot;T&quot;)) arc.set &lt;- matrix(c(&quot;A&quot;, &quot;E&quot;, &quot;S&quot;, &quot;E&quot;, &quot;E&quot;, &quot;O&quot;, &quot;E&quot;, &quot;R&quot;, &quot;O&quot;, &quot;T&quot;, &quot;R&quot;, &quot;T&quot;), byrow = TRUE, ncol = 2, dimnames = list(NULL, c(&quot;from&quot;, &quot;to&quot;))) arcs(dag) &lt;- arc.set nodes(dag) ## [1] &quot;A&quot; &quot;S&quot; &quot;E&quot; &quot;O&quot; &quot;R&quot; &quot;T&quot; arcs(dag) ## from to ## [1,] &quot;A&quot; &quot;E&quot; ## [2,] &quot;S&quot; &quot;E&quot; ## [3,] &quot;E&quot; &quot;O&quot; ## [4,] &quot;E&quot; &quot;R&quot; ## [5,] &quot;O&quot; &quot;T&quot; ## [6,] &quot;R&quot; &quot;T&quot; 3.2.3 Plotting the DAG In this section we discuss the ways that we can visually demonstrate Bayesian networks. You can either use the simple plot function or use the graphviz.plot function from Rgraphviz package. # plot dag with plot function plot(dag) # plot dag with graphviz.plot function. Default layout is dot graphviz.plot(dag, layout = &quot;dot&quot;) # plot dag with graphviz.plot function. change layout to &quot;fdp&quot; graphviz.plot(dag, layout = &quot;fdp&quot;) # plot dag with graphviz.plot function. change layout to &quot;circo&quot; graphviz.plot(dag, layout = &quot;circo&quot;) 3.2.4 Highlighting specific nodes If you want to change the color of the nodes or the edges of your graph, you can do this easily by adding a highlight input to the graphviz.plot function. Let’s assume that we want to change the color of all the nodes and edges of our dag to blue. hlight &lt;- list(nodes = nodes(dag), arcs = arcs(dag), col = &quot;blue&quot;, textCol = &quot;blue&quot;) pp &lt;- graphviz.plot(dag, highlight = hlight) The look of the arcs can be customised as follows using the edgeRenderInfo function from Rgraphviz. edgeRenderInfo(pp) &lt;- list(col = c(&quot;S~E&quot; = &quot;black&quot;, &quot;E~R&quot; = &quot;black&quot;), lwd = c(&quot;S~E&quot; = 3, &quot;E~R&quot; = 3)) Attributes being modified (i.e., col for the colour and lwd for the line width) are specified again as the elements of a list. For each attribute, we specify a list containing the arcs we want to modify and the value to use for each of them. Arcs are identified by labels of the form parent∼child, e.g., S → E is S~E. Similarly, we can highlight nodes with nodeRenderInfo. We set their colour and the colour of the node labels to black and their background to grey. nodeRenderInfo(pp) &lt;- list(col = c(&quot;S&quot; = &quot;black&quot;, &quot;E&quot; = &quot;black&quot;, &quot;R&quot; = &quot;black&quot;), fill = c(&quot;E&quot; = &quot;grey&quot;)) Once we have made all the desired modifications, we can plot the DAG again with the renderGraph function from Rgraphviz. renderGraph(pp) 3.2.5 The directed acyclic graph as a representation of joint probability The DAG represents a factorization of the joint probability distribution into a joint probability distribution. In this section we show how to add custom probability distributions to a DAG, as well as how to estimate the parameters of the conditional probability distribution using maximum likelihood estimation or Bayesian estimation. 3.2.6 Specifying the probability distributions on your own Given the DAG, the joint probability distribution of the survey data variables factorizes as follows: \\(Pr(A, S, E, O, R, T) = Pr(A) Pr(S) Pr(E | A, S) Pr(O | E) Pr(R | E) Pr(T | O, R).\\) A.lv &lt;- c(&quot;young&quot;, &quot;adult&quot;, &quot;old&quot;) S.lv &lt;- c(&quot;M&quot;, &quot;F&quot;) E.lv &lt;- c(&quot;high&quot;, &quot;uni&quot;) O.lv &lt;- c(&quot;emp&quot;, &quot;self&quot;) R.lv &lt;- c(&quot;small&quot;, &quot;big&quot;) T.lv &lt;- c(&quot;car&quot;, &quot;train&quot;, &quot;other&quot;) A.prob &lt;- array(c(0.3,0.5,0.2), dim = 3, dimnames = list(A = A.lv)) S.prob &lt;- array(c(0.6,0.4), dim = 2, dimnames = list(S = S.lv)) E.prob &lt;- array(c(0.75,0.25,0.72,0.28,0.88,0.12,0.64,0.36,0.70,0.30,0.90,0.10), dim = c(2,3,2), dimnames = list(E = E.lv, A = A.lv, S = S.lv)) O.prob &lt;- array(c(0.96,0.04,0.92,0.08), dim = c(2,2), dimnames = list(O = O.lv, E = E.lv)) R.prob &lt;- array(c(0.25,0.75,0.2,0.8), dim = c(2,2), dimnames = list(R = R.lv, E = E.lv)) T.prob &lt;- array(c(0.48,0.42,0.10,0.56,0.36,0.08,0.58,0.24,0.18,0.70,0.21,0.09), dim = c(3,2,2), dimnames = list(T = T.lv, O = O.lv, R = R.lv)) cpt &lt;- list(A = A.prob, S = S.prob, E = E.prob, O = O.prob, R = R.prob, T = T.prob) # custom cpt table cpt ## $A ## A ## young adult old ## 0.3 0.5 0.2 ## ## $S ## S ## M F ## 0.6 0.4 ## ## $E ## , , S = M ## ## A ## E young adult old ## high 0.75 0.72 0.88 ## uni 0.25 0.28 0.12 ## ## , , S = F ## ## A ## E young adult old ## high 0.64 0.7 0.9 ## uni 0.36 0.3 0.1 ## ## ## $O ## E ## O high uni ## emp 0.96 0.92 ## self 0.04 0.08 ## ## $R ## E ## R high uni ## small 0.25 0.2 ## big 0.75 0.8 ## ## $T ## , , R = small ## ## O ## T emp self ## car 0.48 0.56 ## train 0.42 0.36 ## other 0.10 0.08 ## ## , , R = big ## ## O ## T emp self ## car 0.58 0.70 ## train 0.24 0.21 ## other 0.18 0.09 Now that we have defined both the DAG and the local distribution corresponding to each variable, we can combine them to form a fully-specified BN. We combine the DAG we stored in dag and a list containing the local distributions, which we will call cpt, into an object of class bn.fit called bn. # fit cpt table to network bn &lt;- custom.fit(dag, cpt) 3.3 Estimating parameters of conditional probability tables For the hypothetical survey described in this chapter, we have assumed to know both the DAG and the parameters of the local distributions defining the BN. In this scenario, BNs are used as expert systems, because they formalise the knowledge possessed by one or more experts in the relevant fields. However, in most cases the parameters of the local distributions will be estimated (or learned) from an observed sample. Let’s read the survey data: survey &lt;- read.table(&quot;data/survey.txt&quot;, header = TRUE) head(survey) ## A R E O S T ## 1 adult big high emp F car ## 2 adult small uni emp M car ## 3 adult big uni emp F train ## 4 adult big high emp M car ## 5 adult big high emp M car ## 6 adult small high emp F train In the case of this survey, and of discrete BNs in general, the parameters to estimate are the conditional probabilities in the local distributions. They can be estimated, for example, with the corresponding empirical frequencies in the data set, e.g., \\(\\hat{Pr}(O = emp | E = high) = \\frac{\\hat{Pr}(O = emp, E = high)}{\\hat{Pr}(E = high)}= \\frac{\\text{number of observations for which O = emp and E = high}}{\\text{number of observations for which E = high}}\\) This yields the classic frequentist and maximum likelihood estimates. In bnlearn, we can compute them with the bn.fit function. bn.fit complements the custom.fit function we used in the previous section; the latter constructs a BN using a set of custom parameters specified by the user, while the former estimates the same from the data. bn.mle &lt;- bn.fit(dag, data = survey, method = &quot;mle&quot;) bn.mle ## ## Bayesian network parameters ## ## Parameters of node A (multinomial distribution) ## ## Conditional probability table: ## adult old young ## 0.472 0.208 0.320 ## ## Parameters of node S (multinomial distribution) ## ## Conditional probability table: ## F M ## 0.402 0.598 ## ## Parameters of node E (multinomial distribution) ## ## Conditional probability table: ## ## , , S = F ## ## A ## E adult old young ## high 0.6391753 0.8461538 0.5384615 ## uni 0.3608247 0.1538462 0.4615385 ## ## , , S = M ## ## A ## E adult old young ## high 0.7194245 0.8923077 0.8105263 ## uni 0.2805755 0.1076923 0.1894737 ## ## ## Parameters of node O (multinomial distribution) ## ## Conditional probability table: ## ## E ## O high uni ## emp 0.98082192 0.92592593 ## self 0.01917808 0.07407407 ## ## Parameters of node R (multinomial distribution) ## ## Conditional probability table: ## ## E ## R high uni ## big 0.7178082 0.8666667 ## small 0.2821918 0.1333333 ## ## Parameters of node T (multinomial distribution) ## ## Conditional probability table: ## ## , , R = big ## ## O ## T emp self ## car 0.58469945 0.69230769 ## other 0.19945355 0.15384615 ## train 0.21584699 0.15384615 ## ## , , R = small ## ## O ## T emp self ## car 0.54700855 0.75000000 ## other 0.07692308 0.25000000 ## train 0.37606838 0.00000000 Note that we assume we know the structure of the network, so dag is an input of bn.fit function. As an alternative, we can also estimate the same conditional probabilities in a Bayesian setting, using their posterior distributions. In this case, the method argument of bn.fit must be set to “bayes”. bn.bayes &lt;- bn.fit(dag, data = survey, method = &quot;bayes&quot;, iss = 10) The estimated posterior probabilities are computed from a uniform prior over each conditional probability table. The iss optional argument, whose name stands for imaginary sample size (also known as equivalent sample size), determines how much weight is assigned to the prior distribution compared to the data when computing the posterior. The weight is specified as the size of an imaginary sample supporting the prior distribution. 3.3.1 Fit dag to data and predict the value of latent variable # predicting a variable in the test set. training = bn.fit(model2network(&quot;[A][B][E][G][C|A:B][D|B][F|A:D:E:G]&quot;), gaussian.test[1:2000, ]) test = gaussian.test[2001:nrow(gaussian.test), ] predicted &lt;- predict(training, node = &quot;A&quot;, data = test, method = &quot;bayes-lw&quot;) head(predicted) ## [1] 3.2905037 0.5755309 0.6127167 1.7173029 1.0264255 1.0253681 about the method bayes-lw: the predicted values are computed by averaging likelihood weighting simulations performed using all the available nodes as evidence (obviously, with the exception of the node whose values we are predicting). If the variable being predicted is discrete, the predicted level is that with the highest conditional probability. If the variable is continuous, the predicted value is the expected value of the conditional distribution. 3.4 Conditional independence in Bayesian networks Using a DAG structure we can investigate whether a variable is conditionally independent from another variable given a set of variables from the DAG. If the variables depend directly on each other, there will be a single arc connecting the nodes corresponding to those two variables. If the dependence is indirect, there will be two or more arcs passing through the nodes that mediate the association. If \\(\\textbf{X}\\) and \\(\\textbf{Y}\\) are separated by \\(\\textbf{Z}\\), we say that \\(\\textbf{X}\\) and \\(\\textbf{Y}\\) are conditionally independent given \\(\\textbf{Z}\\) and denote it with, \\[\\textbf{X } { \\!\\perp\\!\\!\\!\\perp}_{G} \\textbf{Y } | \\textbf{ Z}\\] Conditioning on \\(\\textbf{Z}\\) is equivalent to fixing the values of its elements, so that they are known quantities. \\(\\textbf{Definition (MAPS).}\\) Let M be the dependence structure of the probability distribution P of data \\(\\textbf{D}\\), that is, the set of conditional independence relationships linking any triplet \\(\\textbf{X}\\), \\(\\textbf{Y}\\), \\(\\textbf{Z}\\) of subsets of \\(\\textbf{D}\\). A graph G is a dependency map (or D-map) of M if there is a one-to-one correspondence between the random variables in \\(\\textbf{D}\\) and the nodes \\(\\textbf{V}\\) of G such that for all disjoint subsets \\(\\textbf{X}\\), \\(\\textbf{Y}\\), \\(\\textbf{Z}\\) of \\(\\textbf{D}\\) we have \\[\\textbf{X } {\\!\\perp\\!\\!\\!\\perp}_{P} \\textbf{ Y } | \\textbf{ Z} \\Longrightarrow \\textbf{X } {\\!\\perp\\!\\!\\!\\perp}_{G} \\textbf{ Y } | \\textbf{ Z}\\] Similarly, G is an independency map (or I-map) of M if \\[\\textbf{X } {\\!\\perp\\!\\!\\!\\perp}_{P} \\textbf{ Y } | \\textbf{ Z} \\Longleftarrow \\textbf{X } {\\!\\perp\\!\\!\\!\\perp}_{G} \\textbf{ Y } | \\textbf{ Z}\\] G is said to be a perfect map of M if it is both a D-map and an I-map, that is \\[\\textbf{X } {\\!\\perp\\!\\!\\!\\perp}_{P} \\textbf{ Y } | \\textbf{ Z} \\Longleftrightarrow \\textbf{X } {\\!\\perp\\!\\!\\!\\perp}_{G} \\textbf{ Y } | \\textbf{ Z}\\] and in this case G is said to be faithful or isomorphic to M. \\(\\textbf{Definition.}\\) A variable V is a collider or has a V structure, if it has 2 upcoming parents. (#fig:02_colider_net)V is a collider You can find all the V structures of a DAG: vstructs(dag) ## X Z Y ## [1,] &quot;A&quot; &quot;E&quot; &quot;S&quot; ## [2,] &quot;O&quot; &quot;T&quot; &quot;R&quot; Note that conditioning on a collider induces dependence even though the parents aren’t directly connected. \\(\\textbf{Definition (d-separation)}\\) If G is a directed graph in which \\(\\textbf{X}\\), \\(\\textbf{Y}\\) and \\(\\textbf{Z}\\) are disjoint sets of vertices, then \\(\\textbf{X}\\) and \\(\\textbf{Y}\\) are d-connected by \\(\\textbf{Z}\\) in G if and only if there exists an undirected path U between some vertex in \\(\\textbf{X}\\) and some vertex in \\(\\textbf{Y}\\) such that for every collider C on U, either C or a descendent of C is in \\(\\textbf{Z}\\), and no non-collider on U is in \\(\\textbf{Z}\\). \\(\\textbf{X}\\) and \\(\\textbf{Y}\\) are d-separated by \\(\\textbf{Z}\\) in G if and only if they are not d-connected by \\(\\textbf{Z}\\) in G. We assume that graphical separation (\\({\\!\\perp\\!\\!\\!\\perp}_{G}\\)) implies probabilistic independence (\\({\\!\\perp\\!\\!\\!\\perp}_{P}\\)) in a Bayesian network. We can investigate whether two nodes in a bn object are d-separated using the dsep function. dsep takes three arguments, x, y and z, corresponding to \\(\\textbf{X}\\), \\(\\textbf{Y}\\) and \\(\\textbf{Z}\\); the first two must be the names of two nodes being tested for d-separation, while the latter is an optional d-separating set. So, for example, dsep(dag, x = &quot;S&quot;, y = &quot;R&quot;) ## [1] FALSE dsep(dag, x = &quot;O&quot;, y = &quot;R&quot;) ## [1] FALSE dsep(dag, x = &quot;S&quot;, y = &quot;R&quot;, z = &quot;E&quot;) ## [1] TRUE 3.4.1 Markov Property, Equivalence classes and CPDAGS \\(\\textbf{Definition (Local Markov property)}\\) Each node \\(X_i\\) is conditionally independent of its non-descendants given its parents. Compared to the previous decomposition, it highlights the fact that parents are not completely independent from their children in the BN; a trivial application of Bayes’ theorem to invert the direction of the conditioning shows how information on a child can change the distribution of the parent. Second, assuming the DAG is an I-map also means that serial and divergent connections result in equivalent factorisations of the variables involved. It is easy to show that \\[\\begin{align} Pr(X_i) Pr(X_j | X_i) Pr(X_k | X_j) &amp;= Pr(X_j,X_i) Pr(X_k | X_j)\\\\ &amp;= Pr(X_i | X_j) Pr(X_j) Pr(X_k | X_j) \\end{align}\\] Then \\(X_i \\longrightarrow X_j \\longrightarrow X_k\\) and \\(X_i \\longleftarrow X_j \\longrightarrow X_k\\) are equivalent. As a result, we can have BNs with different arc sets that encode the same conditional independence relationships and represent the same global distribution in different (but probabilistically equivalent) ways. Such DAGs are said to belong to the same equivalence class. 3.4.2 Skeleton of a network, CPDAGs and equivalence classes The skeleton of a network is the network without any direction. Here is the skeleton of the dag for survey dataset. graphviz.plot(skeleton(dag)) \\(\\textbf{Theorem. (Equivalence classes)}\\) Two DAGs defined over the same set of variables are equivalent and only they have the same skeleton (i.e., the same underlying undirected graph) and the same v-structures. data(learning.test) learn.net1 = empty.graph(names(learning.test)) learn.net2 = empty.graph(c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;,&quot;E&quot;,&quot;F&quot;)) modelstring(learn.net1) = &quot;[A][C][F][B|A][D|A:C][E|B:F]&quot; arc.set2 &lt;- matrix(c(&quot;B&quot;, &quot;A&quot;, &quot;A&quot;, &quot;D&quot;, &quot;C&quot;, &quot;D&quot;, &quot;B&quot;, &quot;E&quot;, &quot;F&quot;, &quot;E&quot;), byrow = TRUE, ncol = 2, dimnames = list(NULL, c(&quot;from&quot;, &quot;to&quot;))) arcs(learn.net2) &lt;- arc.set2 graphviz.compare(learn.net1,learn.net2) score(learn.net1, learning.test, type = &quot;loglik&quot;) ## [1] -23832.13 score(learn.net2, learning.test, type = &quot;loglik&quot;) ## [1] -23832.13 # type == &quot;loglik&quot; means you get the log likelihood of the data given the dag and the MLE of the parameters In other words, the only arcs whose directions are important are those that are part of one or more v-structures. The skeleton of a DAG and it’s V structures identify the equivalence class the DAG belongs to, which is represented by the completed partially directed graph (CPDAG). We can obtain it from a DAG with cpdag function. X &lt;- paste(&quot;[X1][X3][X5][X6|X8][X2|X1][X7|X5][X4|X1:X2]&quot;, &quot;[X8|X3:X7][X9|X2:X7][X10|X1:X9]&quot;, sep = &quot;&quot;) dag2 &lt;- model2network(X) par(mfrow = c(1,2)) graphviz.plot(dag2) graphviz.plot(cpdag(dag2)) 3.4.3 Moral Graphs In previous Section we introduced an alternative graphical representation of the DAG underlying a BN: the CPDAG of the equivalence class the BN belongs to. Another graphical representation that can be derived from the DAG is the moral graph. The moral graph is an undirected graph that is constructed as follows: connecting the non-adjacent nodes in each v-structure with an undirected arc; ignoring the direction of the other arcs, effectively replacing them with undirected arcs. This transformation is called moralisation because it “marries” non-adjacent parents sharing a common child. In the case of our example dag, we can create the moral graph with the moral function as follows: graphviz.plot(moral(dag2)) Moralisation has several uses. First, it provides a simple way to transform a BN into the corresponding Markov network, a graphical model using undirected graphs instead of DAGs to represent dependencies. In a Markov network, we say that \\(\\textbf{X} {\\!\\perp\\!\\!\\!\\perp}_{G} \\textbf{Y} | \\textbf{Z}\\) if every path between \\(\\textbf{X}\\) and \\(\\textbf{Y}\\) contains some node \\(Z \\in \\textbf{Z}\\). 3.5 Plotting Conditional Probability Distributions Plotting the conditional probabilities associated with a conditional probability table or a query is also useful for diagnostic and exploratory purposes. Such plots can be difficult to read when a large number of conditioning variables is involved, but nevertheless they provide useful insights for most synthetic and real-world data sets. As far as conditional probability tables are concerned, bnlearn provides functions to plot barcharts (bn.fit.barchart) and dot plots (bn.fit.dotplot) from bn.fit objects. Both functions are based on the lattice package. For example let’s look at the conditional plot of \\(Pr(T | R,O)\\): bn.fit.barchart(bn.mle$T, main = &quot;Travel&quot;, xlab = &quot;Pr(T | R,O)&quot;, ylab = &quot;&quot;) ## Loading required namespace: lattice "],
["tutorial-on-deep-probabilitic-modeling-with-pyro.html", "4 Tutorial on deep probabilitic modeling with Pyro 4.1 Recap on Motivation 4.2 Introduction to Pyro 4.3 Inference 4.4 Some other Pyro vocabulary", " 4 Tutorial on deep probabilitic modeling with Pyro import torch import pyro pyro.set_rng_seed(101) 4.1 Recap on Motivation Our goal is to understand causal modeling within the context of generative machine learning. We just examined one generative machine learning framework called Bayesian networks (BNs) and how we can use BNs as causal models. Bayesian Networks (BNs) Framework that defines a probabilistic generative model of the world in terms of a directed acyclic graph. causal Bayesian networks: Bayesian networks where the direction of edges in the DAG represent causality. Bayesian networks provide a general-purpose framework for representing a causal data generating story for how the world works. Now we will introduce probabilistic programming, a framework that is more expressive than Bayesian networks. 4.1.1 What is a probabilistic programming language? “A probabilistic programming language (PPL) is a programming language designed to describe probabilistic models and then perform inference in those models. PPLs are closely related to graphical models and Bayesian networks but are more expressive and flexible. Probabilistic programming represents an attempt to”Unify general purpose programming&quot; with probabilistic modeling.&quot; -Wikipedia A PPL is a domain-specific programming language for that lets you write a data generating story as a program. As with a causal Bayesian network, you can write your program in a way that orders the steps of its execution according to cause and effect. 4.1.2 How exactly do Bayesian networks and probabilistic programming differ? Representation of relationships between variables. BNs restricted to representing the relationships between variables in terms of conditional probability distributions (CPDs) factored according to a DAG. Frameworks typically limit you to a small set of parametric CPDs (e.g., Gaussian, multinomial). Just as computer programs are more expressive than flow charts, PPLs let you represent relations any way you like so long as you can represent them in code. PPL relationships can include control flow and recursion. In causal models, we will see that this allows you to be more specific about mechanism than you can with CPDs. DAG vs. open world models. BNs restrict the representation of the joint distribution to a DAG. This constraint enables you to reason easily about the joint distribution through graph-theoretic operations like d-separation. PPLs need not be constrained to a DAG. For example (using an imaginary Python PPL package): X = Bernoulli(p) if X == 1: Y = Gaussian(0, 1) In a DAG, you have a fixed set of variables, i.e. a “closed world”. In the above model, the variable Y is only instantiated if X==1. Y may or may not exist depending on how the generative process unfolds. For a more extreme example, consider this: X = Poisson(λ) Y = [Gaussian(0, 1)] for i in range(1, X): Y[i] = Gaussian(Y[i-1], 1)) Here you have the total number of Y variables itself being a random variable X. Further, the mean of the ith Y is a random variable given by the i-1th Y. You can’t do that with a Bayes net! Unfortunately, we can’t reason about this as directly as we can with a DAG. For example, recall that with the DAG, we had a convenient algorithm called CPDAG that converts the DAG to a partially directed acyclic graph structure called a PDAG that provides a compact representation of all the DAGs in an equivalence class. How might we define an equivalence class on this program? Certainly, enumerating all programs with an equivalent representation of the joint distribution would be very difficult even with constraints on the length of the program. In general, enumerating all programs of minimal description that provide equivalent representations of a joint distribution is an NP-hard problem. Inference When you have a DAG and a constrained set of parametric CPDs, as well as constraints on the kind of inference, queries the user can make, you can implement some inference algorithms in your BN framework that will generally work in a reasonable amount of time. PPLs are more flexible than BNs, but the trade-off s that getting inference to work is harder. PPL’s develop several abstractions for inference and leave it to the user to apply them, requiring the user to become something of an expert in inference algorithms. PPL developers make design decisions to make inference easier for the user, though this often sacrifices some flexibility. One emergent pattern is to build PPLs on tensor-based frameworks like Tensorflow and PyTorch. Tensor-based PPLs allow a data scientist with experience building deep learning models to rely on that experience when doing inference. Image \\[\\texttt{Kevin Smith - Tutorial: Probabilistic Programming}\\] 4.2 Introduction to Pyro Pyro is a universal probabilistic programming language (PPL) written in Python and supported by PyTorch on the backend. Pyro enables flexible and expressive deep probabilistic modeling, unifying the best of modern deep learning and Bayesian modeling. Our purpose of this class, pyro has “do”-operator that allows intervention and counterfactual inference in these probabilistic models. 4.2.1 Stochastic Functions The basic unit of probabilistic programs is the stochastic function. A stochastic function is an arbitrary Python callable that combines two ingredients: deterministic Python code; and primitive stochastic functions that call a random number generator For this course, we will consider these stochastic functions as models. Stochastic functions can be used to represent simplified or abstract descriptions of a data-generating process. 4.2.2 Primitive stochastic functions We call them distributions. We can explicitly compute the probability of the outputs given the inputs. loc = 0. # mean zero scale = 1. # unit variance normal = torch.distributions.Normal(loc, scale) # create a normal distribution object x = normal.rsample() # draw a sample from N(0,1) print(&quot;sample: &quot;, x) ## sample: tensor(-1.3905) Pyro simplifies this process of sampling from distributions. It uses pyro.sample(). x = pyro.sample(&quot;my_sample&quot;, pyro.distributions.Normal(loc, scale)) print(x) ## tensor(-0.8152) Just like a direct call to torch.distributions.Normal().rsample(), this returns a sample from the unit normal distribution. The crucial difference is that this sample is named. Pyro’s backend uses these names to uniquely identify sample statements and change their behavior at runtime depending on how the enclosing stochastic function is being used. This is how Pyro can implement the various manipulations that underlie inference algorithms. Let’s write a simple weather model. 4.2.3 A simple model import pyro.distributions as dist def weather(): cloudy = pyro.sample(&#39;cloudy&#39;, dist.Bernoulli(0.3)) cloudy = &#39;cloudy&#39; if cloudy.item() == 1.0 else &#39;sunny&#39; mean_temp = {&#39;cloudy&#39;: 55.0, &#39;sunny&#39;: 75.0}[cloudy] scale_temp = {&#39;cloudy&#39;: 10.0, &#39;sunny&#39;: 15.0}[cloudy] temp = pyro.sample(&#39;temp&#39;, dist.Normal(mean_temp, scale_temp)) return cloudy, temp.item() for _ in range(3): print(weather()) ## (&#39;cloudy&#39;, 64.5440444946289) ## (&#39;sunny&#39;, 94.37557983398438) ## (&#39;sunny&#39;, 72.5186767578125) First two lines introduce a binary variable cloudy, which is given by a draw from the Bernoulli distribution with a parameter of \\(0.3\\). The Bernoulli distribution returns either \\(0\\) or \\(1\\), line 2 converts that into a string. So, So according to this model, \\(30%\\) of the time it’s cloudy and \\(70%\\) of the time it’s sunny. In line 4 and 5, we initialize mean and scale of the temperature for both values. We then sample, the temperature from a Normal distribution and return that along with cloudy variable. We can build complex model by modularizing and reusing the concepts into functions and use them as programmers use functions. def ice_cream_sales(): cloudy, temp = weather() expected_sales = 200. if cloudy == &#39;sunny&#39; and temp &gt; 80.0 else 50. ice_cream = pyro.sample(&#39;ice_cream&#39;, pyro.distributions.Normal(expected_sales, 10.0)) return ice_cream 4.3 Inference As we discussed earlier, the reason we use PPLs is because they can easily go backwards and reason about cause given the observed effect. There are myriad of inference algorithms available in pyro. Let’s try it on an even simpler model. \\[weight \\mid guess \\sim \\mathcal{N}(guess, 1)\\] \\[measurement \\mid guess, weight \\sim \\mathcal{N}(weight, 0.75)\\] def scale(guess): weight = pyro.sample(&quot;weight&quot;, dist.Normal(guess, 1.0)) measurement = pyro.sample(&quot;measurement&quot;, dist.Normal(weight, 0.75)) return measurement scale(5.) Suppose we observe that the measurement of an object was \\(14\\) lbs. What would have we guessed if we tried to guess it’s weight first? This question is answered in two steps. Condition the model. conditioned_scale = pyro.condition(scale, data={&quot;measurement&quot;: torch.tensor(14.)}) Set the prior and infer the posterior. We will use from pyro.infer.mcmc import MCMC from pyro.infer.mcmc.nuts import HMC from pyro.infer import EmpiricalMarginal import matplotlib.pyplot as plt # %matplotlib inline guess_prior = 10. hmc_kernel = HMC(conditioned_scale, step_size=0.9, num_steps=4) posterior = MCMC(hmc_kernel, num_samples=1000, warmup_steps=50).run(guess_prior) ## Warmup: 0%| | 0/1050 [00:00, ?it/s] Warmup: 0%| | 0/1050 [00:00, ?it/s, step size=6.42e+00, acc. rate=1.000] Warmup: 0%| | 1/1050 [00:00, 11.38it/s, step size=1.08e+00, acc. rate=0.500] Warmup: 0%| | 2/1050 [00:00, 21.95it/s, step size=1.23e-01, acc. rate=0.333] Warmup: 0%| | 3/1050 [00:00, 26.89it/s, step size=1.68e-01, acc. rate=0.500] Warmup: 0%| | 4/1050 [00:00, 35.84it/s, step size=1.68e-01, acc. rate=0.500] Warmup: 0%| | 4/1050 [00:00, 35.84it/s, step size=2.65e-01, acc. rate=0.600] Warmup: 0%| | 5/1050 [00:00, 35.84it/s, step size=4.46e-01, acc. rate=0.667] Warmup: 1%| | 6/1050 [00:00, 35.84it/s, step size=8.00e-01, acc. rate=0.714] Warmup: 1%| | 7/1050 [00:00, 35.84it/s, step size=6.91e-01, acc. rate=0.750] Warmup: 1%| | 8/1050 [00:00, 35.84it/s, step size=1.29e+00, acc. rate=0.778] Warmup: 1%| | 9/1050 [00:00, 35.84it/s, step size=1.02e-01, acc. rate=0.700] Warmup: 1%| | 10/1050 [00:00, 35.84it/s, step size=1.93e-01, acc. rate=0.727] Warmup: 1%|1 | 11/1050 [00:00, 35.84it/s, step size=3.66e-01, acc. rate=0.750] Warmup: 1%|1 | 12/1050 [00:00, 35.84it/s, step size=6.89e-01, acc. rate=0.769] Warmup: 1%|1 | 13/1050 [00:00, 35.84it/s, step size=1.22e+00, acc. rate=0.786] Warmup: 1%|1 | 14/1050 [00:00, 35.84it/s, step size=1.03e-01, acc. rate=0.733] Warmup: 1%|1 | 15/1050 [00:00, 35.84it/s, step size=1.96e-01, acc. rate=0.750] Warmup: 2%|1 | 16/1050 [00:00, 44.56it/s, step size=1.96e-01, acc. rate=0.750] Warmup: 2%|1 | 16/1050 [00:00, 44.56it/s, step size=3.57e-01, acc. rate=0.765] Warmup: 2%|1 | 17/1050 [00:00, 44.56it/s, step size=6.67e-01, acc. rate=0.778] Warmup: 2%|1 | 18/1050 [00:00, 44.56it/s, step size=1.24e+00, acc. rate=0.789] Warmup: 2%|1 | 19/1050 [00:00, 44.56it/s, step size=1.15e-01, acc. rate=0.750] Warmup: 2%|1 | 20/1050 [00:00, 44.56it/s, step size=2.13e-01, acc. rate=0.762] Warmup: 2%|2 | 21/1050 [00:00, 44.56it/s, step size=3.92e-01, acc. rate=0.773] Warmup: 2%|2 | 22/1050 [00:00, 44.56it/s, step size=6.74e-01, acc. rate=0.783] Warmup: 2%|2 | 23/1050 [00:00, 44.56it/s, step size=8.90e-01, acc. rate=0.792] Warmup: 2%|2 | 24/1050 [00:00, 44.56it/s, step size=1.60e+00, acc. rate=0.800] Warmup: 2%|2 | 25/1050 [00:00, 44.56it/s, step size=1.67e-01, acc. rate=0.769] Warmup: 2%|2 | 26/1050 [00:00, 44.56it/s, step size=3.02e-01, acc. rate=0.778] Warmup: 3%|2 | 27/1050 [00:00, 44.56it/s, step size=5.38e-01, acc. rate=0.786] Warmup: 3%|2 | 28/1050 [00:00, 54.82it/s, step size=5.38e-01, acc. rate=0.786] Warmup: 3%|2 | 28/1050 [00:00, 54.82it/s, step size=9.52e-01, acc. rate=0.793] Warmup: 3%|2 | 29/1050 [00:00, 54.82it/s, step size=1.67e+00, acc. rate=0.800] Warmup: 3%|2 | 30/1050 [00:00, 54.82it/s, step size=1.91e-01, acc. rate=0.774] Warmup: 3%|2 | 31/1050 [00:00, 54.82it/s, step size=3.34e-01, acc. rate=0.781] Warmup: 3%|3 | 32/1050 [00:00, 54.82it/s, step size=4.76e-01, acc. rate=0.788] Warmup: 3%|3 | 33/1050 [00:00, 54.82it/s, step size=7.42e-01, acc. rate=0.794] Warmup: 3%|3 | 34/1050 [00:00, 54.82it/s, step size=1.27e+00, acc. rate=0.800] Warmup: 3%|3 | 35/1050 [00:00, 54.82it/s, step size=1.59e-01, acc. rate=0.778] Warmup: 3%|3 | 36/1050 [00:00, 54.82it/s, step size=2.64e-01, acc. rate=0.784] Warmup: 4%|3 | 37/1050 [00:00, 54.82it/s, step size=4.51e-01, acc. rate=0.789] Warmup: 4%|3 | 38/1050 [00:00, 54.82it/s, step size=7.64e-01, acc. rate=0.795] Warmup: 4%|3 | 39/1050 [00:00, 54.82it/s, step size=9.19e-01, acc. rate=0.800] Warmup: 4%|3 | 40/1050 [00:00, 54.82it/s, step size=1.69e-01, acc. rate=0.780] Warmup: 4%|3 | 41/1050 [00:00, 54.82it/s, step size=2.85e-01, acc. rate=0.786] Warmup: 4%|4 | 42/1050 [00:00, 54.82it/s, step size=4.49e-01, acc. rate=0.791] Warmup: 4%|4 | 43/1050 [00:00, 66.98it/s, step size=4.49e-01, acc. rate=0.791] Warmup: 4%|4 | 43/1050 [00:00, 66.98it/s, step size=6.73e-01, acc. rate=0.795] Warmup: 4%|4 | 44/1050 [00:00, 66.98it/s, step size=1.59e+00, acc. rate=0.778] Warmup: 4%|4 | 45/1050 [00:00, 66.98it/s, step size=2.29e+01, acc. rate=0.783] Warmup: 4%|4 | 46/1050 [00:00, 66.98it/s, step size=3.87e+00, acc. rate=0.766] Warmup: 4%|4 | 47/1050 [00:00, 66.98it/s, step size=3.82e-01, acc. rate=0.750] Warmup: 5%|4 | 48/1050 [00:00, 66.98it/s, step size=5.17e-01, acc. rate=0.755] Warmup: 5%|4 | 49/1050 [00:00, 66.98it/s, step size=1.11e+00, acc. rate=0.760] Sample: 5%|4 | 50/1050 [00:00, 66.98it/s, step size=1.11e+00, acc. rate=0.760] Sample: 5%|4 | 50/1050 [00:00, 66.98it/s, step size=1.11e+00, acc. rate=0.765] Sample: 5%|4 | 51/1050 [00:00, 66.98it/s, step size=1.11e+00, acc. rate=0.769] Sample: 5%|4 | 52/1050 [00:00, 66.98it/s, step size=1.11e+00, acc. rate=0.774] Sample: 5%|5 | 53/1050 [00:00, 66.98it/s, step size=1.11e+00, acc. rate=0.778] Sample: 5%|5 | 54/1050 [00:00, 66.98it/s, step size=1.11e+00, acc. rate=0.782] Sample: 5%|5 | 55/1050 [00:00, 66.98it/s, step size=1.11e+00, acc. rate=0.786] Sample: 5%|5 | 56/1050 [00:00, 66.98it/s, step size=1.11e+00, acc. rate=0.789] Sample: 5%|5 | 57/1050 [00:00, 66.98it/s, step size=1.11e+00, acc. rate=0.793] Sample: 6%|5 | 58/1050 [00:00, 66.98it/s, step size=1.11e+00, acc. rate=0.797] Sample: 6%|5 | 59/1050 [00:00, 66.98it/s, step size=1.11e+00, acc. rate=0.800] Sample: 6%|5 | 60/1050 [00:00, 66.98it/s, step size=1.11e+00, acc. rate=0.803] Sample: 6%|5 | 61/1050 [00:00, 66.98it/s, step size=1.11e+00, acc. rate=0.806] Sample: 6%|5 | 62/1050 [00:00, 66.98it/s, step size=1.11e+00, acc. rate=0.810] Sample: 6%|6 | 63/1050 [00:00, 66.98it/s, step size=1.11e+00, acc. rate=0.812] Sample: 6%|6 | 64/1050 [00:00, 66.98it/s, step size=1.11e+00, acc. rate=0.815] Sample: 6%|6 | 65/1050 [00:00, 66.98it/s, step size=1.11e+00, acc. rate=0.818] Sample: 6%|6 | 66/1050 [00:00, 66.98it/s, step size=1.11e+00, acc. rate=0.821] Sample: 6%|6 | 67/1050 [00:00, 66.98it/s, step size=1.11e+00, acc. rate=0.824] Sample: 6%|6 | 68/1050 [00:00, 66.98it/s, step size=1.11e+00, acc. rate=0.826] Sample: 7%|6 | 69/1050 [00:00, 66.98it/s, step size=1.11e+00, acc. rate=0.829] Sample: 7%|6 | 70/1050 [00:00, 66.98it/s, step size=1.11e+00, acc. rate=0.831] Sample: 7%|6 | 71/1050 [00:00, 66.98it/s, step size=1.11e+00, acc. rate=0.833] Sample: 7%|6 | 72/1050 [00:00, 66.98it/s, step size=1.11e+00, acc. rate=0.836] Sample: 7%|6 | 73/1050 [00:00, 66.98it/s, step size=1.11e+00, acc. rate=0.838] Sample: 7%|7 | 74/1050 [00:00, 66.98it/s, step size=1.11e+00, acc. rate=0.840] Sample: 7%|7 | 75/1050 [00:00, 66.98it/s, step size=1.11e+00, acc. rate=0.842] Sample: 7%|7 | 76/1050 [00:00, 66.98it/s, step size=1.11e+00, acc. rate=0.844] Sample: 7%|7 | 77/1050 [00:00, 88.08it/s, step size=1.11e+00, acc. rate=0.844] Sample: 7%|7 | 77/1050 [00:00, 88.08it/s, step size=1.11e+00, acc. rate=0.846] Sample: 7%|7 | 78/1050 [00:00, 88.08it/s, step size=1.11e+00, acc. rate=0.848] Sample: 8%|7 | 79/1050 [00:00, 88.08it/s, step size=1.11e+00, acc. rate=0.850] Sample: 8%|7 | 80/1050 [00:00, 88.08it/s, step size=1.11e+00, acc. rate=0.852] Sample: 8%|7 | 81/1050 [00:00, 88.08it/s, step size=1.11e+00, acc. rate=0.854] Sample: 8%|7 | 82/1050 [00:00, 88.08it/s, step size=1.11e+00, acc. rate=0.855] Sample: 8%|7 | 83/1050 [00:00, 88.08it/s, step size=1.11e+00, acc. rate=0.857] Sample: 8%|8 | 84/1050 [00:00, 88.08it/s, step size=1.11e+00, acc. rate=0.859] Sample: 8%|8 | 85/1050 [00:00, 88.08it/s, step size=1.11e+00, acc. rate=0.860] Sample: 8%|8 | 86/1050 [00:00, 88.08it/s, step size=1.11e+00, acc. rate=0.862] Sample: 8%|8 | 87/1050 [00:00, 88.08it/s, step size=1.11e+00, acc. rate=0.864] Sample: 8%|8 | 88/1050 [00:00, 88.08it/s, step size=1.11e+00, acc. rate=0.865] Sample: 8%|8 | 89/1050 [00:00, 88.08it/s, step size=1.11e+00, acc. rate=0.867] Sample: 9%|8 | 90/1050 [00:00, 88.08it/s, step size=1.11e+00, acc. rate=0.868] Sample: 9%|8 | 91/1050 [00:00, 88.08it/s, step size=1.11e+00, acc. rate=0.870] Sample: 9%|8 | 92/1050 [00:00, 88.08it/s, step size=1.11e+00, acc. rate=0.860] Sample: 9%|8 | 93/1050 [00:00, 88.08it/s, step size=1.11e+00, acc. rate=0.862] Sample: 9%|8 | 94/1050 [00:00, 88.08it/s, step size=1.11e+00, acc. rate=0.863] Sample: 9%|9 | 95/1050 [00:00, 88.08it/s, step size=1.11e+00, acc. rate=0.865] Sample: 9%|9 | 96/1050 [00:00, 88.08it/s, step size=1.11e+00, acc. rate=0.866] Sample: 9%|9 | 97/1050 [00:00, 88.08it/s, step size=1.11e+00, acc. rate=0.867] Sample: 9%|9 | 98/1050 [00:00, 88.08it/s, step size=1.11e+00, acc. rate=0.869] Sample: 9%|9 | 99/1050 [00:00, 88.08it/s, step size=1.11e+00, acc. rate=0.870] Sample: 10%|9 | 100/1050 [00:00, 88.08it/s, step size=1.11e+00, acc. rate=0.871] Sample: 10%|9 | 101/1050 [00:00, 88.08it/s, step size=1.11e+00, acc. rate=0.873] Sample: 10%|9 | 102/1050 [00:00, 88.08it/s, step size=1.11e+00, acc. rate=0.874] Sample: 10%|9 | 103/1050 [00:00, 88.08it/s, step size=1.11e+00, acc. rate=0.875] Sample: 10%|9 | 104/1050 [00:00, 88.08it/s, step size=1.11e+00, acc. rate=0.876] Sample: 10%|# | 105/1050 [00:00, 88.08it/s, step size=1.11e+00, acc. rate=0.877] Sample: 10%|# | 106/1050 [00:00, 88.08it/s, step size=1.11e+00, acc. rate=0.879] Sample: 10%|# | 107/1050 [00:00, 88.08it/s, step size=1.11e+00, acc. rate=0.880] Sample: 10%|# | 108/1050 [00:00, 88.08it/s, step size=1.11e+00, acc. rate=0.872] Sample: 10%|# | 109/1050 [00:00, 88.08it/s, step size=1.11e+00, acc. rate=0.873] Sample: 10%|# | 110/1050 [00:00, 88.08it/s, step size=1.11e+00, acc. rate=0.874] Sample: 11%|# | 111/1050 [00:00, 88.08it/s, step size=1.11e+00, acc. rate=0.875] Sample: 11%|# | 112/1050 [00:00, 113.46it/s, step size=1.11e+00, acc. rate=0.875] Sample: 11%|# | 112/1050 [00:00, 113.46it/s, step size=1.11e+00, acc. rate=0.876] Sample: 11%|# | 113/1050 [00:00, 113.46it/s, step size=1.11e+00, acc. rate=0.877] Sample: 11%|# | 114/1050 [00:00, 113.46it/s, step size=1.11e+00, acc. rate=0.878] Sample: 11%|# | 115/1050 [00:00, 113.46it/s, step size=1.11e+00, acc. rate=0.879] Sample: 11%|#1 | 116/1050 [00:00, 113.46it/s, step size=1.11e+00, acc. rate=0.880] Sample: 11%|#1 | 117/1050 [00:00, 113.46it/s, step size=1.11e+00, acc. rate=0.881] Sample: 11%|#1 | 118/1050 [00:00, 113.46it/s, step size=1.11e+00, acc. rate=0.882] Sample: 11%|#1 | 119/1050 [00:00, 113.46it/s, step size=1.11e+00, acc. rate=0.883] Sample: 11%|#1 | 120/1050 [00:00, 113.46it/s, step size=1.11e+00, acc. rate=0.884] Sample: 12%|#1 | 121/1050 [00:00, 113.46it/s, step size=1.11e+00, acc. rate=0.885] Sample: 12%|#1 | 122/1050 [00:00, 113.46it/s, step size=1.11e+00, acc. rate=0.886] Sample: 12%|#1 | 123/1050 [00:00, 113.46it/s, step size=1.11e+00, acc. rate=0.887] Sample: 12%|#1 | 124/1050 [00:00, 113.46it/s, step size=1.11e+00, acc. rate=0.888] Sample: 12%|#1 | 125/1050 [00:00, 113.46it/s, step size=1.11e+00, acc. rate=0.889] Sample: 12%|#2 | 126/1050 [00:00, 113.46it/s, step size=1.11e+00, acc. rate=0.890] Sample: 12%|#2 | 127/1050 [00:00, 113.46it/s, step size=1.11e+00, acc. rate=0.883] Sample: 12%|#2 | 128/1050 [00:00, 113.46it/s, step size=1.11e+00, acc. rate=0.884] Sample: 12%|#2 | 129/1050 [00:00, 113.46it/s, step size=1.11e+00, acc. rate=0.885] Sample: 12%|#2 | 130/1050 [00:00, 113.46it/s, step size=1.11e+00, acc. rate=0.885] Sample: 12%|#2 | 131/1050 [00:00, 113.46it/s, step size=1.11e+00, acc. rate=0.886] Sample: 13%|#2 | 132/1050 [00:00, 113.46it/s, step size=1.11e+00, acc. rate=0.887] Sample: 13%|#2 | 133/1050 [00:00, 113.46it/s, step size=1.11e+00, acc. rate=0.888] Sample: 13%|#2 | 134/1050 [00:00, 113.46it/s, step size=1.11e+00, acc. rate=0.889] Sample: 13%|#2 | 135/1050 [00:00, 113.46it/s, step size=1.11e+00, acc. rate=0.890] Sample: 13%|#2 | 136/1050 [00:00, 113.46it/s, step size=1.11e+00, acc. rate=0.891] Sample: 13%|#3 | 137/1050 [00:00, 113.46it/s, step size=1.11e+00, acc. rate=0.891] Sample: 13%|#3 | 138/1050 [00:00, 113.46it/s, step size=1.11e+00, acc. rate=0.892] Sample: 13%|#3 | 139/1050 [00:00, 113.46it/s, step size=1.11e+00, acc. rate=0.893] Sample: 13%|#3 | 140/1050 [00:00, 113.46it/s, step size=1.11e+00, acc. rate=0.894] Sample: 13%|#3 | 141/1050 [00:00, 113.46it/s, step size=1.11e+00, acc. rate=0.894] Sample: 14%|#3 | 142/1050 [00:00, 113.46it/s, step size=1.11e+00, acc. rate=0.895] Sample: 14%|#3 | 143/1050 [00:00, 113.46it/s, step size=1.11e+00, acc. rate=0.896] Sample: 14%|#3 | 144/1050 [00:00, 113.46it/s, step size=1.11e+00, acc. rate=0.897] Sample: 14%|#3 | 145/1050 [00:00, 113.46it/s, step size=1.11e+00, acc. rate=0.897] Sample: 14%|#3 | 146/1050 [00:00, 113.46it/s, step size=1.11e+00, acc. rate=0.898] Sample: 14%|#4 | 147/1050 [00:00, 141.91it/s, step size=1.11e+00, acc. rate=0.898] Sample: 14%|#4 | 147/1050 [00:00, 141.91it/s, step size=1.11e+00, acc. rate=0.899] Sample: 14%|#4 | 148/1050 [00:00, 141.91it/s, step size=1.11e+00, acc. rate=0.899] Sample: 14%|#4 | 149/1050 [00:00, 141.91it/s, step size=1.11e+00, acc. rate=0.900] Sample: 14%|#4 | 150/1050 [00:00, 141.91it/s, step size=1.11e+00, acc. rate=0.901] Sample: 14%|#4 | 151/1050 [00:00, 141.91it/s, step size=1.11e+00, acc. rate=0.901] Sample: 14%|#4 | 152/1050 [00:00, 141.91it/s, step size=1.11e+00, acc. rate=0.902] Sample: 15%|#4 | 153/1050 [00:00, 141.91it/s, step size=1.11e+00, acc. rate=0.903] Sample: 15%|#4 | 154/1050 [00:00, 141.91it/s, step size=1.11e+00, acc. rate=0.903] Sample: 15%|#4 | 155/1050 [00:00, 141.91it/s, step size=1.11e+00, acc. rate=0.904] Sample: 15%|#4 | 156/1050 [00:00, 141.91it/s, step size=1.11e+00, acc. rate=0.904] Sample: 15%|#4 | 157/1050 [00:00, 141.91it/s, step size=1.11e+00, acc. rate=0.905] Sample: 15%|#5 | 158/1050 [00:00, 141.91it/s, step size=1.11e+00, acc. rate=0.906] Sample: 15%|#5 | 159/1050 [00:00, 141.91it/s, step size=1.11e+00, acc. rate=0.906] Sample: 15%|#5 | 160/1050 [00:00, 141.91it/s, step size=1.11e+00, acc. rate=0.907] Sample: 15%|#5 | 161/1050 [00:00, 141.91it/s, step size=1.11e+00, acc. rate=0.907] Sample: 15%|#5 | 162/1050 [00:00, 141.91it/s, step size=1.11e+00, acc. rate=0.908] Sample: 16%|#5 | 163/1050 [00:00, 141.91it/s, step size=1.11e+00, acc. rate=0.909] Sample: 16%|#5 | 164/1050 [00:00, 141.91it/s, step size=1.11e+00, acc. rate=0.909] Sample: 16%|#5 | 165/1050 [00:00, 141.91it/s, step size=1.11e+00, acc. rate=0.910] Sample: 16%|#5 | 166/1050 [00:00, 141.91it/s, step size=1.11e+00, acc. rate=0.910] Sample: 16%|#5 | 167/1050 [00:00, 141.91it/s, step size=1.11e+00, acc. rate=0.911] Sample: 16%|#6 | 168/1050 [00:00, 141.91it/s, step size=1.11e+00, acc. rate=0.911] Sample: 16%|#6 | 169/1050 [00:00, 141.91it/s, step size=1.11e+00, acc. rate=0.912] Sample: 16%|#6 | 170/1050 [00:00, 141.91it/s, step size=1.11e+00, acc. rate=0.912] Sample: 16%|#6 | 171/1050 [00:00, 141.91it/s, step size=1.11e+00, acc. rate=0.913] Sample: 16%|#6 | 172/1050 [00:00, 141.91it/s, step size=1.11e+00, acc. rate=0.913] Sample: 16%|#6 | 173/1050 [00:00, 141.91it/s, step size=1.11e+00, acc. rate=0.914] Sample: 17%|#6 | 174/1050 [00:00, 141.91it/s, step size=1.11e+00, acc. rate=0.914] Sample: 17%|#6 | 175/1050 [00:00, 141.91it/s, step size=1.11e+00, acc. rate=0.915] Sample: 17%|#6 | 176/1050 [00:00, 141.91it/s, step size=1.11e+00, acc. rate=0.915] Sample: 17%|#6 | 177/1050 [00:00, 141.91it/s, step size=1.11e+00, acc. rate=0.916] Sample: 17%|#6 | 178/1050 [00:00, 141.91it/s, step size=1.11e+00, acc. rate=0.916] Sample: 17%|#7 | 179/1050 [00:00, 141.91it/s, step size=1.11e+00, acc. rate=0.917] Sample: 17%|#7 | 180/1050 [00:00, 141.91it/s, step size=1.11e+00, acc. rate=0.917] Sample: 17%|#7 | 181/1050 [00:00, 141.91it/s, step size=1.11e+00, acc. rate=0.918] Sample: 17%|#7 | 182/1050 [00:00, 141.91it/s, step size=1.11e+00, acc. rate=0.918] Sample: 17%|#7 | 183/1050 [00:00, 172.91it/s, step size=1.11e+00, acc. rate=0.918] Sample: 17%|#7 | 183/1050 [00:00, 172.91it/s, step size=1.11e+00, acc. rate=0.918] Sample: 18%|#7 | 184/1050 [00:00, 172.91it/s, step size=1.11e+00, acc. rate=0.919] Sample: 18%|#7 | 185/1050 [00:00, 172.91it/s, step size=1.11e+00, acc. rate=0.919] Sample: 18%|#7 | 186/1050 [00:00, 172.91it/s, step size=1.11e+00, acc. rate=0.920] Sample: 18%|#7 | 187/1050 [00:00, 172.91it/s, step size=1.11e+00, acc. rate=0.920] Sample: 18%|#7 | 188/1050 [00:00, 172.91it/s, step size=1.11e+00, acc. rate=0.921] Sample: 18%|#8 | 189/1050 [00:00, 172.91it/s, step size=1.11e+00, acc. rate=0.921] Sample: 18%|#8 | 190/1050 [00:00, 172.91it/s, step size=1.11e+00, acc. rate=0.921] Sample: 18%|#8 | 191/1050 [00:00, 172.91it/s, step size=1.11e+00, acc. rate=0.922] Sample: 18%|#8 | 192/1050 [00:00, 172.91it/s, step size=1.11e+00, acc. rate=0.922] Sample: 18%|#8 | 193/1050 [00:00, 172.91it/s, step size=1.11e+00, acc. rate=0.923] Sample: 18%|#8 | 194/1050 [00:00, 172.91it/s, step size=1.11e+00, acc. rate=0.923] Sample: 19%|#8 | 195/1050 [00:00, 172.91it/s, step size=1.11e+00, acc. rate=0.923] Sample: 19%|#8 | 196/1050 [00:00, 172.91it/s, step size=1.11e+00, acc. rate=0.924] Sample: 19%|#8 | 197/1050 [00:00, 172.91it/s, step size=1.11e+00, acc. rate=0.924] Sample: 19%|#8 | 198/1050 [00:00, 172.91it/s, step size=1.11e+00, acc. rate=0.925] Sample: 19%|#8 | 199/1050 [00:00, 172.91it/s, step size=1.11e+00, acc. rate=0.925] Sample: 19%|#9 | 200/1050 [00:00, 172.91it/s, step size=1.11e+00, acc. rate=0.925] Sample: 19%|#9 | 201/1050 [00:00, 172.91it/s, step size=1.11e+00, acc. rate=0.926] Sample: 19%|#9 | 202/1050 [00:00, 172.91it/s, step size=1.11e+00, acc. rate=0.926] Sample: 19%|#9 | 203/1050 [00:00, 172.91it/s, step size=1.11e+00, acc. rate=0.926] Sample: 19%|#9 | 204/1050 [00:00, 172.91it/s, step size=1.11e+00, acc. rate=0.927] Sample: 20%|#9 | 205/1050 [00:00, 172.91it/s, step size=1.11e+00, acc. rate=0.927] Sample: 20%|#9 | 206/1050 [00:00, 172.91it/s, step size=1.11e+00, acc. rate=0.928] Sample: 20%|#9 | 207/1050 [00:00, 172.91it/s, step size=1.11e+00, acc. rate=0.928] Sample: 20%|#9 | 208/1050 [00:00, 172.91it/s, step size=1.11e+00, acc. rate=0.928] Sample: 20%|#9 | 209/1050 [00:00, 172.91it/s, step size=1.11e+00, acc. rate=0.929] Sample: 20%|## | 210/1050 [00:00, 172.91it/s, step size=1.11e+00, acc. rate=0.929] Sample: 20%|## | 211/1050 [00:00, 172.91it/s, step size=1.11e+00, acc. rate=0.929] Sample: 20%|## | 212/1050 [00:00, 172.91it/s, step size=1.11e+00, acc. rate=0.930] Sample: 20%|## | 213/1050 [00:00, 172.91it/s, step size=1.11e+00, acc. rate=0.930] Sample: 20%|## | 214/1050 [00:00, 172.91it/s, step size=1.11e+00, acc. rate=0.930] Sample: 20%|## | 215/1050 [00:00, 172.91it/s, step size=1.11e+00, acc. rate=0.931] Sample: 21%|## | 216/1050 [00:00, 172.91it/s, step size=1.11e+00, acc. rate=0.931] Sample: 21%|## | 217/1050 [00:00, 172.91it/s, step size=1.11e+00, acc. rate=0.931] Sample: 21%|## | 218/1050 [00:00, 172.91it/s, step size=1.11e+00, acc. rate=0.932] Sample: 21%|## | 219/1050 [00:00, 172.91it/s, step size=1.11e+00, acc. rate=0.932] Sample: 21%|## | 220/1050 [00:00, 205.65it/s, step size=1.11e+00, acc. rate=0.932] Sample: 21%|## | 220/1050 [00:00, 205.65it/s, step size=1.11e+00, acc. rate=0.932] Sample: 21%|##1 | 221/1050 [00:00, 205.65it/s, step size=1.11e+00, acc. rate=0.932] Sample: 21%|##1 | 222/1050 [00:00, 205.65it/s, step size=1.11e+00, acc. rate=0.933] Sample: 21%|##1 | 223/1050 [00:00, 205.65it/s, step size=1.11e+00, acc. rate=0.933] Sample: 21%|##1 | 224/1050 [00:00, 205.65it/s, step size=1.11e+00, acc. rate=0.933] Sample: 21%|##1 | 225/1050 [00:00, 205.65it/s, step size=1.11e+00, acc. rate=0.934] Sample: 22%|##1 | 226/1050 [00:00, 205.65it/s, step size=1.11e+00, acc. rate=0.934] Sample: 22%|##1 | 227/1050 [00:00, 205.65it/s, step size=1.11e+00, acc. rate=0.934] Sample: 22%|##1 | 228/1050 [00:00, 205.65it/s, step size=1.11e+00, acc. rate=0.934] Sample: 22%|##1 | 229/1050 [00:00, 205.65it/s, step size=1.11e+00, acc. rate=0.935] Sample: 22%|##1 | 230/1050 [00:00, 205.65it/s, step size=1.11e+00, acc. rate=0.935] Sample: 22%|##2 | 231/1050 [00:00, 205.65it/s, step size=1.11e+00, acc. rate=0.935] Sample: 22%|##2 | 232/1050 [00:00, 205.65it/s, step size=1.11e+00, acc. rate=0.936] Sample: 22%|##2 | 233/1050 [00:00, 205.65it/s, step size=1.11e+00, acc. rate=0.936] Sample: 22%|##2 | 234/1050 [00:00, 205.65it/s, step size=1.11e+00, acc. rate=0.932] Sample: 22%|##2 | 235/1050 [00:00, 205.65it/s, step size=1.11e+00, acc. rate=0.932] Sample: 22%|##2 | 236/1050 [00:00, 205.65it/s, step size=1.11e+00, acc. rate=0.932] Sample: 23%|##2 | 237/1050 [00:00, 205.65it/s, step size=1.11e+00, acc. rate=0.933] Sample: 23%|##2 | 238/1050 [00:00, 205.65it/s, step size=1.11e+00, acc. rate=0.933] Sample: 23%|##2 | 239/1050 [00:01, 205.65it/s, step size=1.11e+00, acc. rate=0.929] Sample: 23%|##2 | 240/1050 [00:01, 205.65it/s, step size=1.11e+00, acc. rate=0.929] Sample: 23%|##2 | 241/1050 [00:01, 205.65it/s, step size=1.11e+00, acc. rate=0.930] Sample: 23%|##3 | 242/1050 [00:01, 205.65it/s, step size=1.11e+00, acc. rate=0.930] Sample: 23%|##3 | 243/1050 [00:01, 205.65it/s, step size=1.11e+00, acc. rate=0.930] Sample: 23%|##3 | 244/1050 [00:01, 205.65it/s, step size=1.11e+00, acc. rate=0.931] Sample: 23%|##3 | 245/1050 [00:01, 205.65it/s, step size=1.11e+00, acc. rate=0.931] Sample: 23%|##3 | 246/1050 [00:01, 205.65it/s, step size=1.11e+00, acc. rate=0.931] Sample: 24%|##3 | 247/1050 [00:01, 205.65it/s, step size=1.11e+00, acc. rate=0.931] Sample: 24%|##3 | 248/1050 [00:01, 205.65it/s, step size=1.11e+00, acc. rate=0.932] Sample: 24%|##3 | 249/1050 [00:01, 205.65it/s, step size=1.11e+00, acc. rate=0.932] Sample: 24%|##3 | 250/1050 [00:01, 205.65it/s, step size=1.11e+00, acc. rate=0.932] Sample: 24%|##3 | 251/1050 [00:01, 205.65it/s, step size=1.11e+00, acc. rate=0.933] Sample: 24%|##4 | 252/1050 [00:01, 205.65it/s, step size=1.11e+00, acc. rate=0.933] Sample: 24%|##4 | 253/1050 [00:01, 205.65it/s, step size=1.11e+00, acc. rate=0.933] Sample: 24%|##4 | 254/1050 [00:01, 205.65it/s, step size=1.11e+00, acc. rate=0.933] Sample: 24%|##4 | 255/1050 [00:01, 205.65it/s, step size=1.11e+00, acc. rate=0.934] Sample: 24%|##4 | 256/1050 [00:01, 205.65it/s, step size=1.11e+00, acc. rate=0.934] Sample: 24%|##4 | 257/1050 [00:01, 236.96it/s, step size=1.11e+00, acc. rate=0.934] Sample: 24%|##4 | 257/1050 [00:01, 236.96it/s, step size=1.11e+00, acc. rate=0.934] Sample: 25%|##4 | 258/1050 [00:01, 236.96it/s, step size=1.11e+00, acc. rate=0.934] Sample: 25%|##4 | 259/1050 [00:01, 236.96it/s, step size=1.11e+00, acc. rate=0.935] Sample: 25%|##4 | 260/1050 [00:01, 236.96it/s, step size=1.11e+00, acc. rate=0.935] Sample: 25%|##4 | 261/1050 [00:01, 236.96it/s, step size=1.11e+00, acc. rate=0.935] Sample: 25%|##4 | 262/1050 [00:01, 236.96it/s, step size=1.11e+00, acc. rate=0.935] Sample: 25%|##5 | 263/1050 [00:01, 236.96it/s, step size=1.11e+00, acc. rate=0.936] Sample: 25%|##5 | 264/1050 [00:01, 236.96it/s, step size=1.11e+00, acc. rate=0.936] Sample: 25%|##5 | 265/1050 [00:01, 236.96it/s, step size=1.11e+00, acc. rate=0.936] Sample: 25%|##5 | 266/1050 [00:01, 236.96it/s, step size=1.11e+00, acc. rate=0.936] Sample: 25%|##5 | 267/1050 [00:01, 236.96it/s, step size=1.11e+00, acc. rate=0.937] Sample: 26%|##5 | 268/1050 [00:01, 236.96it/s, step size=1.11e+00, acc. rate=0.933] Sample: 26%|##5 | 269/1050 [00:01, 236.96it/s, step size=1.11e+00, acc. rate=0.933] Sample: 26%|##5 | 270/1050 [00:01, 236.96it/s, step size=1.11e+00, acc. rate=0.934] Sample: 26%|##5 | 271/1050 [00:01, 236.96it/s, step size=1.11e+00, acc. rate=0.934] Sample: 26%|##5 | 272/1050 [00:01, 236.96it/s, step size=1.11e+00, acc. rate=0.934] Sample: 26%|##6 | 273/1050 [00:01, 236.96it/s, step size=1.11e+00, acc. rate=0.934] Sample: 26%|##6 | 274/1050 [00:01, 236.96it/s, step size=1.11e+00, acc. rate=0.935] Sample: 26%|##6 | 275/1050 [00:01, 236.96it/s, step size=1.11e+00, acc. rate=0.935] Sample: 26%|##6 | 276/1050 [00:01, 236.96it/s, step size=1.11e+00, acc. rate=0.935] Sample: 26%|##6 | 277/1050 [00:01, 236.96it/s, step size=1.11e+00, acc. rate=0.935] Sample: 26%|##6 | 278/1050 [00:01, 236.96it/s, step size=1.11e+00, acc. rate=0.935] Sample: 27%|##6 | 279/1050 [00:01, 236.96it/s, step size=1.11e+00, acc. rate=0.936] Sample: 27%|##6 | 280/1050 [00:01, 236.96it/s, step size=1.11e+00, acc. rate=0.936] Sample: 27%|##6 | 281/1050 [00:01, 236.96it/s, step size=1.11e+00, acc. rate=0.936] Sample: 27%|##6 | 282/1050 [00:01, 236.96it/s, step size=1.11e+00, acc. rate=0.936] Sample: 27%|##6 | 283/1050 [00:01, 236.96it/s, step size=1.11e+00, acc. rate=0.937] Sample: 27%|##7 | 284/1050 [00:01, 236.96it/s, step size=1.11e+00, acc. rate=0.937] Sample: 27%|##7 | 285/1050 [00:01, 236.96it/s, step size=1.11e+00, acc. rate=0.937] Sample: 27%|##7 | 286/1050 [00:01, 236.96it/s, step size=1.11e+00, acc. rate=0.937] Sample: 27%|##7 | 287/1050 [00:01, 236.96it/s, step size=1.11e+00, acc. rate=0.938] Sample: 27%|##7 | 288/1050 [00:01, 236.96it/s, step size=1.11e+00, acc. rate=0.938] Sample: 28%|##7 | 289/1050 [00:01, 236.96it/s, step size=1.11e+00, acc. rate=0.938] Sample: 28%|##7 | 290/1050 [00:01, 236.96it/s, step size=1.11e+00, acc. rate=0.938] Sample: 28%|##7 | 291/1050 [00:01, 236.96it/s, step size=1.11e+00, acc. rate=0.938] Sample: 28%|##7 | 292/1050 [00:01, 236.96it/s, step size=1.11e+00, acc. rate=0.939] Sample: 28%|##7 | 293/1050 [00:01, 263.59it/s, step size=1.11e+00, acc. rate=0.939] Sample: 28%|##7 | 293/1050 [00:01, 263.59it/s, step size=1.11e+00, acc. rate=0.939] Sample: 28%|##8 | 294/1050 [00:01, 263.59it/s, step size=1.11e+00, acc. rate=0.939] Sample: 28%|##8 | 295/1050 [00:01, 263.59it/s, step size=1.11e+00, acc. rate=0.939] Sample: 28%|##8 | 296/1050 [00:01, 263.59it/s, step size=1.11e+00, acc. rate=0.939] Sample: 28%|##8 | 297/1050 [00:01, 263.59it/s, step size=1.11e+00, acc. rate=0.940] Sample: 28%|##8 | 298/1050 [00:01, 263.59it/s, step size=1.11e+00, acc. rate=0.940] Sample: 28%|##8 | 299/1050 [00:01, 263.59it/s, step size=1.11e+00, acc. rate=0.940] Sample: 29%|##8 | 300/1050 [00:01, 263.59it/s, step size=1.11e+00, acc. rate=0.940] Sample: 29%|##8 | 301/1050 [00:01, 263.59it/s, step size=1.11e+00, acc. rate=0.940] Sample: 29%|##8 | 302/1050 [00:01, 263.59it/s, step size=1.11e+00, acc. rate=0.941] Sample: 29%|##8 | 303/1050 [00:01, 263.59it/s, step size=1.11e+00, acc. rate=0.941] Sample: 29%|##8 | 304/1050 [00:01, 263.59it/s, step size=1.11e+00, acc. rate=0.941] Sample: 29%|##9 | 305/1050 [00:01, 263.59it/s, step size=1.11e+00, acc. rate=0.941] Sample: 29%|##9 | 306/1050 [00:01, 263.59it/s, step size=1.11e+00, acc. rate=0.941] Sample: 29%|##9 | 307/1050 [00:01, 263.59it/s, step size=1.11e+00, acc. rate=0.942] Sample: 29%|##9 | 308/1050 [00:01, 263.59it/s, step size=1.11e+00, acc. rate=0.942] Sample: 29%|##9 | 309/1050 [00:01, 263.59it/s, step size=1.11e+00, acc. rate=0.942] Sample: 30%|##9 | 310/1050 [00:01, 263.59it/s, step size=1.11e+00, acc. rate=0.942] Sample: 30%|##9 | 311/1050 [00:01, 263.59it/s, step size=1.11e+00, acc. rate=0.942] Sample: 30%|##9 | 312/1050 [00:01, 263.59it/s, step size=1.11e+00, acc. rate=0.942] Sample: 30%|##9 | 313/1050 [00:01, 263.59it/s, step size=1.11e+00, acc. rate=0.943] Sample: 30%|##9 | 314/1050 [00:01, 263.59it/s, step size=1.11e+00, acc. rate=0.943] Sample: 30%|### | 315/1050 [00:01, 263.59it/s, step size=1.11e+00, acc. rate=0.943] Sample: 30%|### | 316/1050 [00:01, 263.59it/s, step size=1.11e+00, acc. rate=0.943] Sample: 30%|### | 317/1050 [00:01, 263.59it/s, step size=1.11e+00, acc. rate=0.943] Sample: 30%|### | 318/1050 [00:01, 263.59it/s, step size=1.11e+00, acc. rate=0.944] Sample: 30%|### | 319/1050 [00:01, 263.59it/s, step size=1.11e+00, acc. rate=0.944] Sample: 30%|### | 320/1050 [00:01, 263.59it/s, step size=1.11e+00, acc. rate=0.944] Sample: 31%|### | 321/1050 [00:01, 263.59it/s, step size=1.11e+00, acc. rate=0.941] Sample: 31%|### | 322/1050 [00:01, 263.59it/s, step size=1.11e+00, acc. rate=0.941] Sample: 31%|### | 323/1050 [00:01, 263.59it/s, step size=1.11e+00, acc. rate=0.941] Sample: 31%|### | 324/1050 [00:01, 263.59it/s, step size=1.11e+00, acc. rate=0.942] Sample: 31%|### | 325/1050 [00:01, 263.59it/s, step size=1.11e+00, acc. rate=0.942] Sample: 31%|###1 | 326/1050 [00:01, 263.59it/s, step size=1.11e+00, acc. rate=0.942] Sample: 31%|###1 | 327/1050 [00:01, 263.59it/s, step size=1.11e+00, acc. rate=0.942] Sample: 31%|###1 | 328/1050 [00:01, 263.59it/s, step size=1.11e+00, acc. rate=0.942] Sample: 31%|###1 | 329/1050 [00:01, 263.59it/s, step size=1.11e+00, acc. rate=0.942] Sample: 31%|###1 | 330/1050 [00:01, 287.68it/s, step size=1.11e+00, acc. rate=0.942] Sample: 31%|###1 | 330/1050 [00:01, 287.68it/s, step size=1.11e+00, acc. rate=0.943] Sample: 32%|###1 | 331/1050 [00:01, 287.68it/s, step size=1.11e+00, acc. rate=0.943] Sample: 32%|###1 | 332/1050 [00:01, 287.68it/s, step size=1.11e+00, acc. rate=0.943] Sample: 32%|###1 | 333/1050 [00:01, 287.68it/s, step size=1.11e+00, acc. rate=0.943] Sample: 32%|###1 | 334/1050 [00:01, 287.68it/s, step size=1.11e+00, acc. rate=0.943] Sample: 32%|###1 | 335/1050 [00:01, 287.68it/s, step size=1.11e+00, acc. rate=0.943] Sample: 32%|###2 | 336/1050 [00:01, 287.68it/s, step size=1.11e+00, acc. rate=0.944] Sample: 32%|###2 | 337/1050 [00:01, 287.68it/s, step size=1.11e+00, acc. rate=0.944] Sample: 32%|###2 | 338/1050 [00:01, 287.68it/s, step size=1.11e+00, acc. rate=0.944] Sample: 32%|###2 | 339/1050 [00:01, 287.68it/s, step size=1.11e+00, acc. rate=0.944] Sample: 32%|###2 | 340/1050 [00:01, 287.68it/s, step size=1.11e+00, acc. rate=0.944] Sample: 32%|###2 | 341/1050 [00:01, 287.68it/s, step size=1.11e+00, acc. rate=0.944] Sample: 33%|###2 | 342/1050 [00:01, 287.68it/s, step size=1.11e+00, acc. rate=0.945] Sample: 33%|###2 | 343/1050 [00:01, 287.68it/s, step size=1.11e+00, acc. rate=0.945] Sample: 33%|###2 | 344/1050 [00:01, 287.68it/s, step size=1.11e+00, acc. rate=0.945] Sample: 33%|###2 | 345/1050 [00:01, 287.68it/s, step size=1.11e+00, acc. rate=0.945] Sample: 33%|###2 | 346/1050 [00:01, 287.68it/s, step size=1.11e+00, acc. rate=0.945] Sample: 33%|###3 | 347/1050 [00:01, 287.68it/s, step size=1.11e+00, acc. rate=0.945] Sample: 33%|###3 | 348/1050 [00:01, 287.68it/s, step size=1.11e+00, acc. rate=0.946] Sample: 33%|###3 | 349/1050 [00:01, 287.68it/s, step size=1.11e+00, acc. rate=0.943] Sample: 33%|###3 | 350/1050 [00:01, 287.68it/s, step size=1.11e+00, acc. rate=0.943] Sample: 33%|###3 | 351/1050 [00:01, 287.68it/s, step size=1.11e+00, acc. rate=0.943] Sample: 34%|###3 | 352/1050 [00:01, 287.68it/s, step size=1.11e+00, acc. rate=0.943] Sample: 34%|###3 | 353/1050 [00:01, 287.68it/s, step size=1.11e+00, acc. rate=0.944] Sample: 34%|###3 | 354/1050 [00:01, 287.68it/s, step size=1.11e+00, acc. rate=0.944] Sample: 34%|###3 | 355/1050 [00:01, 287.68it/s, step size=1.11e+00, acc. rate=0.944] Sample: 34%|###3 | 356/1050 [00:01, 287.68it/s, step size=1.11e+00, acc. rate=0.944] Sample: 34%|###4 | 357/1050 [00:01, 287.68it/s, step size=1.11e+00, acc. rate=0.944] Sample: 34%|###4 | 358/1050 [00:01, 287.68it/s, step size=1.11e+00, acc. rate=0.944] Sample: 34%|###4 | 359/1050 [00:01, 287.68it/s, step size=1.11e+00, acc. rate=0.944] Sample: 34%|###4 | 360/1050 [00:01, 287.68it/s, step size=1.11e+00, acc. rate=0.945] Sample: 34%|###4 | 361/1050 [00:01, 287.68it/s, step size=1.11e+00, acc. rate=0.945] Sample: 34%|###4 | 362/1050 [00:01, 287.68it/s, step size=1.11e+00, acc. rate=0.945] Sample: 35%|###4 | 363/1050 [00:01, 287.68it/s, step size=1.11e+00, acc. rate=0.945] Sample: 35%|###4 | 364/1050 [00:01, 287.68it/s, step size=1.11e+00, acc. rate=0.945] Sample: 35%|###4 | 365/1050 [00:01, 298.28it/s, step size=1.11e+00, acc. rate=0.945] Sample: 35%|###4 | 365/1050 [00:01, 298.28it/s, step size=1.11e+00, acc. rate=0.945] Sample: 35%|###4 | 366/1050 [00:01, 298.28it/s, step size=1.11e+00, acc. rate=0.946] Sample: 35%|###4 | 367/1050 [00:01, 298.28it/s, step size=1.11e+00, acc. rate=0.946] Sample: 35%|###5 | 368/1050 [00:01, 298.28it/s, step size=1.11e+00, acc. rate=0.946] Sample: 35%|###5 | 369/1050 [00:01, 298.28it/s, step size=1.11e+00, acc. rate=0.946] Sample: 35%|###5 | 370/1050 [00:01, 298.28it/s, step size=1.11e+00, acc. rate=0.946] Sample: 35%|###5 | 371/1050 [00:01, 298.28it/s, step size=1.11e+00, acc. rate=0.946] Sample: 35%|###5 | 372/1050 [00:01, 298.28it/s, step size=1.11e+00, acc. rate=0.946] Sample: 36%|###5 | 373/1050 [00:01, 298.28it/s, step size=1.11e+00, acc. rate=0.947] Sample: 36%|###5 | 374/1050 [00:01, 298.28it/s, step size=1.11e+00, acc. rate=0.947] Sample: 36%|###5 | 375/1050 [00:01, 298.28it/s, step size=1.11e+00, acc. rate=0.947] Sample: 36%|###5 | 376/1050 [00:01, 298.28it/s, step size=1.11e+00, acc. rate=0.947] Sample: 36%|###5 | 377/1050 [00:01, 298.28it/s, step size=1.11e+00, acc. rate=0.947] Sample: 36%|###6 | 378/1050 [00:01, 298.28it/s, step size=1.11e+00, acc. rate=0.947] Sample: 36%|###6 | 379/1050 [00:01, 298.28it/s, step size=1.11e+00, acc. rate=0.947] Sample: 36%|###6 | 380/1050 [00:01, 298.28it/s, step size=1.11e+00, acc. rate=0.948] Sample: 36%|###6 | 381/1050 [00:01, 298.28it/s, step size=1.11e+00, acc. rate=0.948] Sample: 36%|###6 | 382/1050 [00:01, 298.28it/s, step size=1.11e+00, acc. rate=0.948] Sample: 36%|###6 | 383/1050 [00:01, 298.28it/s, step size=1.11e+00, acc. rate=0.948] Sample: 37%|###6 | 384/1050 [00:01, 298.28it/s, step size=1.11e+00, acc. rate=0.948] Sample: 37%|###6 | 385/1050 [00:01, 298.28it/s, step size=1.11e+00, acc. rate=0.948] Sample: 37%|###6 | 386/1050 [00:01, 298.28it/s, step size=1.11e+00, acc. rate=0.948] Sample: 37%|###6 | 387/1050 [00:01, 298.28it/s, step size=1.11e+00, acc. rate=0.948] Sample: 37%|###6 | 388/1050 [00:01, 298.28it/s, step size=1.11e+00, acc. rate=0.949] Sample: 37%|###7 | 389/1050 [00:01, 298.28it/s, step size=1.11e+00, acc. rate=0.949] Sample: 37%|###7 | 390/1050 [00:01, 298.28it/s, step size=1.11e+00, acc. rate=0.949] Sample: 37%|###7 | 391/1050 [00:01, 298.28it/s, step size=1.11e+00, acc. rate=0.949] Sample: 37%|###7 | 392/1050 [00:01, 298.28it/s, step size=1.11e+00, acc. rate=0.949] Sample: 37%|###7 | 393/1050 [00:01, 298.28it/s, step size=1.11e+00, acc. rate=0.949] Sample: 38%|###7 | 394/1050 [00:01, 298.28it/s, step size=1.11e+00, acc. rate=0.949] Sample: 38%|###7 | 395/1050 [00:01, 298.28it/s, step size=1.11e+00, acc. rate=0.949] Sample: 38%|###7 | 396/1050 [00:01, 298.28it/s, step size=1.11e+00, acc. rate=0.950] Sample: 38%|###7 | 397/1050 [00:01, 298.28it/s, step size=1.11e+00, acc. rate=0.950] Sample: 38%|###7 | 398/1050 [00:01, 298.28it/s, step size=1.11e+00, acc. rate=0.950] Sample: 38%|###8 | 399/1050 [00:01, 298.28it/s, step size=1.11e+00, acc. rate=0.950] Sample: 38%|###8 | 400/1050 [00:01, 298.28it/s, step size=1.11e+00, acc. rate=0.950] Sample: 38%|###8 | 401/1050 [00:01, 312.25it/s, step size=1.11e+00, acc. rate=0.950] Sample: 38%|###8 | 401/1050 [00:01, 312.25it/s, step size=1.11e+00, acc. rate=0.950] Sample: 38%|###8 | 402/1050 [00:01, 312.25it/s, step size=1.11e+00, acc. rate=0.950] Sample: 38%|###8 | 403/1050 [00:01, 312.25it/s, step size=1.11e+00, acc. rate=0.950] Sample: 38%|###8 | 404/1050 [00:01, 312.25it/s, step size=1.11e+00, acc. rate=0.951] Sample: 39%|###8 | 405/1050 [00:01, 312.25it/s, step size=1.11e+00, acc. rate=0.951] Sample: 39%|###8 | 406/1050 [00:01, 312.25it/s, step size=1.11e+00, acc. rate=0.951] Sample: 39%|###8 | 407/1050 [00:01, 312.25it/s, step size=1.11e+00, acc. rate=0.951] Sample: 39%|###8 | 408/1050 [00:01, 312.25it/s, step size=1.11e+00, acc. rate=0.951] Sample: 39%|###8 | 409/1050 [00:01, 312.25it/s, step size=1.11e+00, acc. rate=0.951] Sample: 39%|###9 | 410/1050 [00:01, 312.25it/s, step size=1.11e+00, acc. rate=0.951] Sample: 39%|###9 | 411/1050 [00:01, 312.25it/s, step size=1.11e+00, acc. rate=0.951] Sample: 39%|###9 | 412/1050 [00:01, 312.25it/s, step size=1.11e+00, acc. rate=0.952] Sample: 39%|###9 | 413/1050 [00:01, 312.25it/s, step size=1.11e+00, acc. rate=0.952] Sample: 39%|###9 | 414/1050 [00:01, 312.25it/s, step size=1.11e+00, acc. rate=0.952] Sample: 40%|###9 | 415/1050 [00:01, 312.25it/s, step size=1.11e+00, acc. rate=0.952] Sample: 40%|###9 | 416/1050 [00:01, 312.25it/s, step size=1.11e+00, acc. rate=0.952] Sample: 40%|###9 | 417/1050 [00:01, 312.25it/s, step size=1.11e+00, acc. rate=0.952] Sample: 40%|###9 | 418/1050 [00:01, 312.25it/s, step size=1.11e+00, acc. rate=0.952] Sample: 40%|###9 | 419/1050 [00:01, 312.25it/s, step size=1.11e+00, acc. rate=0.952] Sample: 40%|#### | 420/1050 [00:01, 312.25it/s, step size=1.11e+00, acc. rate=0.952] Sample: 40%|#### | 421/1050 [00:01, 312.25it/s, step size=1.11e+00, acc. rate=0.953] Sample: 40%|#### | 422/1050 [00:01, 312.25it/s, step size=1.11e+00, acc. rate=0.953] Sample: 40%|#### | 423/1050 [00:01, 312.25it/s, step size=1.11e+00, acc. rate=0.953] Sample: 40%|#### | 424/1050 [00:01, 312.25it/s, step size=1.11e+00, acc. rate=0.953] Sample: 40%|#### | 425/1050 [00:01, 312.25it/s, step size=1.11e+00, acc. rate=0.953] Sample: 41%|#### | 426/1050 [00:01, 312.25it/s, step size=1.11e+00, acc. rate=0.953] Sample: 41%|#### | 427/1050 [00:01, 312.25it/s, step size=1.11e+00, acc. rate=0.953] Sample: 41%|#### | 428/1050 [00:01, 312.25it/s, step size=1.11e+00, acc. rate=0.953] Sample: 41%|#### | 429/1050 [00:01, 312.25it/s, step size=1.11e+00, acc. rate=0.953] Sample: 41%|#### | 430/1050 [00:01, 312.25it/s, step size=1.11e+00, acc. rate=0.954] Sample: 41%|####1 | 431/1050 [00:01, 312.25it/s, step size=1.11e+00, acc. rate=0.954] Sample: 41%|####1 | 432/1050 [00:01, 312.25it/s, step size=1.11e+00, acc. rate=0.954] Sample: 41%|####1 | 433/1050 [00:01, 312.25it/s, step size=1.11e+00, acc. rate=0.954] Sample: 41%|####1 | 434/1050 [00:01, 312.25it/s, step size=1.11e+00, acc. rate=0.954] Sample: 41%|####1 | 435/1050 [00:01, 312.25it/s, step size=1.11e+00, acc. rate=0.954] Sample: 42%|####1 | 436/1050 [00:01, 312.25it/s, step size=1.11e+00, acc. rate=0.954] Sample: 42%|####1 | 437/1050 [00:01, 324.00it/s, step size=1.11e+00, acc. rate=0.954] Sample: 42%|####1 | 437/1050 [00:01, 324.00it/s, step size=1.11e+00, acc. rate=0.954] Sample: 42%|####1 | 438/1050 [00:01, 324.00it/s, step size=1.11e+00, acc. rate=0.954] Sample: 42%|####1 | 439/1050 [00:01, 324.00it/s, step size=1.11e+00, acc. rate=0.955] Sample: 42%|####1 | 440/1050 [00:01, 324.00it/s, step size=1.11e+00, acc. rate=0.955] Sample: 42%|####2 | 441/1050 [00:01, 324.00it/s, step size=1.11e+00, acc. rate=0.955] Sample: 42%|####2 | 442/1050 [00:01, 324.00it/s, step size=1.11e+00, acc. rate=0.955] Sample: 42%|####2 | 443/1050 [00:01, 324.00it/s, step size=1.11e+00, acc. rate=0.955] Sample: 42%|####2 | 444/1050 [00:01, 324.00it/s, step size=1.11e+00, acc. rate=0.955] Sample: 42%|####2 | 445/1050 [00:01, 324.00it/s, step size=1.11e+00, acc. rate=0.955] Sample: 42%|####2 | 446/1050 [00:01, 324.00it/s, step size=1.11e+00, acc. rate=0.955] Sample: 43%|####2 | 447/1050 [00:01, 324.00it/s, step size=1.11e+00, acc. rate=0.955] Sample: 43%|####2 | 448/1050 [00:01, 324.00it/s, step size=1.11e+00, acc. rate=0.955] Sample: 43%|####2 | 449/1050 [00:01, 324.00it/s, step size=1.11e+00, acc. rate=0.956] Sample: 43%|####2 | 450/1050 [00:01, 324.00it/s, step size=1.11e+00, acc. rate=0.956] Sample: 43%|####2 | 451/1050 [00:01, 324.00it/s, step size=1.11e+00, acc. rate=0.956] Sample: 43%|####3 | 452/1050 [00:01, 324.00it/s, step size=1.11e+00, acc. rate=0.956] Sample: 43%|####3 | 453/1050 [00:01, 324.00it/s, step size=1.11e+00, acc. rate=0.956] Sample: 43%|####3 | 454/1050 [00:01, 324.00it/s, step size=1.11e+00, acc. rate=0.956] Sample: 43%|####3 | 455/1050 [00:01, 324.00it/s, step size=1.11e+00, acc. rate=0.956] Sample: 43%|####3 | 456/1050 [00:01, 324.00it/s, step size=1.11e+00, acc. rate=0.956] Sample: 44%|####3 | 457/1050 [00:01, 324.00it/s, step size=1.11e+00, acc. rate=0.956] Sample: 44%|####3 | 458/1050 [00:01, 324.00it/s, step size=1.11e+00, acc. rate=0.956] Sample: 44%|####3 | 459/1050 [00:01, 324.00it/s, step size=1.11e+00, acc. rate=0.957] Sample: 44%|####3 | 460/1050 [00:01, 324.00it/s, step size=1.11e+00, acc. rate=0.957] Sample: 44%|####3 | 461/1050 [00:01, 324.00it/s, step size=1.11e+00, acc. rate=0.957] Sample: 44%|####4 | 462/1050 [00:01, 324.00it/s, step size=1.11e+00, acc. rate=0.957] Sample: 44%|####4 | 463/1050 [00:01, 324.00it/s, step size=1.11e+00, acc. rate=0.957] Sample: 44%|####4 | 464/1050 [00:01, 324.00it/s, step size=1.11e+00, acc. rate=0.957] Sample: 44%|####4 | 465/1050 [00:01, 324.00it/s, step size=1.11e+00, acc. rate=0.957] Sample: 44%|####4 | 466/1050 [00:01, 324.00it/s, step size=1.11e+00, acc. rate=0.957] Sample: 44%|####4 | 467/1050 [00:01, 324.00it/s, step size=1.11e+00, acc. rate=0.957] Sample: 45%|####4 | 468/1050 [00:01, 324.00it/s, step size=1.11e+00, acc. rate=0.957] Sample: 45%|####4 | 469/1050 [00:01, 324.00it/s, step size=1.11e+00, acc. rate=0.957] Sample: 45%|####4 | 470/1050 [00:01, 324.00it/s, step size=1.11e+00, acc. rate=0.958] Sample: 45%|####4 | 471/1050 [00:01, 324.00it/s, step size=1.11e+00, acc. rate=0.958] Sample: 45%|####4 | 472/1050 [00:01, 324.00it/s, step size=1.11e+00, acc. rate=0.956] Sample: 45%|####5 | 473/1050 [00:01, 332.78it/s, step size=1.11e+00, acc. rate=0.956] Sample: 45%|####5 | 473/1050 [00:01, 332.78it/s, step size=1.11e+00, acc. rate=0.956] Sample: 45%|####5 | 474/1050 [00:01, 332.78it/s, step size=1.11e+00, acc. rate=0.956] Sample: 45%|####5 | 475/1050 [00:01, 332.78it/s, step size=1.11e+00, acc. rate=0.956] Sample: 45%|####5 | 476/1050 [00:01, 332.78it/s, step size=1.11e+00, acc. rate=0.956] Sample: 45%|####5 | 477/1050 [00:01, 332.78it/s, step size=1.11e+00, acc. rate=0.956] Sample: 46%|####5 | 478/1050 [00:01, 332.78it/s, step size=1.11e+00, acc. rate=0.956] Sample: 46%|####5 | 479/1050 [00:01, 332.78it/s, step size=1.11e+00, acc. rate=0.956] Sample: 46%|####5 | 480/1050 [00:01, 332.78it/s, step size=1.11e+00, acc. rate=0.956] Sample: 46%|####5 | 481/1050 [00:01, 332.78it/s, step size=1.11e+00, acc. rate=0.956] Sample: 46%|####5 | 482/1050 [00:01, 332.78it/s, step size=1.11e+00, acc. rate=0.957] Sample: 46%|####6 | 483/1050 [00:01, 332.78it/s, step size=1.11e+00, acc. rate=0.957] Sample: 46%|####6 | 484/1050 [00:01, 332.78it/s, step size=1.11e+00, acc. rate=0.957] Sample: 46%|####6 | 485/1050 [00:01, 332.78it/s, step size=1.11e+00, acc. rate=0.957] Sample: 46%|####6 | 486/1050 [00:01, 332.78it/s, step size=1.11e+00, acc. rate=0.957] Sample: 46%|####6 | 487/1050 [00:01, 332.78it/s, step size=1.11e+00, acc. rate=0.957] Sample: 46%|####6 | 488/1050 [00:01, 332.78it/s, step size=1.11e+00, acc. rate=0.957] Sample: 47%|####6 | 489/1050 [00:01, 332.78it/s, step size=1.11e+00, acc. rate=0.957] Sample: 47%|####6 | 490/1050 [00:01, 332.78it/s, step size=1.11e+00, acc. rate=0.957] Sample: 47%|####6 | 491/1050 [00:01, 332.78it/s, step size=1.11e+00, acc. rate=0.957] Sample: 47%|####6 | 492/1050 [00:01, 332.78it/s, step size=1.11e+00, acc. rate=0.957] Sample: 47%|####6 | 493/1050 [00:01, 332.78it/s, step size=1.11e+00, acc. rate=0.957] Sample: 47%|####7 | 494/1050 [00:01, 332.78it/s, step size=1.11e+00, acc. rate=0.958] Sample: 47%|####7 | 495/1050 [00:01, 332.78it/s, step size=1.11e+00, acc. rate=0.958] Sample: 47%|####7 | 496/1050 [00:01, 332.78it/s, step size=1.11e+00, acc. rate=0.958] Sample: 47%|####7 | 497/1050 [00:01, 332.78it/s, step size=1.11e+00, acc. rate=0.958] Sample: 47%|####7 | 498/1050 [00:01, 332.78it/s, step size=1.11e+00, acc. rate=0.958] Sample: 48%|####7 | 499/1050 [00:01, 332.78it/s, step size=1.11e+00, acc. rate=0.958] Sample: 48%|####7 | 500/1050 [00:01, 332.78it/s, step size=1.11e+00, acc. rate=0.958] Sample: 48%|####7 | 501/1050 [00:01, 332.78it/s, step size=1.11e+00, acc. rate=0.958] Sample: 48%|####7 | 502/1050 [00:01, 332.78it/s, step size=1.11e+00, acc. rate=0.958] Sample: 48%|####7 | 503/1050 [00:01, 332.78it/s, step size=1.11e+00, acc. rate=0.958] Sample: 48%|####8 | 504/1050 [00:01, 332.78it/s, step size=1.11e+00, acc. rate=0.958] Sample: 48%|####8 | 505/1050 [00:01, 332.78it/s, step size=1.11e+00, acc. rate=0.958] Sample: 48%|####8 | 506/1050 [00:01, 332.78it/s, step size=1.11e+00, acc. rate=0.959] Sample: 48%|####8 | 507/1050 [00:01, 332.78it/s, step size=1.11e+00, acc. rate=0.959] Sample: 48%|####8 | 508/1050 [00:01, 332.78it/s, step size=1.11e+00, acc. rate=0.959] Sample: 48%|####8 | 509/1050 [00:01, 332.78it/s, step size=1.11e+00, acc. rate=0.959] Sample: 49%|####8 | 510/1050 [00:01, 340.78it/s, step size=1.11e+00, acc. rate=0.959] Sample: 49%|####8 | 510/1050 [00:01, 340.78it/s, step size=1.11e+00, acc. rate=0.959] Sample: 49%|####8 | 511/1050 [00:01, 340.78it/s, step size=1.11e+00, acc. rate=0.959] Sample: 49%|####8 | 512/1050 [00:01, 340.78it/s, step size=1.11e+00, acc. rate=0.959] Sample: 49%|####8 | 513/1050 [00:01, 340.78it/s, step size=1.11e+00, acc. rate=0.957] Sample: 49%|####8 | 514/1050 [00:01, 340.78it/s, step size=1.11e+00, acc. rate=0.955] Sample: 49%|####9 | 515/1050 [00:01, 340.78it/s, step size=1.11e+00, acc. rate=0.955] Sample: 49%|####9 | 516/1050 [00:01, 340.78it/s, step size=1.11e+00, acc. rate=0.956] Sample: 49%|####9 | 517/1050 [00:01, 340.78it/s, step size=1.11e+00, acc. rate=0.956] Sample: 49%|####9 | 518/1050 [00:01, 340.78it/s, step size=1.11e+00, acc. rate=0.956] Sample: 49%|####9 | 519/1050 [00:01, 340.78it/s, step size=1.11e+00, acc. rate=0.956] Sample: 50%|####9 | 520/1050 [00:01, 340.78it/s, step size=1.11e+00, acc. rate=0.956] Sample: 50%|####9 | 521/1050 [00:01, 340.78it/s, step size=1.11e+00, acc. rate=0.956] Sample: 50%|####9 | 522/1050 [00:01, 340.78it/s, step size=1.11e+00, acc. rate=0.956] Sample: 50%|####9 | 523/1050 [00:01, 340.78it/s, step size=1.11e+00, acc. rate=0.956] Sample: 50%|####9 | 524/1050 [00:01, 340.78it/s, step size=1.11e+00, acc. rate=0.956] Sample: 50%|##### | 525/1050 [00:01, 340.78it/s, step size=1.11e+00, acc. rate=0.956] Sample: 50%|##### | 526/1050 [00:01, 340.78it/s, step size=1.11e+00, acc. rate=0.956] Sample: 50%|##### | 527/1050 [00:01, 340.78it/s, step size=1.11e+00, acc. rate=0.956] Sample: 50%|##### | 528/1050 [00:01, 340.78it/s, step size=1.11e+00, acc. rate=0.957] Sample: 50%|##### | 529/1050 [00:01, 340.78it/s, step size=1.11e+00, acc. rate=0.955] Sample: 50%|##### | 530/1050 [00:01, 340.78it/s, step size=1.11e+00, acc. rate=0.955] Sample: 51%|##### | 531/1050 [00:01, 340.78it/s, step size=1.11e+00, acc. rate=0.955] Sample: 51%|##### | 532/1050 [00:01, 340.78it/s, step size=1.11e+00, acc. rate=0.955] Sample: 51%|##### | 533/1050 [00:01, 340.78it/s, step size=1.11e+00, acc. rate=0.955] Sample: 51%|##### | 534/1050 [00:01, 340.78it/s, step size=1.11e+00, acc. rate=0.955] Sample: 51%|##### | 535/1050 [00:01, 340.78it/s, step size=1.11e+00, acc. rate=0.955] Sample: 51%|#####1 | 536/1050 [00:01, 340.78it/s, step size=1.11e+00, acc. rate=0.955] Sample: 51%|#####1 | 537/1050 [00:01, 340.78it/s, step size=1.11e+00, acc. rate=0.955] Sample: 51%|#####1 | 538/1050 [00:01, 340.78it/s, step size=1.11e+00, acc. rate=0.955] Sample: 51%|#####1 | 539/1050 [00:01, 340.78it/s, step size=1.11e+00, acc. rate=0.956] Sample: 51%|#####1 | 540/1050 [00:01, 340.78it/s, step size=1.11e+00, acc. rate=0.956] Sample: 52%|#####1 | 541/1050 [00:01, 340.78it/s, step size=1.11e+00, acc. rate=0.956] Sample: 52%|#####1 | 542/1050 [00:01, 340.78it/s, step size=1.11e+00, acc. rate=0.956] Sample: 52%|#####1 | 543/1050 [00:01, 340.78it/s, step size=1.11e+00, acc. rate=0.954] Sample: 52%|#####1 | 544/1050 [00:01, 340.78it/s, step size=1.11e+00, acc. rate=0.954] Sample: 52%|#####1 | 545/1050 [00:01, 340.78it/s, step size=1.11e+00, acc. rate=0.954] Sample: 52%|#####2 | 546/1050 [00:01, 340.78it/s, step size=1.11e+00, acc. rate=0.954] Sample: 52%|#####2 | 547/1050 [00:01, 347.44it/s, step size=1.11e+00, acc. rate=0.954] Sample: 52%|#####2 | 547/1050 [00:01, 347.44it/s, step size=1.11e+00, acc. rate=0.954] Sample: 52%|#####2 | 548/1050 [00:01, 347.44it/s, step size=1.11e+00, acc. rate=0.954] Sample: 52%|#####2 | 549/1050 [00:01, 347.44it/s, step size=1.11e+00, acc. rate=0.955] Sample: 52%|#####2 | 550/1050 [00:01, 347.44it/s, step size=1.11e+00, acc. rate=0.955] Sample: 52%|#####2 | 551/1050 [00:01, 347.44it/s, step size=1.11e+00, acc. rate=0.955] Sample: 53%|#####2 | 552/1050 [00:01, 347.44it/s, step size=1.11e+00, acc. rate=0.955] Sample: 53%|#####2 | 553/1050 [00:01, 347.44it/s, step size=1.11e+00, acc. rate=0.955] Sample: 53%|#####2 | 554/1050 [00:01, 347.44it/s, step size=1.11e+00, acc. rate=0.955] Sample: 53%|#####2 | 555/1050 [00:01, 347.44it/s, step size=1.11e+00, acc. rate=0.955] Sample: 53%|#####2 | 556/1050 [00:01, 347.44it/s, step size=1.11e+00, acc. rate=0.955] Sample: 53%|#####3 | 557/1050 [00:01, 347.44it/s, step size=1.11e+00, acc. rate=0.955] Sample: 53%|#####3 | 558/1050 [00:01, 347.44it/s, step size=1.11e+00, acc. rate=0.955] Sample: 53%|#####3 | 559/1050 [00:01, 347.44it/s, step size=1.11e+00, acc. rate=0.955] Sample: 53%|#####3 | 560/1050 [00:01, 347.44it/s, step size=1.11e+00, acc. rate=0.955] Sample: 53%|#####3 | 561/1050 [00:01, 347.44it/s, step size=1.11e+00, acc. rate=0.956] Sample: 54%|#####3 | 562/1050 [00:01, 347.44it/s, step size=1.11e+00, acc. rate=0.956] Sample: 54%|#####3 | 563/1050 [00:01, 347.44it/s, step size=1.11e+00, acc. rate=0.956] Sample: 54%|#####3 | 564/1050 [00:01, 347.44it/s, step size=1.11e+00, acc. rate=0.956] Sample: 54%|#####3 | 565/1050 [00:01, 347.44it/s, step size=1.11e+00, acc. rate=0.956] Sample: 54%|#####3 | 566/1050 [00:01, 347.44it/s, step size=1.11e+00, acc. rate=0.956] Sample: 54%|#####4 | 567/1050 [00:01, 347.44it/s, step size=1.11e+00, acc. rate=0.956] Sample: 54%|#####4 | 568/1050 [00:01, 347.44it/s, step size=1.11e+00, acc. rate=0.956] Sample: 54%|#####4 | 569/1050 [00:01, 347.44it/s, step size=1.11e+00, acc. rate=0.956] Sample: 54%|#####4 | 570/1050 [00:01, 347.44it/s, step size=1.11e+00, acc. rate=0.956] Sample: 54%|#####4 | 571/1050 [00:01, 347.44it/s, step size=1.11e+00, acc. rate=0.956] Sample: 54%|#####4 | 572/1050 [00:01, 347.44it/s, step size=1.11e+00, acc. rate=0.956] Sample: 55%|#####4 | 573/1050 [00:01, 347.44it/s, step size=1.11e+00, acc. rate=0.956] Sample: 55%|#####4 | 574/1050 [00:01, 347.44it/s, step size=1.11e+00, acc. rate=0.957] Sample: 55%|#####4 | 575/1050 [00:01, 347.44it/s, step size=1.11e+00, acc. rate=0.957] Sample: 55%|#####4 | 576/1050 [00:01, 347.44it/s, step size=1.11e+00, acc. rate=0.957] Sample: 55%|#####4 | 577/1050 [00:01, 347.44it/s, step size=1.11e+00, acc. rate=0.957] Sample: 55%|#####5 | 578/1050 [00:01, 347.44it/s, step size=1.11e+00, acc. rate=0.957] Sample: 55%|#####5 | 579/1050 [00:01, 347.44it/s, step size=1.11e+00, acc. rate=0.957] Sample: 55%|#####5 | 580/1050 [00:01, 347.44it/s, step size=1.11e+00, acc. rate=0.957] Sample: 55%|#####5 | 581/1050 [00:01, 347.44it/s, step size=1.11e+00, acc. rate=0.957] Sample: 55%|#####5 | 582/1050 [00:01, 347.44it/s, step size=1.11e+00, acc. rate=0.957] Sample: 56%|#####5 | 583/1050 [00:01, 350.83it/s, step size=1.11e+00, acc. rate=0.957] Sample: 56%|#####5 | 583/1050 [00:01, 350.83it/s, step size=1.11e+00, acc. rate=0.957] Sample: 56%|#####5 | 584/1050 [00:01, 350.83it/s, step size=1.11e+00, acc. rate=0.957] Sample: 56%|#####5 | 585/1050 [00:01, 350.83it/s, step size=1.11e+00, acc. rate=0.957] Sample: 56%|#####5 | 586/1050 [00:01, 350.83it/s, step size=1.11e+00, acc. rate=0.957] Sample: 56%|#####5 | 587/1050 [00:01, 350.83it/s, step size=1.11e+00, acc. rate=0.957] Sample: 56%|#####6 | 588/1050 [00:01, 350.83it/s, step size=1.11e+00, acc. rate=0.958] Sample: 56%|#####6 | 589/1050 [00:01, 350.83it/s, step size=1.11e+00, acc. rate=0.958] Sample: 56%|#####6 | 590/1050 [00:01, 350.83it/s, step size=1.11e+00, acc. rate=0.958] Sample: 56%|#####6 | 591/1050 [00:01, 350.83it/s, step size=1.11e+00, acc. rate=0.958] Sample: 56%|#####6 | 592/1050 [00:01, 350.83it/s, step size=1.11e+00, acc. rate=0.958] Sample: 56%|#####6 | 593/1050 [00:01, 350.83it/s, step size=1.11e+00, acc. rate=0.958] Sample: 57%|#####6 | 594/1050 [00:01, 350.83it/s, step size=1.11e+00, acc. rate=0.958] Sample: 57%|#####6 | 595/1050 [00:02, 350.83it/s, step size=1.11e+00, acc. rate=0.958] Sample: 57%|#####6 | 596/1050 [00:02, 350.83it/s, step size=1.11e+00, acc. rate=0.958] Sample: 57%|#####6 | 597/1050 [00:02, 350.83it/s, step size=1.11e+00, acc. rate=0.958] Sample: 57%|#####6 | 598/1050 [00:02, 350.83it/s, step size=1.11e+00, acc. rate=0.958] Sample: 57%|#####7 | 599/1050 [00:02, 350.83it/s, step size=1.11e+00, acc. rate=0.958] Sample: 57%|#####7 | 600/1050 [00:02, 350.83it/s, step size=1.11e+00, acc. rate=0.958] Sample: 57%|#####7 | 601/1050 [00:02, 350.83it/s, step size=1.11e+00, acc. rate=0.958] Sample: 57%|#####7 | 602/1050 [00:02, 350.83it/s, step size=1.11e+00, acc. rate=0.959] Sample: 57%|#####7 | 603/1050 [00:02, 350.83it/s, step size=1.11e+00, acc. rate=0.959] Sample: 58%|#####7 | 604/1050 [00:02, 350.83it/s, step size=1.11e+00, acc. rate=0.959] Sample: 58%|#####7 | 605/1050 [00:02, 350.83it/s, step size=1.11e+00, acc. rate=0.959] Sample: 58%|#####7 | 606/1050 [00:02, 350.83it/s, step size=1.11e+00, acc. rate=0.959] Sample: 58%|#####7 | 607/1050 [00:02, 350.83it/s, step size=1.11e+00, acc. rate=0.959] Sample: 58%|#####7 | 608/1050 [00:02, 350.83it/s, step size=1.11e+00, acc. rate=0.959] Sample: 58%|#####8 | 609/1050 [00:02, 350.83it/s, step size=1.11e+00, acc. rate=0.959] Sample: 58%|#####8 | 610/1050 [00:02, 350.83it/s, step size=1.11e+00, acc. rate=0.959] Sample: 58%|#####8 | 611/1050 [00:02, 350.83it/s, step size=1.11e+00, acc. rate=0.959] Sample: 58%|#####8 | 612/1050 [00:02, 350.83it/s, step size=1.11e+00, acc. rate=0.959] Sample: 58%|#####8 | 613/1050 [00:02, 350.83it/s, step size=1.11e+00, acc. rate=0.959] Sample: 58%|#####8 | 614/1050 [00:02, 350.83it/s, step size=1.11e+00, acc. rate=0.959] Sample: 59%|#####8 | 615/1050 [00:02, 350.83it/s, step size=1.11e+00, acc. rate=0.959] Sample: 59%|#####8 | 616/1050 [00:02, 350.83it/s, step size=1.11e+00, acc. rate=0.959] Sample: 59%|#####8 | 617/1050 [00:02, 350.83it/s, step size=1.11e+00, acc. rate=0.960] Sample: 59%|#####8 | 618/1050 [00:02, 350.83it/s, step size=1.11e+00, acc. rate=0.960] Sample: 59%|#####8 | 619/1050 [00:02, 349.89it/s, step size=1.11e+00, acc. rate=0.960] Sample: 59%|#####8 | 619/1050 [00:02, 349.89it/s, step size=1.11e+00, acc. rate=0.960] Sample: 59%|#####9 | 620/1050 [00:02, 349.89it/s, step size=1.11e+00, acc. rate=0.960] Sample: 59%|#####9 | 621/1050 [00:02, 349.89it/s, step size=1.11e+00, acc. rate=0.960] Sample: 59%|#####9 | 622/1050 [00:02, 349.89it/s, step size=1.11e+00, acc. rate=0.960] Sample: 59%|#####9 | 623/1050 [00:02, 349.89it/s, step size=1.11e+00, acc. rate=0.960] Sample: 59%|#####9 | 624/1050 [00:02, 349.89it/s, step size=1.11e+00, acc. rate=0.960] Sample: 60%|#####9 | 625/1050 [00:02, 349.89it/s, step size=1.11e+00, acc. rate=0.960] Sample: 60%|#####9 | 626/1050 [00:02, 349.89it/s, step size=1.11e+00, acc. rate=0.960] Sample: 60%|#####9 | 627/1050 [00:02, 349.89it/s, step size=1.11e+00, acc. rate=0.960] Sample: 60%|#####9 | 628/1050 [00:02, 349.89it/s, step size=1.11e+00, acc. rate=0.960] Sample: 60%|#####9 | 629/1050 [00:02, 349.89it/s, step size=1.11e+00, acc. rate=0.960] Sample: 60%|###### | 630/1050 [00:02, 349.89it/s, step size=1.11e+00, acc. rate=0.960] Sample: 60%|###### | 631/1050 [00:02, 349.89it/s, step size=1.11e+00, acc. rate=0.960] Sample: 60%|###### | 632/1050 [00:02, 349.89it/s, step size=1.11e+00, acc. rate=0.961] Sample: 60%|###### | 633/1050 [00:02, 349.89it/s, step size=1.11e+00, acc. rate=0.961] Sample: 60%|###### | 634/1050 [00:02, 349.89it/s, step size=1.11e+00, acc. rate=0.961] Sample: 60%|###### | 635/1050 [00:02, 349.89it/s, step size=1.11e+00, acc. rate=0.961] Sample: 61%|###### | 636/1050 [00:02, 349.89it/s, step size=1.11e+00, acc. rate=0.961] Sample: 61%|###### | 637/1050 [00:02, 349.89it/s, step size=1.11e+00, acc. rate=0.961] Sample: 61%|###### | 638/1050 [00:02, 349.89it/s, step size=1.11e+00, acc. rate=0.961] Sample: 61%|###### | 639/1050 [00:02, 349.89it/s, step size=1.11e+00, acc. rate=0.961] Sample: 61%|###### | 640/1050 [00:02, 349.89it/s, step size=1.11e+00, acc. rate=0.961] Sample: 61%|######1 | 641/1050 [00:02, 349.89it/s, step size=1.11e+00, acc. rate=0.961] Sample: 61%|######1 | 642/1050 [00:02, 349.89it/s, step size=1.11e+00, acc. rate=0.961] Sample: 61%|######1 | 643/1050 [00:02, 349.89it/s, step size=1.11e+00, acc. rate=0.961] Sample: 61%|######1 | 644/1050 [00:02, 349.89it/s, step size=1.11e+00, acc. rate=0.961] Sample: 61%|######1 | 645/1050 [00:02, 349.89it/s, step size=1.11e+00, acc. rate=0.961] Sample: 62%|######1 | 646/1050 [00:02, 349.89it/s, step size=1.11e+00, acc. rate=0.961] Sample: 62%|######1 | 647/1050 [00:02, 349.89it/s, step size=1.11e+00, acc. rate=0.961] Sample: 62%|######1 | 648/1050 [00:02, 349.89it/s, step size=1.11e+00, acc. rate=0.961] Sample: 62%|######1 | 649/1050 [00:02, 349.89it/s, step size=1.11e+00, acc. rate=0.962] Sample: 62%|######1 | 650/1050 [00:02, 349.89it/s, step size=1.11e+00, acc. rate=0.962] Sample: 62%|######2 | 651/1050 [00:02, 349.89it/s, step size=1.11e+00, acc. rate=0.962] Sample: 62%|######2 | 652/1050 [00:02, 349.89it/s, step size=1.11e+00, acc. rate=0.962] Sample: 62%|######2 | 653/1050 [00:02, 349.89it/s, step size=1.11e+00, acc. rate=0.962] Sample: 62%|######2 | 654/1050 [00:02, 349.89it/s, step size=1.11e+00, acc. rate=0.962] Sample: 62%|######2 | 655/1050 [00:02, 315.56it/s, step size=1.11e+00, acc. rate=0.962] Sample: 62%|######2 | 655/1050 [00:02, 315.56it/s, step size=1.11e+00, acc. rate=0.962] Sample: 62%|######2 | 656/1050 [00:02, 315.56it/s, step size=1.11e+00, acc. rate=0.962] Sample: 63%|######2 | 657/1050 [00:02, 315.56it/s, step size=1.11e+00, acc. rate=0.962] Sample: 63%|######2 | 658/1050 [00:02, 315.56it/s, step size=1.11e+00, acc. rate=0.962] Sample: 63%|######2 | 659/1050 [00:02, 315.56it/s, step size=1.11e+00, acc. rate=0.962] Sample: 63%|######2 | 660/1050 [00:02, 315.56it/s, step size=1.11e+00, acc. rate=0.962] Sample: 63%|######2 | 661/1050 [00:02, 315.56it/s, step size=1.11e+00, acc. rate=0.962] Sample: 63%|######3 | 662/1050 [00:02, 315.56it/s, step size=1.11e+00, acc. rate=0.962] Sample: 63%|######3 | 663/1050 [00:02, 315.56it/s, step size=1.11e+00, acc. rate=0.962] Sample: 63%|######3 | 664/1050 [00:02, 315.56it/s, step size=1.11e+00, acc. rate=0.962] Sample: 63%|######3 | 665/1050 [00:02, 315.56it/s, step size=1.11e+00, acc. rate=0.962] Sample: 63%|######3 | 666/1050 [00:02, 315.56it/s, step size=1.11e+00, acc. rate=0.963] Sample: 64%|######3 | 667/1050 [00:02, 315.56it/s, step size=1.11e+00, acc. rate=0.963] Sample: 64%|######3 | 668/1050 [00:02, 315.56it/s, step size=1.11e+00, acc. rate=0.963] Sample: 64%|######3 | 669/1050 [00:02, 315.56it/s, step size=1.11e+00, acc. rate=0.963] Sample: 64%|######3 | 670/1050 [00:02, 315.56it/s, step size=1.11e+00, acc. rate=0.963] Sample: 64%|######3 | 671/1050 [00:02, 315.56it/s, step size=1.11e+00, acc. rate=0.963] Sample: 64%|######4 | 672/1050 [00:02, 315.56it/s, step size=1.11e+00, acc. rate=0.963] Sample: 64%|######4 | 673/1050 [00:02, 315.56it/s, step size=1.11e+00, acc. rate=0.963] Sample: 64%|######4 | 674/1050 [00:02, 315.56it/s, step size=1.11e+00, acc. rate=0.963] Sample: 64%|######4 | 675/1050 [00:02, 315.56it/s, step size=1.11e+00, acc. rate=0.963] Sample: 64%|######4 | 676/1050 [00:02, 315.56it/s, step size=1.11e+00, acc. rate=0.963] Sample: 64%|######4 | 677/1050 [00:02, 315.56it/s, step size=1.11e+00, acc. rate=0.963] Sample: 65%|######4 | 678/1050 [00:02, 315.56it/s, step size=1.11e+00, acc. rate=0.963] Sample: 65%|######4 | 679/1050 [00:02, 315.56it/s, step size=1.11e+00, acc. rate=0.963] Sample: 65%|######4 | 680/1050 [00:02, 315.56it/s, step size=1.11e+00, acc. rate=0.963] Sample: 65%|######4 | 681/1050 [00:02, 315.56it/s, step size=1.11e+00, acc. rate=0.963] Sample: 65%|######4 | 682/1050 [00:02, 315.56it/s, step size=1.11e+00, acc. rate=0.963] Sample: 65%|######5 | 683/1050 [00:02, 315.56it/s, step size=1.11e+00, acc. rate=0.963] Sample: 65%|######5 | 684/1050 [00:02, 315.56it/s, step size=1.11e+00, acc. rate=0.964] Sample: 65%|######5 | 685/1050 [00:02, 315.56it/s, step size=1.11e+00, acc. rate=0.964] Sample: 65%|######5 | 686/1050 [00:02, 315.56it/s, step size=1.11e+00, acc. rate=0.964] Sample: 65%|######5 | 687/1050 [00:02, 315.56it/s, step size=1.11e+00, acc. rate=0.964] Sample: 66%|######5 | 688/1050 [00:02, 315.56it/s, step size=1.11e+00, acc. rate=0.964] Sample: 66%|######5 | 689/1050 [00:02, 320.89it/s, step size=1.11e+00, acc. rate=0.964] Sample: 66%|######5 | 689/1050 [00:02, 320.89it/s, step size=1.11e+00, acc. rate=0.964] Sample: 66%|######5 | 690/1050 [00:02, 320.89it/s, step size=1.11e+00, acc. rate=0.964] Sample: 66%|######5 | 691/1050 [00:02, 320.89it/s, step size=1.11e+00, acc. rate=0.964] Sample: 66%|######5 | 692/1050 [00:02, 320.89it/s, step size=1.11e+00, acc. rate=0.964] Sample: 66%|######6 | 693/1050 [00:02, 320.89it/s, step size=1.11e+00, acc. rate=0.964] Sample: 66%|######6 | 694/1050 [00:02, 320.89it/s, step size=1.11e+00, acc. rate=0.964] Sample: 66%|######6 | 695/1050 [00:02, 320.89it/s, step size=1.11e+00, acc. rate=0.964] Sample: 66%|######6 | 696/1050 [00:02, 320.89it/s, step size=1.11e+00, acc. rate=0.964] Sample: 66%|######6 | 697/1050 [00:02, 320.89it/s, step size=1.11e+00, acc. rate=0.964] Sample: 66%|######6 | 698/1050 [00:02, 320.89it/s, step size=1.11e+00, acc. rate=0.964] Sample: 67%|######6 | 699/1050 [00:02, 320.89it/s, step size=1.11e+00, acc. rate=0.964] Sample: 67%|######6 | 700/1050 [00:02, 320.89it/s, step size=1.11e+00, acc. rate=0.964] Sample: 67%|######6 | 701/1050 [00:02, 320.89it/s, step size=1.11e+00, acc. rate=0.964] Sample: 67%|######6 | 702/1050 [00:02, 320.89it/s, step size=1.11e+00, acc. rate=0.964] Sample: 67%|######6 | 703/1050 [00:02, 320.89it/s, step size=1.11e+00, acc. rate=0.964] Sample: 67%|######7 | 704/1050 [00:02, 320.89it/s, step size=1.11e+00, acc. rate=0.965] Sample: 67%|######7 | 705/1050 [00:02, 320.89it/s, step size=1.11e+00, acc. rate=0.965] Sample: 67%|######7 | 706/1050 [00:02, 320.89it/s, step size=1.11e+00, acc. rate=0.965] Sample: 67%|######7 | 707/1050 [00:02, 320.89it/s, step size=1.11e+00, acc. rate=0.965] Sample: 67%|######7 | 708/1050 [00:02, 320.89it/s, step size=1.11e+00, acc. rate=0.965] Sample: 68%|######7 | 709/1050 [00:02, 320.89it/s, step size=1.11e+00, acc. rate=0.965] Sample: 68%|######7 | 710/1050 [00:02, 320.89it/s, step size=1.11e+00, acc. rate=0.965] Sample: 68%|######7 | 711/1050 [00:02, 320.89it/s, step size=1.11e+00, acc. rate=0.965] Sample: 68%|######7 | 712/1050 [00:02, 320.89it/s, step size=1.11e+00, acc. rate=0.965] Sample: 68%|######7 | 713/1050 [00:02, 320.89it/s, step size=1.11e+00, acc. rate=0.965] Sample: 68%|######8 | 714/1050 [00:02, 320.89it/s, step size=1.11e+00, acc. rate=0.965] Sample: 68%|######8 | 715/1050 [00:02, 320.89it/s, step size=1.11e+00, acc. rate=0.965] Sample: 68%|######8 | 716/1050 [00:02, 320.89it/s, step size=1.11e+00, acc. rate=0.965] Sample: 68%|######8 | 717/1050 [00:02, 320.89it/s, step size=1.11e+00, acc. rate=0.965] Sample: 68%|######8 | 718/1050 [00:02, 320.89it/s, step size=1.11e+00, acc. rate=0.965] Sample: 68%|######8 | 719/1050 [00:02, 320.89it/s, step size=1.11e+00, acc. rate=0.965] Sample: 69%|######8 | 720/1050 [00:02, 320.89it/s, step size=1.11e+00, acc. rate=0.965] Sample: 69%|######8 | 721/1050 [00:02, 320.89it/s, step size=1.11e+00, acc. rate=0.965] Sample: 69%|######8 | 722/1050 [00:02, 320.89it/s, step size=1.11e+00, acc. rate=0.965] Sample: 69%|######8 | 723/1050 [00:02, 323.73it/s, step size=1.11e+00, acc. rate=0.965] Sample: 69%|######8 | 723/1050 [00:02, 323.73it/s, step size=1.11e+00, acc. rate=0.965] Sample: 69%|######8 | 724/1050 [00:02, 323.73it/s, step size=1.11e+00, acc. rate=0.966] Sample: 69%|######9 | 725/1050 [00:02, 323.73it/s, step size=1.11e+00, acc. rate=0.966] Sample: 69%|######9 | 726/1050 [00:02, 323.73it/s, step size=1.11e+00, acc. rate=0.966] Sample: 69%|######9 | 727/1050 [00:02, 323.73it/s, step size=1.11e+00, acc. rate=0.966] Sample: 69%|######9 | 728/1050 [00:02, 323.73it/s, step size=1.11e+00, acc. rate=0.966] Sample: 69%|######9 | 729/1050 [00:02, 323.73it/s, step size=1.11e+00, acc. rate=0.966] Sample: 70%|######9 | 730/1050 [00:02, 323.73it/s, step size=1.11e+00, acc. rate=0.966] Sample: 70%|######9 | 731/1050 [00:02, 323.73it/s, step size=1.11e+00, acc. rate=0.966] Sample: 70%|######9 | 732/1050 [00:02, 323.73it/s, step size=1.11e+00, acc. rate=0.966] Sample: 70%|######9 | 733/1050 [00:02, 323.73it/s, step size=1.11e+00, acc. rate=0.966] Sample: 70%|######9 | 734/1050 [00:02, 323.73it/s, step size=1.11e+00, acc. rate=0.966] Sample: 70%|####### | 735/1050 [00:02, 323.73it/s, step size=1.11e+00, acc. rate=0.966] Sample: 70%|####### | 736/1050 [00:02, 323.73it/s, step size=1.11e+00, acc. rate=0.966] Sample: 70%|####### | 737/1050 [00:02, 323.73it/s, step size=1.11e+00, acc. rate=0.966] Sample: 70%|####### | 738/1050 [00:02, 323.73it/s, step size=1.11e+00, acc. rate=0.966] Sample: 70%|####### | 739/1050 [00:02, 323.73it/s, step size=1.11e+00, acc. rate=0.966] Sample: 70%|####### | 740/1050 [00:02, 323.73it/s, step size=1.11e+00, acc. rate=0.966] Sample: 71%|####### | 741/1050 [00:02, 323.73it/s, step size=1.11e+00, acc. rate=0.966] Sample: 71%|####### | 742/1050 [00:02, 323.73it/s, step size=1.11e+00, acc. rate=0.966] Sample: 71%|####### | 743/1050 [00:02, 323.73it/s, step size=1.11e+00, acc. rate=0.966] Sample: 71%|####### | 744/1050 [00:02, 323.73it/s, step size=1.11e+00, acc. rate=0.966] Sample: 71%|####### | 745/1050 [00:02, 323.73it/s, step size=1.11e+00, acc. rate=0.966] Sample: 71%|#######1 | 746/1050 [00:02, 323.73it/s, step size=1.11e+00, acc. rate=0.967] Sample: 71%|#######1 | 747/1050 [00:02, 323.73it/s, step size=1.11e+00, acc. rate=0.967] Sample: 71%|#######1 | 748/1050 [00:02, 323.73it/s, step size=1.11e+00, acc. rate=0.967] Sample: 71%|#######1 | 749/1050 [00:02, 323.73it/s, step size=1.11e+00, acc. rate=0.967] Sample: 71%|#######1 | 750/1050 [00:02, 323.73it/s, step size=1.11e+00, acc. rate=0.967] Sample: 72%|#######1 | 751/1050 [00:02, 323.73it/s, step size=1.11e+00, acc. rate=0.967] Sample: 72%|#######1 | 752/1050 [00:02, 323.73it/s, step size=1.11e+00, acc. rate=0.967] Sample: 72%|#######1 | 753/1050 [00:02, 323.73it/s, step size=1.11e+00, acc. rate=0.967] Sample: 72%|#######1 | 754/1050 [00:02, 323.73it/s, step size=1.11e+00, acc. rate=0.967] Sample: 72%|#######1 | 755/1050 [00:02, 323.73it/s, step size=1.11e+00, acc. rate=0.967] Sample: 72%|#######2 | 756/1050 [00:02, 323.73it/s, step size=1.11e+00, acc. rate=0.967] Sample: 72%|#######2 | 757/1050 [00:02, 323.73it/s, step size=1.11e+00, acc. rate=0.967] Sample: 72%|#######2 | 758/1050 [00:02, 330.15it/s, step size=1.11e+00, acc. rate=0.967] Sample: 72%|#######2 | 758/1050 [00:02, 330.15it/s, step size=1.11e+00, acc. rate=0.967] Sample: 72%|#######2 | 759/1050 [00:02, 330.15it/s, step size=1.11e+00, acc. rate=0.967] Sample: 72%|#######2 | 760/1050 [00:02, 330.15it/s, step size=1.11e+00, acc. rate=0.967] Sample: 72%|#######2 | 761/1050 [00:02, 330.15it/s, step size=1.11e+00, acc. rate=0.967] Sample: 73%|#######2 | 762/1050 [00:02, 330.15it/s, step size=1.11e+00, acc. rate=0.967] Sample: 73%|#######2 | 763/1050 [00:02, 330.15it/s, step size=1.11e+00, acc. rate=0.967] Sample: 73%|#######2 | 764/1050 [00:02, 330.15it/s, step size=1.11e+00, acc. rate=0.967] Sample: 73%|#######2 | 765/1050 [00:02, 330.15it/s, step size=1.11e+00, acc. rate=0.967] Sample: 73%|#######2 | 766/1050 [00:02, 330.15it/s, step size=1.11e+00, acc. rate=0.967] Sample: 73%|#######3 | 767/1050 [00:02, 330.15it/s, step size=1.11e+00, acc. rate=0.967] Sample: 73%|#######3 | 768/1050 [00:02, 330.15it/s, step size=1.11e+00, acc. rate=0.967] Sample: 73%|#######3 | 769/1050 [00:02, 330.15it/s, step size=1.11e+00, acc. rate=0.968] Sample: 73%|#######3 | 770/1050 [00:02, 330.15it/s, step size=1.11e+00, acc. rate=0.968] Sample: 73%|#######3 | 771/1050 [00:02, 330.15it/s, step size=1.11e+00, acc. rate=0.968] Sample: 74%|#######3 | 772/1050 [00:02, 330.15it/s, step size=1.11e+00, acc. rate=0.968] Sample: 74%|#######3 | 773/1050 [00:02, 330.15it/s, step size=1.11e+00, acc. rate=0.968] Sample: 74%|#######3 | 774/1050 [00:02, 330.15it/s, step size=1.11e+00, acc. rate=0.968] Sample: 74%|#######3 | 775/1050 [00:02, 330.15it/s, step size=1.11e+00, acc. rate=0.968] Sample: 74%|#######3 | 776/1050 [00:02, 330.15it/s, step size=1.11e+00, acc. rate=0.968] Sample: 74%|#######4 | 777/1050 [00:02, 330.15it/s, step size=1.11e+00, acc. rate=0.968] Sample: 74%|#######4 | 778/1050 [00:02, 330.15it/s, step size=1.11e+00, acc. rate=0.968] Sample: 74%|#######4 | 779/1050 [00:02, 330.15it/s, step size=1.11e+00, acc. rate=0.968] Sample: 74%|#######4 | 780/1050 [00:02, 330.15it/s, step size=1.11e+00, acc. rate=0.968] Sample: 74%|#######4 | 781/1050 [00:02, 330.15it/s, step size=1.11e+00, acc. rate=0.968] Sample: 74%|#######4 | 782/1050 [00:02, 330.15it/s, step size=1.11e+00, acc. rate=0.968] Sample: 75%|#######4 | 783/1050 [00:02, 330.15it/s, step size=1.11e+00, acc. rate=0.968] Sample: 75%|#######4 | 784/1050 [00:02, 330.15it/s, step size=1.11e+00, acc. rate=0.968] Sample: 75%|#######4 | 785/1050 [00:02, 330.15it/s, step size=1.11e+00, acc. rate=0.968] Sample: 75%|#######4 | 786/1050 [00:02, 330.15it/s, step size=1.11e+00, acc. rate=0.968] Sample: 75%|#######4 | 787/1050 [00:02, 330.15it/s, step size=1.11e+00, acc. rate=0.968] Sample: 75%|#######5 | 788/1050 [00:02, 330.15it/s, step size=1.11e+00, acc. rate=0.968] Sample: 75%|#######5 | 789/1050 [00:02, 330.15it/s, step size=1.11e+00, acc. rate=0.968] Sample: 75%|#######5 | 790/1050 [00:02, 330.15it/s, step size=1.11e+00, acc. rate=0.968] Sample: 75%|#######5 | 791/1050 [00:02, 330.15it/s, step size=1.11e+00, acc. rate=0.968] Sample: 75%|#######5 | 792/1050 [00:02, 290.48it/s, step size=1.11e+00, acc. rate=0.968] Sample: 75%|#######5 | 792/1050 [00:02, 290.48it/s, step size=1.11e+00, acc. rate=0.968] Sample: 76%|#######5 | 793/1050 [00:02, 290.48it/s, step size=1.11e+00, acc. rate=0.969] Sample: 76%|#######5 | 794/1050 [00:02, 290.48it/s, step size=1.11e+00, acc. rate=0.969] Sample: 76%|#######5 | 795/1050 [00:02, 290.48it/s, step size=1.11e+00, acc. rate=0.969] Sample: 76%|#######5 | 796/1050 [00:02, 290.48it/s, step size=1.11e+00, acc. rate=0.969] Sample: 76%|#######5 | 797/1050 [00:02, 290.48it/s, step size=1.11e+00, acc. rate=0.969] Sample: 76%|#######6 | 798/1050 [00:02, 290.48it/s, step size=1.11e+00, acc. rate=0.969] Sample: 76%|#######6 | 799/1050 [00:02, 290.48it/s, step size=1.11e+00, acc. rate=0.969] Sample: 76%|#######6 | 800/1050 [00:02, 290.48it/s, step size=1.11e+00, acc. rate=0.969] Sample: 76%|#######6 | 801/1050 [00:02, 290.48it/s, step size=1.11e+00, acc. rate=0.969] Sample: 76%|#######6 | 802/1050 [00:02, 290.48it/s, step size=1.11e+00, acc. rate=0.969] Sample: 76%|#######6 | 803/1050 [00:02, 290.48it/s, step size=1.11e+00, acc. rate=0.969] Sample: 77%|#######6 | 804/1050 [00:02, 290.48it/s, step size=1.11e+00, acc. rate=0.969] Sample: 77%|#######6 | 805/1050 [00:02, 290.48it/s, step size=1.11e+00, acc. rate=0.969] Sample: 77%|#######6 | 806/1050 [00:02, 290.48it/s, step size=1.11e+00, acc. rate=0.969] Sample: 77%|#######6 | 807/1050 [00:02, 290.48it/s, step size=1.11e+00, acc. rate=0.969] Sample: 77%|#######6 | 808/1050 [00:02, 290.48it/s, step size=1.11e+00, acc. rate=0.969] Sample: 77%|#######7 | 809/1050 [00:02, 290.48it/s, step size=1.11e+00, acc. rate=0.969] Sample: 77%|#######7 | 810/1050 [00:02, 290.48it/s, step size=1.11e+00, acc. rate=0.969] Sample: 77%|#######7 | 811/1050 [00:02, 290.48it/s, step size=1.11e+00, acc. rate=0.969] Sample: 77%|#######7 | 812/1050 [00:02, 290.48it/s, step size=1.11e+00, acc. rate=0.969] Sample: 77%|#######7 | 813/1050 [00:02, 290.48it/s, step size=1.11e+00, acc. rate=0.969] Sample: 78%|#######7 | 814/1050 [00:02, 290.48it/s, step size=1.11e+00, acc. rate=0.969] Sample: 78%|#######7 | 815/1050 [00:02, 290.48it/s, step size=1.11e+00, acc. rate=0.969] Sample: 78%|#######7 | 816/1050 [00:02, 290.48it/s, step size=1.11e+00, acc. rate=0.969] Sample: 78%|#######7 | 817/1050 [00:02, 290.48it/s, step size=1.11e+00, acc. rate=0.969] Sample: 78%|#######7 | 818/1050 [00:02, 290.48it/s, step size=1.11e+00, acc. rate=0.969] Sample: 78%|#######8 | 819/1050 [00:02, 290.48it/s, step size=1.11e+00, acc. rate=0.970] Sample: 78%|#######8 | 820/1050 [00:02, 290.48it/s, step size=1.11e+00, acc. rate=0.970] Sample: 78%|#######8 | 821/1050 [00:02, 290.48it/s, step size=1.11e+00, acc. rate=0.970] Sample: 78%|#######8 | 822/1050 [00:02, 290.48it/s, step size=1.11e+00, acc. rate=0.970] Sample: 78%|#######8 | 823/1050 [00:02, 290.48it/s, step size=1.11e+00, acc. rate=0.970] Sample: 78%|#######8 | 824/1050 [00:02, 290.48it/s, step size=1.11e+00, acc. rate=0.970] Sample: 79%|#######8 | 825/1050 [00:02, 290.48it/s, step size=1.11e+00, acc. rate=0.970] Sample: 79%|#######8 | 826/1050 [00:02, 290.48it/s, step size=1.11e+00, acc. rate=0.970] Sample: 79%|#######8 | 827/1050 [00:02, 290.48it/s, step size=1.11e+00, acc. rate=0.970] Sample: 79%|#######8 | 828/1050 [00:02, 290.48it/s, step size=1.11e+00, acc. rate=0.970] Sample: 79%|#######8 | 829/1050 [00:02, 308.63it/s, step size=1.11e+00, acc. rate=0.970] Sample: 79%|#######8 | 829/1050 [00:02, 308.63it/s, step size=1.11e+00, acc. rate=0.970] Sample: 79%|#######9 | 830/1050 [00:02, 308.63it/s, step size=1.11e+00, acc. rate=0.970] Sample: 79%|#######9 | 831/1050 [00:02, 308.63it/s, step size=1.11e+00, acc. rate=0.970] Sample: 79%|#######9 | 832/1050 [00:02, 308.63it/s, step size=1.11e+00, acc. rate=0.970] Sample: 79%|#######9 | 833/1050 [00:02, 308.63it/s, step size=1.11e+00, acc. rate=0.970] Sample: 79%|#######9 | 834/1050 [00:02, 308.63it/s, step size=1.11e+00, acc. rate=0.970] Sample: 80%|#######9 | 835/1050 [00:02, 308.63it/s, step size=1.11e+00, acc. rate=0.970] Sample: 80%|#######9 | 836/1050 [00:02, 308.63it/s, step size=1.11e+00, acc. rate=0.970] Sample: 80%|#######9 | 837/1050 [00:02, 308.63it/s, step size=1.11e+00, acc. rate=0.970] Sample: 80%|#######9 | 838/1050 [00:02, 308.63it/s, step size=1.11e+00, acc. rate=0.970] Sample: 80%|#######9 | 839/1050 [00:02, 308.63it/s, step size=1.11e+00, acc. rate=0.970] Sample: 80%|######## | 840/1050 [00:02, 308.63it/s, step size=1.11e+00, acc. rate=0.970] Sample: 80%|######## | 841/1050 [00:02, 308.63it/s, step size=1.11e+00, acc. rate=0.970] Sample: 80%|######## | 842/1050 [00:02, 308.63it/s, step size=1.11e+00, acc. rate=0.970] Sample: 80%|######## | 843/1050 [00:02, 308.63it/s, step size=1.11e+00, acc. rate=0.970] Sample: 80%|######## | 844/1050 [00:02, 308.63it/s, step size=1.11e+00, acc. rate=0.970] Sample: 80%|######## | 845/1050 [00:02, 308.63it/s, step size=1.11e+00, acc. rate=0.970] Sample: 81%|######## | 846/1050 [00:02, 308.63it/s, step size=1.11e+00, acc. rate=0.970] Sample: 81%|######## | 847/1050 [00:02, 308.63it/s, step size=1.11e+00, acc. rate=0.971] Sample: 81%|######## | 848/1050 [00:02, 308.63it/s, step size=1.11e+00, acc. rate=0.971] Sample: 81%|######## | 849/1050 [00:02, 308.63it/s, step size=1.11e+00, acc. rate=0.971] Sample: 81%|######## | 850/1050 [00:02, 308.63it/s, step size=1.11e+00, acc. rate=0.971] Sample: 81%|########1 | 851/1050 [00:02, 308.63it/s, step size=1.11e+00, acc. rate=0.971] Sample: 81%|########1 | 852/1050 [00:02, 308.63it/s, step size=1.11e+00, acc. rate=0.971] Sample: 81%|########1 | 853/1050 [00:02, 308.63it/s, step size=1.11e+00, acc. rate=0.971] Sample: 81%|########1 | 854/1050 [00:02, 308.63it/s, step size=1.11e+00, acc. rate=0.971] Sample: 81%|########1 | 855/1050 [00:02, 308.63it/s, step size=1.11e+00, acc. rate=0.971] Sample: 82%|########1 | 856/1050 [00:02, 308.63it/s, step size=1.11e+00, acc. rate=0.971] Sample: 82%|########1 | 857/1050 [00:02, 308.63it/s, step size=1.11e+00, acc. rate=0.971] Sample: 82%|########1 | 858/1050 [00:02, 308.63it/s, step size=1.11e+00, acc. rate=0.971] Sample: 82%|########1 | 859/1050 [00:02, 308.63it/s, step size=1.11e+00, acc. rate=0.971] Sample: 82%|########1 | 860/1050 [00:02, 308.63it/s, step size=1.11e+00, acc. rate=0.971] Sample: 82%|########2 | 861/1050 [00:02, 308.63it/s, step size=1.11e+00, acc. rate=0.971] Sample: 82%|########2 | 862/1050 [00:02, 308.63it/s, step size=1.11e+00, acc. rate=0.971] Sample: 82%|########2 | 863/1050 [00:02, 316.40it/s, step size=1.11e+00, acc. rate=0.971] Sample: 82%|########2 | 863/1050 [00:02, 316.40it/s, step size=1.11e+00, acc. rate=0.971] Sample: 82%|########2 | 864/1050 [00:02, 316.40it/s, step size=1.11e+00, acc. rate=0.971] Sample: 82%|########2 | 865/1050 [00:02, 316.40it/s, step size=1.11e+00, acc. rate=0.971] Sample: 82%|########2 | 866/1050 [00:02, 316.40it/s, step size=1.11e+00, acc. rate=0.971] Sample: 83%|########2 | 867/1050 [00:02, 316.40it/s, step size=1.11e+00, acc. rate=0.971] Sample: 83%|########2 | 868/1050 [00:02, 316.40it/s, step size=1.11e+00, acc. rate=0.971] Sample: 83%|########2 | 869/1050 [00:02, 316.40it/s, step size=1.11e+00, acc. rate=0.971] Sample: 83%|########2 | 870/1050 [00:02, 316.40it/s, step size=1.11e+00, acc. rate=0.971] Sample: 83%|########2 | 871/1050 [00:02, 316.40it/s, step size=1.11e+00, acc. rate=0.971] Sample: 83%|########3 | 872/1050 [00:02, 316.40it/s, step size=1.11e+00, acc. rate=0.971] Sample: 83%|########3 | 873/1050 [00:02, 316.40it/s, step size=1.11e+00, acc. rate=0.971] Sample: 83%|########3 | 874/1050 [00:02, 316.40it/s, step size=1.11e+00, acc. rate=0.971] Sample: 83%|########3 | 875/1050 [00:02, 316.40it/s, step size=1.11e+00, acc. rate=0.971] Sample: 83%|########3 | 876/1050 [00:02, 316.40it/s, step size=1.11e+00, acc. rate=0.971] Sample: 84%|########3 | 877/1050 [00:02, 316.40it/s, step size=1.11e+00, acc. rate=0.972] Sample: 84%|########3 | 878/1050 [00:02, 316.40it/s, step size=1.11e+00, acc. rate=0.972] Sample: 84%|########3 | 879/1050 [00:02, 316.40it/s, step size=1.11e+00, acc. rate=0.972] Sample: 84%|########3 | 880/1050 [00:02, 316.40it/s, step size=1.11e+00, acc. rate=0.972] Sample: 84%|########3 | 881/1050 [00:02, 316.40it/s, step size=1.11e+00, acc. rate=0.972] Sample: 84%|########4 | 882/1050 [00:02, 316.40it/s, step size=1.11e+00, acc. rate=0.972] Sample: 84%|########4 | 883/1050 [00:02, 316.40it/s, step size=1.11e+00, acc. rate=0.972] Sample: 84%|########4 | 884/1050 [00:02, 316.40it/s, step size=1.11e+00, acc. rate=0.972] Sample: 84%|########4 | 885/1050 [00:02, 316.40it/s, step size=1.11e+00, acc. rate=0.972] Sample: 84%|########4 | 886/1050 [00:02, 316.40it/s, step size=1.11e+00, acc. rate=0.972] Sample: 84%|########4 | 887/1050 [00:02, 316.40it/s, step size=1.11e+00, acc. rate=0.972] Sample: 85%|########4 | 888/1050 [00:02, 316.40it/s, step size=1.11e+00, acc. rate=0.972] Sample: 85%|########4 | 889/1050 [00:02, 316.40it/s, step size=1.11e+00, acc. rate=0.972] Sample: 85%|########4 | 890/1050 [00:02, 316.40it/s, step size=1.11e+00, acc. rate=0.972] Sample: 85%|########4 | 891/1050 [00:02, 316.40it/s, step size=1.11e+00, acc. rate=0.972] Sample: 85%|########4 | 892/1050 [00:02, 316.40it/s, step size=1.11e+00, acc. rate=0.972] Sample: 85%|########5 | 893/1050 [00:02, 316.40it/s, step size=1.11e+00, acc. rate=0.972] Sample: 85%|########5 | 894/1050 [00:02, 316.40it/s, step size=1.11e+00, acc. rate=0.972] Sample: 85%|########5 | 895/1050 [00:02, 316.40it/s, step size=1.11e+00, acc. rate=0.972] Sample: 85%|########5 | 896/1050 [00:02, 316.40it/s, step size=1.11e+00, acc. rate=0.972] Sample: 85%|########5 | 897/1050 [00:02, 316.40it/s, step size=1.11e+00, acc. rate=0.972] Sample: 86%|########5 | 898/1050 [00:02, 324.67it/s, step size=1.11e+00, acc. rate=0.972] Sample: 86%|########5 | 898/1050 [00:02, 324.67it/s, step size=1.11e+00, acc. rate=0.972] Sample: 86%|########5 | 899/1050 [00:02, 324.67it/s, step size=1.11e+00, acc. rate=0.972] Sample: 86%|########5 | 900/1050 [00:02, 324.67it/s, step size=1.11e+00, acc. rate=0.972] Sample: 86%|########5 | 901/1050 [00:02, 324.67it/s, step size=1.11e+00, acc. rate=0.972] Sample: 86%|########5 | 902/1050 [00:02, 324.67it/s, step size=1.11e+00, acc. rate=0.972] Sample: 86%|########6 | 903/1050 [00:02, 324.67it/s, step size=1.11e+00, acc. rate=0.972] Sample: 86%|########6 | 904/1050 [00:02, 324.67it/s, step size=1.11e+00, acc. rate=0.971] Sample: 86%|########6 | 905/1050 [00:02, 324.67it/s, step size=1.11e+00, acc. rate=0.971] Sample: 86%|########6 | 906/1050 [00:02, 324.67it/s, step size=1.11e+00, acc. rate=0.971] Sample: 86%|########6 | 907/1050 [00:02, 324.67it/s, step size=1.11e+00, acc. rate=0.971] Sample: 86%|########6 | 908/1050 [00:02, 324.67it/s, step size=1.11e+00, acc. rate=0.971] Sample: 87%|########6 | 909/1050 [00:03, 324.67it/s, step size=1.11e+00, acc. rate=0.971] Sample: 87%|########6 | 910/1050 [00:03, 324.67it/s, step size=1.11e+00, acc. rate=0.971] Sample: 87%|########6 | 911/1050 [00:03, 324.67it/s, step size=1.11e+00, acc. rate=0.971] Sample: 87%|########6 | 912/1050 [00:03, 324.67it/s, step size=1.11e+00, acc. rate=0.972] Sample: 87%|########6 | 913/1050 [00:03, 324.67it/s, step size=1.11e+00, acc. rate=0.972] Sample: 87%|########7 | 914/1050 [00:03, 324.67it/s, step size=1.11e+00, acc. rate=0.972] Sample: 87%|########7 | 915/1050 [00:03, 324.67it/s, step size=1.11e+00, acc. rate=0.972] Sample: 87%|########7 | 916/1050 [00:03, 324.67it/s, step size=1.11e+00, acc. rate=0.972] Sample: 87%|########7 | 917/1050 [00:03, 324.67it/s, step size=1.11e+00, acc. rate=0.972] Sample: 87%|########7 | 918/1050 [00:03, 324.67it/s, step size=1.11e+00, acc. rate=0.972] Sample: 88%|########7 | 919/1050 [00:03, 324.67it/s, step size=1.11e+00, acc. rate=0.972] Sample: 88%|########7 | 920/1050 [00:03, 324.67it/s, step size=1.11e+00, acc. rate=0.972] Sample: 88%|########7 | 921/1050 [00:03, 324.67it/s, step size=1.11e+00, acc. rate=0.972] Sample: 88%|########7 | 922/1050 [00:03, 324.67it/s, step size=1.11e+00, acc. rate=0.972] Sample: 88%|########7 | 923/1050 [00:03, 324.67it/s, step size=1.11e+00, acc. rate=0.972] Sample: 88%|########8 | 924/1050 [00:03, 324.67it/s, step size=1.11e+00, acc. rate=0.972] Sample: 88%|########8 | 925/1050 [00:03, 324.67it/s, step size=1.11e+00, acc. rate=0.972] Sample: 88%|########8 | 926/1050 [00:03, 324.67it/s, step size=1.11e+00, acc. rate=0.972] Sample: 88%|########8 | 927/1050 [00:03, 324.67it/s, step size=1.11e+00, acc. rate=0.972] Sample: 88%|########8 | 928/1050 [00:03, 324.67it/s, step size=1.11e+00, acc. rate=0.972] Sample: 88%|########8 | 929/1050 [00:03, 324.67it/s, step size=1.11e+00, acc. rate=0.972] Sample: 89%|########8 | 930/1050 [00:03, 324.67it/s, step size=1.11e+00, acc. rate=0.972] Sample: 89%|########8 | 931/1050 [00:03, 324.67it/s, step size=1.11e+00, acc. rate=0.972] Sample: 89%|########8 | 932/1050 [00:03, 324.67it/s, step size=1.11e+00, acc. rate=0.972] Sample: 89%|########8 | 933/1050 [00:03, 331.43it/s, step size=1.11e+00, acc. rate=0.972] Sample: 89%|########8 | 933/1050 [00:03, 331.43it/s, step size=1.11e+00, acc. rate=0.972] Sample: 89%|########8 | 934/1050 [00:03, 331.43it/s, step size=1.11e+00, acc. rate=0.972] Sample: 89%|########9 | 935/1050 [00:03, 331.43it/s, step size=1.11e+00, acc. rate=0.972] Sample: 89%|########9 | 936/1050 [00:03, 331.43it/s, step size=1.11e+00, acc. rate=0.972] Sample: 89%|########9 | 937/1050 [00:03, 331.43it/s, step size=1.11e+00, acc. rate=0.972] Sample: 89%|########9 | 938/1050 [00:03, 331.43it/s, step size=1.11e+00, acc. rate=0.972] Sample: 89%|########9 | 939/1050 [00:03, 331.43it/s, step size=1.11e+00, acc. rate=0.972] Sample: 90%|########9 | 940/1050 [00:03, 331.43it/s, step size=1.11e+00, acc. rate=0.972] Sample: 90%|########9 | 941/1050 [00:03, 331.43it/s, step size=1.11e+00, acc. rate=0.972] Sample: 90%|########9 | 942/1050 [00:03, 331.43it/s, step size=1.11e+00, acc. rate=0.972] Sample: 90%|########9 | 943/1050 [00:03, 331.43it/s, step size=1.11e+00, acc. rate=0.972] Sample: 90%|########9 | 944/1050 [00:03, 331.43it/s, step size=1.11e+00, acc. rate=0.972] Sample: 90%|######### | 945/1050 [00:03, 331.43it/s, step size=1.11e+00, acc. rate=0.973] Sample: 90%|######### | 946/1050 [00:03, 331.43it/s, step size=1.11e+00, acc. rate=0.973] Sample: 90%|######### | 947/1050 [00:03, 331.43it/s, step size=1.11e+00, acc. rate=0.973] Sample: 90%|######### | 948/1050 [00:03, 331.43it/s, step size=1.11e+00, acc. rate=0.973] Sample: 90%|######### | 949/1050 [00:03, 331.43it/s, step size=1.11e+00, acc. rate=0.973] Sample: 90%|######### | 950/1050 [00:03, 331.43it/s, step size=1.11e+00, acc. rate=0.973] Sample: 91%|######### | 951/1050 [00:03, 331.43it/s, step size=1.11e+00, acc. rate=0.973] Sample: 91%|######### | 952/1050 [00:03, 331.43it/s, step size=1.11e+00, acc. rate=0.973] Sample: 91%|######### | 953/1050 [00:03, 331.43it/s, step size=1.11e+00, acc. rate=0.973] Sample: 91%|######### | 954/1050 [00:03, 331.43it/s, step size=1.11e+00, acc. rate=0.973] Sample: 91%|######### | 955/1050 [00:03, 331.43it/s, step size=1.11e+00, acc. rate=0.973] Sample: 91%|#########1| 956/1050 [00:03, 331.43it/s, step size=1.11e+00, acc. rate=0.973] Sample: 91%|#########1| 957/1050 [00:03, 331.43it/s, step size=1.11e+00, acc. rate=0.973] Sample: 91%|#########1| 958/1050 [00:03, 331.43it/s, step size=1.11e+00, acc. rate=0.973] Sample: 91%|#########1| 959/1050 [00:03, 331.43it/s, step size=1.11e+00, acc. rate=0.973] Sample: 91%|#########1| 960/1050 [00:03, 331.43it/s, step size=1.11e+00, acc. rate=0.973] Sample: 92%|#########1| 961/1050 [00:03, 331.43it/s, step size=1.11e+00, acc. rate=0.973] Sample: 92%|#########1| 962/1050 [00:03, 331.43it/s, step size=1.11e+00, acc. rate=0.973] Sample: 92%|#########1| 963/1050 [00:03, 331.43it/s, step size=1.11e+00, acc. rate=0.973] Sample: 92%|#########1| 964/1050 [00:03, 331.43it/s, step size=1.11e+00, acc. rate=0.973] Sample: 92%|#########1| 965/1050 [00:03, 331.43it/s, step size=1.11e+00, acc. rate=0.973] Sample: 92%|#########2| 966/1050 [00:03, 331.43it/s, step size=1.11e+00, acc. rate=0.973] Sample: 92%|#########2| 967/1050 [00:03, 331.43it/s, step size=1.11e+00, acc. rate=0.973] Sample: 92%|#########2| 968/1050 [00:03, 331.43it/s, step size=1.11e+00, acc. rate=0.973] Sample: 92%|#########2| 969/1050 [00:03, 338.93it/s, step size=1.11e+00, acc. rate=0.973] Sample: 92%|#########2| 969/1050 [00:03, 338.93it/s, step size=1.11e+00, acc. rate=0.973] Sample: 92%|#########2| 970/1050 [00:03, 338.93it/s, step size=1.11e+00, acc. rate=0.973] Sample: 92%|#########2| 971/1050 [00:03, 338.93it/s, step size=1.11e+00, acc. rate=0.973] Sample: 93%|#########2| 972/1050 [00:03, 338.93it/s, step size=1.11e+00, acc. rate=0.973] Sample: 93%|#########2| 973/1050 [00:03, 338.93it/s, step size=1.11e+00, acc. rate=0.973] Sample: 93%|#########2| 974/1050 [00:03, 338.93it/s, step size=1.11e+00, acc. rate=0.973] Sample: 93%|#########2| 975/1050 [00:03, 338.93it/s, step size=1.11e+00, acc. rate=0.973] Sample: 93%|#########2| 976/1050 [00:03, 338.93it/s, step size=1.11e+00, acc. rate=0.973] Sample: 93%|#########3| 977/1050 [00:03, 338.93it/s, step size=1.11e+00, acc. rate=0.973] Sample: 93%|#########3| 978/1050 [00:03, 338.93it/s, step size=1.11e+00, acc. rate=0.973] Sample: 93%|#########3| 979/1050 [00:03, 338.93it/s, step size=1.11e+00, acc. rate=0.973] Sample: 93%|#########3| 980/1050 [00:03, 338.93it/s, step size=1.11e+00, acc. rate=0.973] Sample: 93%|#########3| 981/1050 [00:03, 338.93it/s, step size=1.11e+00, acc. rate=0.974] Sample: 94%|#########3| 982/1050 [00:03, 338.93it/s, step size=1.11e+00, acc. rate=0.974] Sample: 94%|#########3| 983/1050 [00:03, 338.93it/s, step size=1.11e+00, acc. rate=0.974] Sample: 94%|#########3| 984/1050 [00:03, 338.93it/s, step size=1.11e+00, acc. rate=0.974] Sample: 94%|#########3| 985/1050 [00:03, 338.93it/s, step size=1.11e+00, acc. rate=0.974] Sample: 94%|#########3| 986/1050 [00:03, 338.93it/s, step size=1.11e+00, acc. rate=0.974] Sample: 94%|#########3| 987/1050 [00:03, 338.93it/s, step size=1.11e+00, acc. rate=0.974] Sample: 94%|#########4| 988/1050 [00:03, 338.93it/s, step size=1.11e+00, acc. rate=0.974] Sample: 94%|#########4| 989/1050 [00:03, 338.93it/s, step size=1.11e+00, acc. rate=0.974] Sample: 94%|#########4| 990/1050 [00:03, 338.93it/s, step size=1.11e+00, acc. rate=0.974] Sample: 94%|#########4| 991/1050 [00:03, 338.93it/s, step size=1.11e+00, acc. rate=0.974] Sample: 94%|#########4| 992/1050 [00:03, 338.93it/s, step size=1.11e+00, acc. rate=0.974] Sample: 95%|#########4| 993/1050 [00:03, 338.93it/s, step size=1.11e+00, acc. rate=0.974] Sample: 95%|#########4| 994/1050 [00:03, 338.93it/s, step size=1.11e+00, acc. rate=0.974] Sample: 95%|#########4| 995/1050 [00:03, 338.93it/s, step size=1.11e+00, acc. rate=0.974] Sample: 95%|#########4| 996/1050 [00:03, 338.93it/s, step size=1.11e+00, acc. rate=0.974] Sample: 95%|#########4| 997/1050 [00:03, 338.93it/s, step size=1.11e+00, acc. rate=0.974] Sample: 95%|#########5| 998/1050 [00:03, 338.93it/s, step size=1.11e+00, acc. rate=0.974] Sample: 95%|#########5| 999/1050 [00:03, 338.93it/s, step size=1.11e+00, acc. rate=0.974] Sample: 95%|#########5| 1000/1050 [00:03, 338.93it/s, step size=1.11e+00, acc. rate=0.974] Sample: 95%|#########5| 1001/1050 [00:03, 338.93it/s, step size=1.11e+00, acc. rate=0.974] Sample: 95%|#########5| 1002/1050 [00:03, 338.93it/s, step size=1.11e+00, acc. rate=0.974] Sample: 96%|#########5| 1003/1050 [00:03, 338.93it/s, step size=1.11e+00, acc. rate=0.974] Sample: 96%|#########5| 1004/1050 [00:03, 339.08it/s, step size=1.11e+00, acc. rate=0.974] Sample: 96%|#########5| 1004/1050 [00:03, 339.08it/s, step size=1.11e+00, acc. rate=0.974] Sample: 96%|#########5| 1005/1050 [00:03, 339.08it/s, step size=1.11e+00, acc. rate=0.974] Sample: 96%|#########5| 1006/1050 [00:03, 339.08it/s, step size=1.11e+00, acc. rate=0.974] Sample: 96%|#########5| 1007/1050 [00:03, 339.08it/s, step size=1.11e+00, acc. rate=0.973] Sample: 96%|#########6| 1008/1050 [00:03, 339.08it/s, step size=1.11e+00, acc. rate=0.973] Sample: 96%|#########6| 1009/1050 [00:03, 339.08it/s, step size=1.11e+00, acc. rate=0.973] Sample: 96%|#########6| 1010/1050 [00:03, 339.08it/s, step size=1.11e+00, acc. rate=0.973] Sample: 96%|#########6| 1011/1050 [00:03, 339.08it/s, step size=1.11e+00, acc. rate=0.973] Sample: 96%|#########6| 1012/1050 [00:03, 339.08it/s, step size=1.11e+00, acc. rate=0.973] Sample: 96%|#########6| 1013/1050 [00:03, 339.08it/s, step size=1.11e+00, acc. rate=0.973] Sample: 97%|#########6| 1014/1050 [00:03, 339.08it/s, step size=1.11e+00, acc. rate=0.973] Sample: 97%|#########6| 1015/1050 [00:03, 339.08it/s, step size=1.11e+00, acc. rate=0.973] Sample: 97%|#########6| 1016/1050 [00:03, 339.08it/s, step size=1.11e+00, acc. rate=0.973] Sample: 97%|#########6| 1017/1050 [00:03, 339.08it/s, step size=1.11e+00, acc. rate=0.973] Sample: 97%|#########6| 1018/1050 [00:03, 339.08it/s, step size=1.11e+00, acc. rate=0.974] Sample: 97%|#########7| 1019/1050 [00:03, 339.08it/s, step size=1.11e+00, acc. rate=0.974] Sample: 97%|#########7| 1020/1050 [00:03, 339.08it/s, step size=1.11e+00, acc. rate=0.974] Sample: 97%|#########7| 1021/1050 [00:03, 339.08it/s, step size=1.11e+00, acc. rate=0.974] Sample: 97%|#########7| 1022/1050 [00:03, 339.08it/s, step size=1.11e+00, acc. rate=0.974] Sample: 97%|#########7| 1023/1050 [00:03, 339.08it/s, step size=1.11e+00, acc. rate=0.974] Sample: 98%|#########7| 1024/1050 [00:03, 339.08it/s, step size=1.11e+00, acc. rate=0.974] Sample: 98%|#########7| 1025/1050 [00:03, 339.08it/s, step size=1.11e+00, acc. rate=0.974] Sample: 98%|#########7| 1026/1050 [00:03, 339.08it/s, step size=1.11e+00, acc. rate=0.974] Sample: 98%|#########7| 1027/1050 [00:03, 339.08it/s, step size=1.11e+00, acc. rate=0.974] Sample: 98%|#########7| 1028/1050 [00:03, 339.08it/s, step size=1.11e+00, acc. rate=0.974] Sample: 98%|#########8| 1029/1050 [00:03, 339.08it/s, step size=1.11e+00, acc. rate=0.974] Sample: 98%|#########8| 1030/1050 [00:03, 339.08it/s, step size=1.11e+00, acc. rate=0.974] Sample: 98%|#########8| 1031/1050 [00:03, 339.08it/s, step size=1.11e+00, acc. rate=0.974] Sample: 98%|#########8| 1032/1050 [00:03, 339.08it/s, step size=1.11e+00, acc. rate=0.974] Sample: 98%|#########8| 1033/1050 [00:03, 339.08it/s, step size=1.11e+00, acc. rate=0.974] Sample: 98%|#########8| 1034/1050 [00:03, 339.08it/s, step size=1.11e+00, acc. rate=0.974] Sample: 99%|#########8| 1035/1050 [00:03, 339.08it/s, step size=1.11e+00, acc. rate=0.974] Sample: 99%|#########8| 1036/1050 [00:03, 339.08it/s, step size=1.11e+00, acc. rate=0.974] Sample: 99%|#########8| 1037/1050 [00:03, 339.08it/s, step size=1.11e+00, acc. rate=0.974] Sample: 99%|#########8| 1038/1050 [00:03, 339.08it/s, step size=1.11e+00, acc. rate=0.974] Sample: 99%|#########8| 1039/1050 [00:03, 337.51it/s, step size=1.11e+00, acc. rate=0.974] Sample: 99%|#########8| 1039/1050 [00:03, 337.51it/s, step size=1.11e+00, acc. rate=0.974] Sample: 99%|#########9| 1040/1050 [00:03, 337.51it/s, step size=1.11e+00, acc. rate=0.974] Sample: 99%|#########9| 1041/1050 [00:03, 337.51it/s, step size=1.11e+00, acc. rate=0.974] Sample: 99%|#########9| 1042/1050 [00:03, 337.51it/s, step size=1.11e+00, acc. rate=0.974] Sample: 99%|#########9| 1043/1050 [00:03, 337.51it/s, step size=1.11e+00, acc. rate=0.974] Sample: 99%|#########9| 1044/1050 [00:03, 337.51it/s, step size=1.11e+00, acc. rate=0.974] Sample: 100%|#########9| 1045/1050 [00:03, 337.51it/s, step size=1.11e+00, acc. rate=0.974] Sample: 100%|#########9| 1046/1050 [00:03, 337.51it/s, step size=1.11e+00, acc. rate=0.974] Sample: 100%|#########9| 1047/1050 [00:03, 337.51it/s, step size=1.11e+00, acc. rate=0.974] Sample: 100%|#########9| 1048/1050 [00:03, 337.51it/s, step size=1.11e+00, acc. rate=0.974] Sample: 100%|#########9| 1049/1050 [00:03, 337.51it/s, step size=1.11e+00, acc. rate=0.974] Sample: 100%|##########| 1050/1050 [00:03, 307.59it/s, step size=1.11e+00, acc. rate=0.974] marginal = EmpiricalMarginal(posterior, &quot;weight&quot;) plt.hist([marginal().item() for _ in range(1000)],) plt.title(&quot;P(weight | measurement = 14)&quot;) plt.xlabel(&quot;Weight&quot;) plt.ylabel(&quot;#&quot;) 4.3.0.1 Shapes in distribution: We know that PyTorch tensor have single shape attribute, Distributions have two shape attributes with special meaning. * .batch_shape: Indices over .batch_shape denote conditionally independent random variables, * .event_shape: indices over .event_shape denote dependent random variables (ie one draw from a distribution). These two combine to define the total shape of a sample. Thus the total shape of .log_prob() of distribution is .batch_shape. Also, Distribution.sample() also has a sample_shape attribute that indexes over independent and identically distributed(iid) random variables. | iid | independent | dependent ------+--------------+-------------+------------ shape = sample_shape + batch_shape + event_shape To know more about + , go through broadcasting tensors in PyTorch. 4.3.1 Examples One way to introduce batch_shape is use expand. d = dist.MultivariateNormal(torch.zeros(3), torch.eye(3, 3)).expand([5]) # expand - 3 of these Multivariate Normal Dists print(&quot;batch_shape: &quot;, d.batch_shape) ## batch_shape: torch.Size([5]) print(&quot;event_shape: &quot;, d.event_shape) #x = d.sample(torch.Size([5])) ## event_shape: torch.Size([3]) x = d.sample() print(&quot;x shape: &quot;, x.shape) # == sample_shape + batch_shape + event_shape ## x shape: torch.Size([5, 3]) print(&quot;d.log_prob(x) shape:&quot;, d.log_prob(x).shape) # == batch_shape ## d.log_prob(x) shape: torch.Size([5]) The other way is using plate context manager. Pyro models can use the context manager pyro.plate to declare that certain batch dimensions are independent. Inference algorithms can then take advantage of this independence to e.g. construct lower variance gradient estimators or to enumerate in linear space rather than exponential space. with pyro.plate(&quot;x_axis&quot;, 5): d = dist.MultivariateNormal(torch.zeros(3), torch.eye(3, 3)) x = pyro.sample(&quot;x&quot;, d) x.shape In fact, we can also nest plates. The only thing we need to care about is, which dimensions are independent. Pyro automatically manages this but sometimes we need to explicitely specify the dimensions. Once we specify that, we can leverage PyTorch’s CUDA enabled capabilities to run inference on GPUs. with pyro.plate(&quot;x_axis&quot;, 320): # within this context, batch dimension -1 is independent with pyro.plate(&quot;y_axis&quot;, 200): # within this context, batch dimensions -2 and -1 are independent Note that we always count from the right by using negative indices like \\(-2\\), \\(-1\\). 4.3.2 Gaussian Mixture Model \\[\\texttt{Blei - Build, Compute, Critique, Repeat:Data Analysis with Latent Variable Models}\\] from __future__ import print_function import os from collections import defaultdict import numpy as np import scipy.stats import torch from torch.distributions import constraints from pyro import poutine from pyro.contrib.autoguide import AutoDelta from pyro.optim import Adam from pyro.infer import SVI, TraceEnum_ELBO, config_enumerate, infer_discrete from matplotlib import pyplot # %matplotlib inline pyro.enable_validation(True) data = torch.tensor([0., 1., 10., 11., 12.]) K = 2 # Fixed number of components. @config_enumerate def model(data): # Global variables. weights = pyro.sample(&#39;weights&#39;, dist.Dirichlet(0.5 * torch.ones(K))) scale = pyro.sample(&#39;scale&#39;, dist.LogNormal(0., 2.)) with pyro.plate(&#39;components&#39;, K): locs = pyro.sample(&#39;locs&#39;, dist.Normal(0., 10.)) with pyro.plate(&#39;data&#39;, len(data)): # Local variables. assignment = pyro.sample(&#39;assignment&#39;, dist.Categorical(weights)) pyro.sample(&#39;obs&#39;, dist.Normal(locs[assignment], scale), obs=data) 4.3.3 Review of Approximate Inference We have variables \\(Z\\)s (cluster assignments) and \\(X\\)s (data points) in our mixture model, where \\(X\\) is observed and \\(Z\\) is latent (unobserved). As we saw earlier, a generative model entails a joint distribution \\[p(Z,X)\\] Inference of unknown can be achieved through conditioning on the observations. \\[p(Z \\mid X) = \\frac{p(Z, X)}{p(X)}\\] And for the most interesting problems, the integral for the denominator(marginal) is not tractable. \\[p(X) = \\int dZp(X \\mid Z)p(Z)\\] So we have to directly approximate \\(p(Z \\mid X)\\). There are two ways of approximate this posterior. Sampling methods like Gibbs sampler. Variational inference. 4.3.4 Variational Inference: We can’t compute \\(p(Z \\mid X)\\) directly, so let’s approximate with some other distribution \\(q(Z; \\nu)\\) over Z that is tractable (for example, Gaussions or other exponential family). Image \\[\\texttt{David Blei - Variational Inference (NeurIPS 2016 Tutorial)}\\] Since q is tractable, we can play with it’s parameter \\(\\nu\\) such that it reaches as close to \\(p(Z\\mid X)\\) as possible. More precisely, we want to minimize the KL divergence between \\(q\\) and \\(p\\). With this trick, we just turned an inference problem to an optimization problem! \\[ \\begin{align*} KL(q(Z;\\nu) \\mid\\mid p(Z\\mid X)) &amp;= -\\int dZ\\ q(Z) \\log\\frac{P(Z\\mid X)}{q(Z)}\\\\ &amp;= -\\int dZ\\ q(Z) \\log \\frac{\\frac{p(Z,X)}{p(X)}}{q(Z)}\\\\ &amp;= -\\int dZ\\ q(Z) \\log \\frac{p(Z,X)}{p(X)q(Z)}\\\\ &amp;= -\\int dZ\\ q(Z) \\left[ \\log \\frac{p(Z,X)}{q(Z)} - \\log p(X) \\right]\\\\ &amp;= - \\int dZ\\ \\log \\frac{p(Z,X)}{q(Z)} + \\underbrace{\\int dZ\\ q(Z)}_{\\text{=1}}\\log p(X)\\\\ &amp;= - \\int dZ\\ \\log \\frac{p(Z,X)}{q(Z)} + \\log p(X)\\\\ \\log p(X) &amp;= KL(q(Z;\\nu)\\mid\\mid p(Z\\mid X) + \\underbrace{\\int dZ\\ q(Z;\\nu) \\log \\frac{p(Z,X)}{q(Z;\\nu)}}_{\\mathcal{L}}\\\\ \\end{align*} \\] Note that we already observed \\(X\\) and we conditioned the model to get \\(p(Z \\mid X)\\). But given \\(X\\), \\(\\log p(X)\\) is constant! So, minimizing KL is equivalent to maximizing \\(\\mathcal{L}\\). How do you maximize \\(\\mathcal{L}\\)? Take \\(\\nabla_{\\nu} \\mathcal{L}\\). \\(\\mathcal{L}\\) is called variational lower bound. It is often called ELBO. Stochastic Variational Inference scales variational inference to massive data. Just like in stochastic variational inference, you subsample the data and update the posterior! 4.3.5 Stochastic Optimization In stochastic optimization, we replace the gradient with cheaper noisy estimate which is guranteed to converge to a local optimum. \\[\\nu_{t+1} = \\nu_t + \\rho_t \\hat{\\nabla}_{\\nu} \\mathcal{L}(\\nu_t)\\] Requirements: Unbiased gradients, i.e. \\[\\mathbb{E}[\\hat{\\nabla}_{\\nu} \\mathcal{L}(\\nu_t)] = \\nabla_{\\nu}\\mathcal{L}(\\nu)\\] Step-size sequence \\(\\rho_t\\) that follows Robbins-Monro conditions. Stochastic variational inference takes inspiration from stochastic optimization and natural graidient. We follow the same procedure as stochastic gradient descent. 4.3.6 A Rough Stochastic variational inference algorithm: Initialize \\(q\\) with some \\(\\nu\\) Until Converge: subsample from Data: compute gradient \\(\\hat{\\nabla_{\\nu}}\\mathcal{L}_{\\nu_t}\\) update global parameter \\(\\nu_{t+1} = \\nu_t + \\rho_t \\hat{\\nabla_{\\nu}}\\mathcal{L}_{\\nu_t}\\) Return \\(q(Z;\\nu)\\) 4.3.7 Training a MAP estimator Let’s start by learning model parameters weights, locs, and scale given priors and data. We will use AutoDelta guide function. Our model will learn global mixture weights, the location of each mixture component, and a shared scale that is common to both components. During inference, TraceEnum_ELBO will marginalize out the assignments of datapoints to clusters. max_plate_nesting lets Pyro know that we’re using the rightmost dimension plate and that Pyro can use any other dimension for parallelization. 4.4 Some other Pyro vocabulary poutine - Beneath the built-in inference algorithms, Pyro has a library of composable effect handlers for creating new inference algorithms and working with probabilistic programs. Pyro’s inference algorithms are all built by applying these handlers to stochastic functions. poutine.block - blocks pyro premitives. By default, it blocks everything. param - Parameters in Pyro are basically thin wrappers around PyTorch Tensors that carry unique names. As such Parameters are the primary stateful objects in Pyro. Users typically interact with parameters via the Pyro primitive pyro.param. Parameters play a central role in stochastic variational inference, where they are used to represent point estimates for the parameters in parameterized families of models and guides. param_store - Global store for parameters in Pyro. This is basically a key-value store. global_guide = AutoDelta(poutine.block(model, expose=[&#39;weights&#39;, &#39;locs&#39;, &#39;scale&#39;])) optim = pyro.optim.Adam({&#39;lr&#39;: 0.1, &#39;betas&#39;: [0.8, 0.99]}) elbo = TraceEnum_ELBO(max_plate_nesting=1) svi = SVI(model, global_guide, optim, loss=elbo) def initialize(seed): pyro.set_rng_seed(seed) pyro.clear_param_store() # Initialize weights to uniform. pyro.param(&#39;auto_weights&#39;, 0.5 * torch.ones(K), constraint=constraints.simplex) # Assume half of the data variance is due to intra-component noise. pyro.param(&#39;auto_scale&#39;, (data.var() / 2).sqrt(), constraint=constraints.positive) # Initialize means from a subsample of data. pyro.param(&#39;auto_locs&#39;, data[torch.multinomial(torch.ones(len(data)) / len(data), K)]); loss = svi.loss(model, global_guide, data) return loss # Choose the best among 100 random initializations. loss, seed = min((initialize(seed), seed) for seed in range(100)) initialize(seed) print(&#39;seed = {}, initial_loss = {}&#39;.format(seed, loss)) ## seed = 7, initial_loss = 25.665584564208984 # Register hooks to monitor gradient norms. gradient_norms = defaultdict(list) for name, value in pyro.get_param_store().named_parameters(): value.register_hook(lambda g, name=name: gradient_norms[name].append(g.norm().item())) losses = [] for i in range(200): loss = svi.step(data) losses.append(loss) print(&#39;.&#39; if i % 100 else &#39;\\n&#39;, end=&#39;&#39;) ## ## ................................................................................................... ## ................................................................................................... pyplot.figure(figsize=(10,3), dpi=100).set_facecolor(&#39;white&#39;) pyplot.plot(losses) pyplot.xlabel(&#39;iters&#39;) pyplot.ylabel(&#39;loss&#39;) pyplot.yscale(&#39;log&#39;) pyplot.title(&#39;Convergence of SVI&#39;); map_estimates = global_guide(data) weights = map_estimates[&#39;weights&#39;] locs = map_estimates[&#39;locs&#39;] scale = map_estimates[&#39;scale&#39;] print(&#39;weights = {}&#39;.format(weights.data.numpy())) ## weights = [0.375 0.625] print(&#39;locs = {}&#39;.format(locs.data.numpy())) ## locs = [ 0.49887404 10.984463 ] print(&#39;scale = {}&#39;.format(scale.data.numpy())) ## scale = 0.6514337062835693 X = np.arange(-3,15,0.1) Y1 = weights[0].item() * scipy.stats.norm.pdf((X - locs[0].item()) / scale.item()) Y2 = weights[1].item() * scipy.stats.norm.pdf((X - locs[1].item()) / scale.item()) pyplot.figure(figsize=(10, 4), dpi=100).set_facecolor(&#39;white&#39;) pyplot.plot(X, Y1, &#39;r-&#39;) pyplot.plot(X, Y2, &#39;b-&#39;) pyplot.plot(X, Y1 + Y2, &#39;k--&#39;) pyplot.plot(data.data.numpy(), np.zeros(len(data)), &#39;k*&#39;) pyplot.title(&#39;Density of two-component mixture model&#39;) pyplot.ylabel(&#39;probability density&#39;); "],
["reasoning-on-dags.html", "5 Reasoning on DAGs 5.1 Recap: Causal models as generative models 5.2 Reasoning with DAGs 5.3 Causality and DAGs", " 5 Reasoning on DAGs 5.1 Recap: Causal models as generative models Our goal is to understand causal modeling within the context of generative machine learning. We just examined one generative machine learning framework called Bayesian networks (BNs) and how we can use BNs as causal models. 5.1.1 Ladder of causality (slides) Associative: Broad class of statistically learned models (discriminiative and generative) Includes deep learning (unless you tweak it) Intervention Why associative models can’t do this – changing the joint Causal Bayes nets Tradition PPL program Counterfactual Causal Bayesian networks can’t do this Structural causal models – can be implemented in a PPL 5.1.2 Some definitions and notation Joint probability distribution: \\(P_{\\mathbb{X}}\\) Density \\(P_{\\mathbb{X}=x} = \\pi(x_1, ..., x_d)\\) Bivariate \\(P_{Z, Y}\\), marginal \\(P_{Z}\\), conditional \\(P_{Z|Y}\\) Generative model \\(\\mathbb{M}\\) is a machine learning model that “entails” joint distribution, either explicitly or implicitly We denote the joint probability distribution “entailed” by a generative model as \\(P_{\\mathbb{X}}^{\\mathbb{M}}\\) Directed acyclic graph DAG \\(\\mathbb(G) = (V, E)\\), where E is a set of directed edges. Parents in the DAG: Parents of \\(X_j\\) in the DAG \\(\\mathbb(G)\\) is denoted \\(\\text{pa}_j^{\\mathbb{G}}\\) A Bayesian network is a generative model that entails a joint distribution that factorizes over a DAG. A causal generative model is a generative model of a causal mechanism. A causal Bayesian networks is a causal generative model that is simply a Bayesian network where the direction of edges in the DAG represent causality. Probabilistic programming: Writing generative models as program. Usually done with a framework that provides a DSL and abstractions for inference “Causal program”: Let’s call this a probabilistic program that As with a causal Bayesian network, you can write your program in a way that orders the steps of its execution according to cause and effect. 5.1.3 Difference between Bayesian networks and probabilistic programming BNs have more constraint Probabilistic relationships limited to conditional probability distributions (CPDs) factored according to a DAG. Frameworks typically limit you to a small set of parametric CPDs (e.g., Gaussian, multinomial). bnlearn allows multinomial or ordinal variables for discrete, Gaussian for continuous. PPLs let you represent relations any way you like so long as you can represent them in code. Nonparameterics Strange distributions Control flow and recursion DAGs all variables are known in advance “Open world models”: control flow may create new variables (blackboard) X = Bernoulli(p) if X == 1: Y = Gaussian(0, 1) X = Poisson(λ) Y = zeros(X) Y[0] = [Gaussian(0, 1)] for i in range(1, X): Y[i] = Gaussian(Y[i-1], 1)) Inference Inference is easier in BNs given the constraints PGM inference such as belief probagation, variable elimination In PPLs inference is tougher Require you to become something of an inference expert That said, PPL developers provide inference abstractions so you don’t have to work from scratch PPLs use cutting-edge inference algorithms Include tensor-based frameworks like Tensorflow and PyTorch, allow you to build on data science intuition. Stochastic variation inference Minibatching – process groups of training examples simultaneously to take advantage of modern hardware like GPUs Finally, it is easier to reason about the joint distribution if you have a DAG. 5.2 Reasoning with DAGs 5.2.1 Intuition DAGs as a graphical language for reasoning about conditional independence Impossible to learn a language all at once, we’ll focus on learning what it can do for us 5.2.2 Reading DAGs as factorizations 5.2.2.1 Recap on core concepts Conditional probability Conditional independence Notation: \\(U \\perp_{P_{\\mathbb{X}}} W|V\\) Implications to factorization Conditional independence in the joint changes the DAG 5.2.3 Core graphical concepts A path in \\(\\mathbb{G}\\) is a sequence of edges between two vertices todo: formal notation, see page 82 of Peters 2017 Pearl’s d-seperation – Reading conditional independence from the DAG todo: formal notation So what? We saw how conditional independence shapes the DAG. This shows V-structures / Colliders moral v-structure immoral v-structure and conditional independence Sprinkler example 5.2.4 Taking a step back – what does conditional independence have to do with causality? correlation vs causation Latent variables and confounding That v-structure example was causal If you can’t remember, use the algorithm (bnlearn, pgmpy) Reduces the problem to reasoning about the joint probability distribution to graph algorithms. Without a DAG, no d-separation. Could there be a more general form of d-separation that could operate on a probabilistic program? 5.2.5 Markov Property Markov blanket probability definition DAG definition (slides) Implications to prediction Markov property (slides) Global Local Markov factorization Markov equivalence Recap: The definition conditional probability is P(A|B) = P(A,B)/P(B) (blackboard) This definition means you can factorize any joint into a product of conditionals. For example P(A, B, C) = P(A)P(B|A)P(C|A, B) A product of conditionals can be represented as a DAG. In this case with edges {A-&gt; B, B -&gt; C, A-&gt;C}. But you can also factorize P(A, B, C) in to P(C)P(B|C)P(A|B,C), getting edges {C-&gt;B, B-&gt;A, C-&gt;A}. So you have two different DAGs that are equivalent factorizations of the joint probability. Call this equivalence Markov equivalence. Generally, given a DAG, the set that includes that DAG and all the DAGs that are Markov equivalent to that DAG are called a Markov equivalence class. PDAG is a compact representation of the equivalence class When you have an equivalence class of some thing, trying to find some meaningful representation of that class without enumerating all of its members is a hard thing to do. Usually the best you can do is look for some kind of isomorphism between two objects to test if they are equivalent, according to some definition of equivalence The nice thing about the equivalence classes of DAGs is that all the DAGs will have the same “skeleton”, meaning set of connections between nodes. The difference is that some or all of those edges will have different directions in different DAGs. This is where we get the PDAG as a compact representation of an equivalence class. The PDAG has the same skeleton as all of the members of the equivalence class. The undirected edges in the PDAG correspond to edges that vary in direction among members of the class. A directed edge in the PDAG mean that all members of the class have that edge oriented in that direction. There are other graphical representations of joint probability distributions Undirected graph – doesn’t admit causal reasoning Ancestral graphs (slide) – Doesnt directly map to a generative model 5.3 Causality and DAGs Assume no latent variables (very strong assumption) Causation vs correlation – only two options A second look at PDAGs "],
["introduction-to-interventions.html", "6 Introduction to Interventions 6.1 Recap: DAGs as a grammar for reasoning about a joint distribution 6.2 Ladder of causality (slides) 6.3 Interventions and implications to prediction 6.4 Structural Causal Models", " 6 Introduction to Interventions 6.1 Recap: DAGs as a grammar for reasoning about a joint distribution Review of generative modeling definitions DAGs provided a graph theory-based grammar for reasoning about a joint distribution They tell you what is dependent and what is conditionally independent Causal Bayesian networks follow the Markov property. One way of expressing this is the local Markov property (slides) Each factor in the local Markov property factorization is represented as some conditional probability distribution. If we follow the ordering of the DAG, we get the algorithm for generating the data. 6.2 Ladder of causality (slides) Associative: Broad class of statistically learned models (discriminiative and generative) Includes deep learning (unless you tweak it) Intervention Why associative models can’t do this – changing the joint Causal Bayes nets Tradition PPL program Counterfactual Causal Bayesian networks can’t do this Structural causal models – can be implemented in a PPL 6.3 Interventions and implications to prediction Definition in terms of probability distribution Implications to prediction: Predicting the weather vs predicting the sales Perfect/ideal intervention - artificially assigning the value of a random variable Perfect intervention represented as a node in a Causal Bayes Net Represented as variable with no causes and two states (on/off). Outcome is deterministic, irregardless of other parents Can be awkward to represent in terms of conditional probability I X Y Prob 0 0 0 .8 0 0 1 .2 0 1 0 .1 0 1 1 .9 1 \\(\\forall x\\) 1 1.0 Can be more awkward for continuous random variables \\[Y \\sim \\left\\{\\begin{matrix} \\text{Normal}(\\beta X + \\alpha, 1) &amp; \\text{I} = 0 \\\\ \\text{Dirac}(y)) &amp; \\text{I} = 1 \\end{matrix}\\right.\\] In a probabilistic program : y = 0 # or some other intervention value def program(I): X ~ Normal(0, 1) if I: Y ~ Normal(beta X + alpha, 1) else: Y ~ Dirac(y) # or just `Y = y` Perfect interventions as graph mutilation – “mutilate” the DAG by removing incoming edges the intervened upon variable implemented in bnlearn (slides) Equivalence class as graph manipulation Finding the PDAG of the mutilated class Other kinds of interventions Soft interventions Changing or replacing the distribution, but the outcome is not deterministic Change of slope example Not implemented in Pyro (here’s what syntax might look like) Fat-fingered interventions (board) Randomization as intervention – breaking influence of latent confounders Probabilistic program for randomization using metaprogramming Interventions as Pearl’s Do-calculus \\(p(y|do(x))\\): What is the distribution of Y if I were to set the value of X to x. Generally different from $ p(y|x)$ Notation: \\(P^{\\text{do}(X = x)}\\) vs \\(P^{(X = x)}\\) do operator as metaprogramming Pyro Advantages of the do-operater Consider that it allows us to reason about interventions, when it is not feasable to do the intervention That is why they are called “perfect” or “ideal” Powerful – means we can answer causal questions without running an experiment Controversy about the do-operator What does it mean to intervene on ‘obesity’? What does it mean to intervene on race? (slides) 6.4 Structural Causal Models "],
["calculating-intervention-distributions-by-covariate-adjustment.html", "7 Calculating intervention distributions by covariate adjustment 7.1 Review of causal sufficiency and interventions 7.2 Review of intervention 7.3 Intuition about experimental effects and confounding 7.4 Graph-based 7.5 Calculating intervention distributions by covariate adjustment 7.6 Truncated formula AKA g-formula 7.7 Valid adjustment sets", " 7 Calculating intervention distributions by covariate adjustment 7.1 Review of causal sufficiency and interventions Global Markov property Causal faithfulness Causal minimality Faithfulness implies causal minimality 7.2 Review of intervention Intervention is an artificial manipulation of the DAG. Interventions change the joint distribution such that the intervened variable no longer correlates with its causes. Randomized experiments are a type of intervention. Ladder of causality – How can we predict interventions? Associational models don’t have an a way of calculating how the joint distribution changes under intervention. Best you can do is actually perform interventions and include the intervention data in your training data. Causal models allows us to predict the effect of an intervention. Why would we want to predict an intervention? If a randomized experiment is a type of intervention, then you might ask why we would want to predict the outcome of a randomized experiment. Randomized experiment may be costly in terms of time and resources. They may be impossible or unethical to run. But wait, why do we run randomized experiments again? 7.3 Intuition about experimental effects and confounding Confounding example 1 I want to know the effect someone screaming “fire” in a movie theater has on making people run for the fire exit. But when people usually scream fire, there is usually a fire. When people run to the exit, are they responding to the scream or to the fire? So here we are interested in a direct relationship between (scream and fire), and are having troubling seperating it from indirect relationship between screaming and the actual presence of a fire, as well as people running and the actual effect of a fire. Confounding example 2 Running an A/B test. So on Monday morning, you send all the users to A. On Monday evening, you send all the users to B. You have asked a question of the universe: What the difference is between A and B under the conditions of the experiment? Key word here is “difference” \\[ \\begin{align} &amp; P(R = 1 | T = a) - P(R = 1 | T = b) \\nonumber\\\\ =&amp; \\sum_{z}P(R = 1 | T = a, Z=z)P(T=a|Z=z)P(Z=z) - \\sum_{z}P(R = 1 | T = b, Z=z)P(T=b|Z=z)P(Z=z) \\nonumber \\end{align} \\] These probabilities vary depending on what level of z we are looking at. We know what is wrong with our experiment – randomization. How does it fix this problem? Can you explain it without graphs? Language problem Statistics cannot define the term “confounding”, need a causal grammar. Similarly, interpret the differences between treatment populations. 7.4 Graph-based Confounding bias occurs when a variable influences both who is selected for the treatment and the outcome of an experiment. Sometimes they are known, sometimes they are latent. Contrast this with a latent variable model where we are typically trying to infer the state of the latent. What about if we want to predict the latent, but there is another confounder? Topic model example What does it mean to “control for something”? In terms of tables? 7.5 Calculating intervention distributions by covariate adjustment We have a causal model \\(\\mathbb{M}\\) that entails the joint distribution \\(\\mathbb{P}\\). Our model is a machine that gives us joint, conditional, or marginal probability of any outcome (though we may need an inference algorithm to compute the probability). Recall local Markov property: \\[p^{\\mathbb{M}}(x_1, ..x_d)= \\prod_{j=1}^{d}p^{\\mathbb{M}}(x_j|x_{pa(j)})\\] Our causal model’s DAG gives use this factorization. The intervention question: if an intervention changes joint distribution, how can we calculate the results of an intervention without actually having to do the intervention? Covariate adjustment: calculating the results of interventions without do-calculus (graph mutilation). Motivation? The Do-calculus is a tool. We better understand the power of a tool and how it works if we understand exactly what we can accomplish without the tool. Much of the causal inference community doesn’t use do-calculus, but they do use covariate adjustment. Will give us a better understanding of confounding – we said a RCT is a c Given a model entailing \\(\\mathbb{M}\\), if we apply an intervention to the model and acquire \\(\\mathbb{\\tilde{M}}\\), then \\[\\pi_{\\mathbb{M}}(x_j | x_{\\text{pa(j)}}) = \\pi_{\\mathbb{M}}(x_j | x_{\\text{pa(j)}})\\] 7.6 Truncated formula AKA g-formula 7.6.1 Invariance property of interventions Assume we have a model \\(\\mathbb{M}\\), that factorizes according to some DAG. Then by the local Markov property: \\(p^{\\mathbb{M}}(x_1, ..x_d) &amp;= \\prod_{j}^{d}p^{\\mathbb{M}}(x_j|x_{pa(j)})\\), where \\(x_{pa(j)}\\) is a vector of values for the parents of \\(X_j\\) in the DAG. So we know the values for each factor \\(p^{\\mathbb{M}}(x_j|x_{pa(j)})\\) ( – that’s part of what the model encodes. Again, what we don’t know is what the new distribution under intervention is going to be. Let \\(\\mathbb{\\tilde{M}}\\) be the mutated (mutilated) model we get after we apply a soft intervention \\(do(X_k := \\tilde{N})\\), where \\(\\tilde{N}\\) has a probability density function \\(\\pi\\). Then according to the local Markov property. \\[ \\begin{align} p^{\\mathbb{\\tilde{M}}}(x_1, ..x_d) &amp;= p^{\\mathbb{M}; do(X_k = \\tilde{N})}(x_k) \\prod_{j\\neq k}^{d}p^{\\mathbb{M}}(x_j|x_{pa(j)}) \\nonumber \\\\ &amp;= \\pi(x_k) \\prod_{j\\neq k}^{d}p^{\\mathbb{M}}(x_j|x_{pa(j)}) \\nonumber \\end{align} \\] The interventions changes only the factor \\(p^{\\mathbb{M}}(x_k|x_{pa(k))}\\) – it becomes \\(\\pi(x_k) = p^{\\mathbb{M}; do(X_k = \\tilde{N})}(x_j)\\), which no longer depends on the parents \\(x_{pa(j)}\\). The key thing here is that, all of the factors from \\(\\mathbb{M}\\) are the same in \\(\\mathbb{\\tilde{N}}\\). In the special case of a hard intervention, this simplifies to \\[ \\begin{align} p^{\\mathbb{M}; \\text{do}(X_k = a)}(x_1, ..x_d) = \\left\\{\\begin{matrix} \\prod_{j\\neq k}^{d}p^{\\mathbb{M}}(x_j|x_{pa(j)}) &amp; \\text{if} X_k = a \\\\ 0 &amp; \\text{otherwise} \\end{matrix}\\right. \\nonumber \\end{align} \\] 7.6.2 Conditioning and do are the same for variables without parents Consider what would happen if \\(x_k\\) had no parents? \\[ \\begin{align} p^{\\mathbb{M}}(x_1, ..x_d|X_k =a) &amp;= \\frac{\\prod_{j}^{d}p^{\\mathbb{M}}(x_j|x_{pa(j)}) }{P^{\\mathbb{M}}(x_k=a)}\\nonumber \\\\ &amp;= \\frac{p^{\\mathbb{M}}(x_k|x_{pa(k)}) \\prod_{j\\neq k}^{d}p^{\\mathbb{M}}(x_j|x_{pa(j)})}{P^{\\mathbb{M}}(X_k=a)} \\nonumber \\\\ &amp;= p^{\\mathbb{M}}(x_k) \\prod_{j \\neq k}^{d}p^{\\mathbb{M}}(x_j|x_{pa(j)}) \\nonumber \\\\ &amp;= \\left\\{\\begin{matrix} \\prod_{j\\neq k}^{d}p^{\\mathbb{M}}(x_j|x_{pa(j)}) &amp; \\text{if} X_k = a \\\\ 0 &amp; \\text{otherwise} \\\\ \\end{matrix}\\right. \\nonumber \\\\ &amp;= p^{\\mathbb{M}; \\text{do}(X_k = a)}(x_1, ..x_d) \\end{align} \\] 7.7 Valid adjustment sets Valid adjustment sets, and the problem of over controlling. Even amongst statisticians, knowing what to control for, or what confounding is, has been a problem. This motivates all the work we are doing in parsing DAGs. Parsing Ezra Klein TODO Gender wage gap example. “The question to ask about the various statistical controls that can be applied to shrink the gender gap is what are they actually telling us… The answer, I think, is that it’s telling how the wage gap works.” One should not control for things that are part of the causal mechanism. \\(p^{\\mathbb{M}; X =x}(y)&amp;= \\sum_{z} p^{\\mathbb{M}; X =x}(y|x, z)p^{\\mathbb{M}}(z)\\) Parent adjustment Backdoor criterion Toward neccessity Front door If we do not observe the latent, we can’t use the back-door, but we can: \\(p^{\\mathbb{M}; \\text{do}(X=x)}(y)= \\sum_z p^{\\mathbb{M};X=x}(z) \\sum_{\\tilde{x}} p^{\\mathbb{M};X=\\tilde{x}Z=z}(y) p^{\\mathbb{M}}(\\tilde{x})\\) "],
["confounding-paradoxes.html", "8 Confounding, Paradoxes 8.1 A second look at confounding 8.2 Monte Hall 8.3 Berkson Paradox 8.4 Examples of valid adjustment 8.5 Covariate adjustment: Simpson’s Paradox 8.6 Front-door adjustment 8.7 Propensity score", " 8 Confounding, Paradoxes 8.1 A second look at confounding Confounding Interested in an average treatment effect. In context of randomized A/B, this is the difference on average between outcome under group A and outcome under group B More generally, the average difference between interventions. So we want this intervention distribution. The adjustment criterion tell us what variables we can control for. We think of these in terms of sets, because some confounders may be hidden. It may not be possible to control for everythings The adjustment formula tells us that once we have the covariate-adjustment formula, we can estimate this intervention distribution. 8.2 Monte Hall Describe problem Game show Choose a door Monte will open a door that does not have the car Should always pick. Monty Hall must open a door that does not have a car behind it chosen door -&gt; door open &lt;- location of the car (scribe create figure) Door opened is a collider 8.3 Berkson Paradox Two features seem to have no relation to each other in general, they can appear to be associated within a context This is the core of sampling bias 8.4 Examples of valid adjustment 8.5 Covariate adjustment: Simpson’s Paradox You are a data scientist at a prominent tech company with paid subscription entertainment media streaming service. You come across there results of an A/B test that a rival data scientist ran. The test targeted 70K subscibers users who were coming to a subscription renewal time and were at high risk of not renewing. They were targeted with either treatment 0 - a personalized promotional offer that gave the user reduced rates on the media they consume the most, or treatment 1 - a promotional offer that gave the user reduced rates on a general set of content. Overall Treatment 0: Personalized Promotion 77.9% (27272/35000) Treatment 1: Generalized Promotion 82.6% (28902/35000) This reads that 78% of the users who recieved the personalized promotion ended up renewing their subscription, while 83% of those who recieved the generalized promotion ended up renewing. Let R be the 0 if a subscriber leaves, and 1 if a subsciber stays. In his report, the analyst quantified the effect size as: \\[ E(R | T = 0) - E(R | T = 1) = P(R = 1 | T = 0) - P(R = 1 | T = 1) \\approx .779 - .826 = -0.047 \\] … where .779 and .826 are empirical estimates. So the conclusion was that the probability a subscriber stays is nearly .05 higher on the generalized promotion than on the personalized promotion. The marketing executives took this as a no-brainer. While the effect size is small, the p-value was near 0 so these results were significant (never mind that this was simply because sample size is large, which is typically the case in tech). Users generally prefer high quality content, and high quality content generally has higher royalty costs. So personalized promotions are typically more expensive than the generalized ones, where you can mix less popular but more cost-effective content into the promotion. So if the generalized promotion is cheaper AND performs better in the test, then its clearly better choice as a policy for dealing with subscribers who at high risk of leaving… Right? Curious, you find the SQL query that generated the data, and play around with selecting a few more columns and joining a few other tables. You notice that for these subscribes, there was also data on how happy the customers were, based on interactions with customer service. You create a new table that works in this new Z(not disgruntled/disgruntled) variable. Overall Not disgruntled Disgruntled Treatment 0: Personalized Promotion 77.9% (27272/35000) 93.2% (8173/8769) 73.3% (19228/26231) Treatment 1: Generalized Promotion 82.6% (28902/35000) 86.9% (23339 / 26872) 68.7% (5582/8128) Lo and behold, the conclusion is reversed within each level of Z! While generalized promotion seems favorable relative to personalized promotion in general, the personalized promotion seems to perform better within each of the non-disgruntled and disgruntled subgroups. This turns out to be an example of Simpon’s paradox. Suppose the true underlying model has the following DAG: simpson_model …where Z is 0 for non-disgruntled, 1 if disgruntled. Consider two SCMs \\(\\mathbb{C}^{do(T:=0)}\\) and \\(\\mathbb{C}^{do(T:=0)}\\) that are obtained by interventions setting \\(T := 0\\) and \\(T := 1\\). Let \\(P^{\\mathbb{C};do(T:=0)}\\) and \\(P^{\\mathbb{C};do(T:=0)}\\) denote the probability distributes entailed by these SCMs. Calculate the expected difference in outcome between the two treatments using the empirical counts in the above table $ E^{T=0}(R) - E^{T=1}(R)$. Calculate analytical the Average Treatment Effect using the empirical values in the table: \\[ \\begin{align*} ATE &amp;= E^{do(T:=0)}(R) - E^{do(T:=1)}(R)\\\\ &amp;= P^{\\mathbb{C};do(T:=0)}(R=1) - P^{\\mathbb{C};do(T:=1)}(R-1)\\\\ \\ \\\\ P^{\\mathbb{C};do(T:=0)}(R=1) &amp;= \\sum_{z=0}^1 P^{\\mathbb{C};do(T:=0)}(R=1, Z=z) \\\\ &amp;= \\sum_{z=0}^1 P^{\\mathbb{C};do(T:=0)}(R=1, T=0, Z=z) \\\\ &amp;= \\sum_{z=0}^1 P^{\\mathbb{C};do(T:=0)}(R=1| T=0, Z=z) P^{\\mathbb{C};do(T:=0)}(T=0, Z=z) \\\\ &amp;= \\sum_{z=0}^1 P^{\\mathbb{C};do(T:=0)}(R=1| T=0, Z=z) P^{\\mathbb{C};do(T:=0)}(Z=z) \\\\ &amp;= \\sum_{z=0}^1 P^{\\mathbb{C}}(R=1| T=0, Z=z) P^{\\mathbb{C}}(Z=z) \\\\ &amp;\\approx .932 * 35641/70000 + .733 * 34359/70000 = .8343 \\ \\\\ P^{\\mathbb{C};do(T:=1)}(R=1) &amp;\\approx .869 * 35641/70000 + .687 * 34359/70000 = .7818 \\ \\\\ \\ \\\\ ATE &amp;= .8343 - .7818 = 0.0525\\\\ \\end{align*} \\] 8.6 Front-door adjustment Front-door criterion: A set of variables Z is siad to satisfy the front-door criterion relative to an ordered pair of variables (X, Y) if Z intercepts all directed paths from X to Y There are no unblocked paths from X to Z All backdoor paths from Z to Y are blocked by X You are a data scientist investing the effects of social media use on a purchase. You assume the following DAG Circles mean unobserved, squares mean observed In this model the causal effect of social media on conversions is not identifiable; one can never ascertain which portion of the observed correlation between X and Y is attributed to user context U. It is worth noting that there are ways of analyzing how strong that confounding effects must be in order to entirely explain the association between X and Y Now suppose you modify the SQL query and get an additional variable: whether or not the person was using an ad blox. In this case we can apply the front-door criterion. Assume that were query the database for a past experiment where a randomly selected sample of 800000 “whales” – tech lingo for users who generally have a high conversion rate (because of evironmental factors, like their generation or social/professional in-group). Assume the following tqble (blackboard) One person on your team argues that the table proves that social media does not drive conversions. They point to the fact that only 15% of people who converted used social media, compared to 92.25% of people who don’t use social media. Another member oof your team argues that social media use actual increases, not decreases, conversions. Their argument is as follows: If you use social media, then your chances of seeing a high level of ads is 95% (380/400) compared to 5% if you do not use social media (20/400). The effect of ad exposure, if we look seperately at the two groups, social media users and non-users in in the second table (blackboard). In social media users it increases conversion rates from 10% to 15% in non-social media users it increases conversion rates from 90 to 95% Here is how we break the stalemate with a technique called the front-door formula: First, we see that the effect of X on Z is identifiable because there is no backdoor path from Z to X. \\(P(Z = z|do(X = x))\\) = P(Z = z|X = x) Next, we not that the effect of Z on Y is identifiable. The backdow path from Z to Y, namely Z &lt;- X &lt;- U -&gt; Y can conditioning on X. \\(P(Y = y|do(Z = z)) = \\sum_x P(Y = y|Z = z, X=x)P(X = x)\\) We chain toghether these two parital effects to obtain the overall effect of X on Y. If nature chooses to assign Z the value z, then the probability of Y would be \\(P(Y=y | do(Z = z))\\). The probability that nature would choose to do that, given that we choose to set X to x is \\(P(Z = z|do(X = x))\\) Summing up over all the possible states z of Z we have \\[ P(Y = y|do(X = x)) = \\sum_Z P(Y = y |do(Z = z))P(Z =z|do(X = x)) \\] Finally, we replace the do-expressions with their covariate adjustment counterparts. The final expression is \\[P(Y = y|do(X = x)) = \\sum \\sum_{x&#39;} P(Y = y|Z = z, X = x&#39;)P(X = x&#39;)P(Z = z|X = x)\\] 8.7 Propensity score Consider the following case of confounding: The set {Z1, Z2, Z3} is a valid adjustment set by parent adjustment So we could estimate the intervention distribution using \\(p^{M; do(X:=x)}(y) = \\sum_{z_1, z_2, z_3}p^{\\mathbb{M}}(y|x, z_1, z_2, z_3)p^{\\mathbb{M}}(z_1, z_2, z_3)\\) Here we consider the case where Z1, Z2, and Z3 there exists some function \\(L(Z_1, Z_2, Z_3)\\) that renders X conditionally independent from Z1, Z2, Z3, i.e. $ X{ Z_1, Z_2, Z_3 } |L(Z_1, Z_2, Z_3)$ To help imagine this, it modify our causal model to add \\(L(Z_1, Z_2, Z_3)\\) to the graph as a type of continuous causal AND gate as in . Let \\(l\\) represent a value codomain of \\(L(.)\\) Our new adjustment formula is \\(p^{M; do(X:=x)}(y) = \\sum_{l}p^{\\mathbb{M}}(y|x, l)p^{\\mathbb{M}}(l)\\) What have we gained here? \\(p(y |x, l)\\) is potentially of lower dimention that \\(p(y|x, z_1, z_2, z_3)\\). It might be computationally easier to estimate \\(p^{M; do(X:=x)}(y)\\) with covariate adjustment over \\(L(.)\\) than over Z1, Z2, Z3. In industry and practice, you often hear of “propensity matching”, where you try to group examples that have similar values of \\(L(.)\\) and then calculste cause effects between X and Y within those groups. In other words, you control for/condition on/adjust for L Next class (scribes delete) * Structural causal models * Instrumental variables * Potential outcomes * Inverse proability weighting * mediation "],
["other-stuff.html", "9 Other stuff 9.1 Recap 9.2 How to simulating adjustment using a propensity score function and inverse probability weighting 9.3 Structural causal models 9.4 Causal inference in linear systems 9.5 Counterfactuals 9.6 Bayesian counterfactual algorithm with SMCs in Pyro 9.7 Mediation", " 9 Other stuff 9.1 Recap On the difference between covariate adjustment and do-calculus Covariate adjustment allows us to predict the results of an intervention. Requires we observe the variables required to do the adjustment Covariate adjustment is taking a weighted average of an effect over every combination of strata in the adjustment set. This is practically difficult (in terms of computation and expectation) if the number of strata is large. If Z is continuous, then you have to integrate, which has its own set of practical challenges. Do-calculus is simulation of an intervention When you don’t have the variables neccessary to do covariate adjustment, you can still use do-calculus 9.2 How to simulating adjustment using a propensity score function and inverse probability weighting This is useful when adjusting over all the strata in the adjustment set is practically difficult Adjustment formula: \\[P(Y=y|do(X = x)) = \\sum_{Z} P(Y= y|X = x, Z = z)P(Z = z)\\] Looking just at \\(P(Y= y|X = x, Z = z)\\), Baye’s rule tells us that: \\[P(Y= y|X = x, Z = z) = \\frac{P(X = x, Y= y, Z = z)}{P(X = x, Z = z)}\\] Bring back \\(P(Z = z)\\) \\[\\begin{align} P(Y= y|X = x, Z = z)P(Z=z) &amp;= \\frac{P(X = x, Y= y, Z = z)}{P(X = x, Z = z)}P(Z=z) \\\\ &amp;= \\frac{P(X = x, Y= y, Z = z)}{P(X = x| Z = z)P(Z=z)}P(Z=z) \\\\ &amp;= \\frac{P(X = x, Y= y, Z = z)}{P(X = x| Z = z)} \\end{align}\\] Therefore we can rewrite the adjustment formula as: \\[P(Y=y|do(X = x)) = \\sum_z \\frac{P(X = x, Y= y, Z = z)}{P(X = x| Z = z)}\\] Suppose we are able to estimate a propensity score function \\(g(x, z) = P(X = x | Z = z)\\) Then we can estimate \\(P(Y = y|do(X =x))\\) using the following inverse probability weighting algorithm “”&quot; # using Pyro-ish code # M is desired number of samples samples = [] weights = [] for i in M: x, y, z = model() sample_prob = model.prob(x, y, z) propensity_score = g(x, z) weight = sample_prob / propensity_score samples.append((x, y, z)) weights.append(weight) # resample according to new weights new_samples = resample(samples, weights = weights) “”&quot; It is called inverse probability weighting because you multiply the joint probability of a sample by the inverse of a probability, in this case \\(g(x, z) = P(X = x|Z =z)\\) The frequencies in new_samples is such that you can estimate \\(P(Y=y|do(X = x)\\) with \\(\\hat{p}(Y = y|X = x)\\), where \\(\\hat{p}\\) is a proportion in new_samples. 9.3 Structural causal models Recall Laplace’s demon example As we mentioned before, a structural causal model is a deterministic extention to \\(\\mathbb{C}\\) has a causal DAG \\(\\mathbb{D}\\). Assume there are \\(J\\) random variables in the DAG. Each varible in the DAG is paired with on independent random variables called exogenous noise terms, I will call them noise terms A distribution \\(P_{\\mathbf{N}}^{\\mathbb{C}}\\) on independent random variables \\(\\mathbf{N} = \\{N_i; i \\in J \\}\\) The value of each variable is set deterministically by a function \\(f_i\\) for the ith random variable called a structural assignments, such that \\(X_i = f_i(\\mathbf{PA}_{\\mathbb{C}, i}, N_i), \\forall i \\in J\\) where \\(\\mathbf{PA}_{\\mathbb{C}, i} \\subseteq \\mathbf{X} \\setminus X_i\\) are the parents of \\(X_i\\) in \\(\\mathbb{D}\\). Some draw the noise terms, I usually do not. \\(\\mathbb{C}\\) is a generative model that entails \\(P^{\\mathbb{G}}\\), the same observational distribution as \\(\\mathbb{G}\\) These are going to allow us to compute counterfactuals, they are on the highest rung of the ladder. They are not the only model on the top rung. In a subsequent class, I will introduce some generalizations of structural causal models to open universe models. 9.4 Causal inference in linear systems So far we have focused on covariate adjustment with discrete variables. We’ve avoided continuous variables generally for a few reasons. Setting an intervention to point on a continuous domain seems weird. Why \\(do(X = 1.0)\\) and not \\(do(X = 1.00001)\\)? Integration and Bayesian probability math is practically challenging. The math of course is simpler when you use linear modeling with Gaussian distributions. However, this class casts causal modeling as an extention of generative machine learning; in cutting-edge generative machine learning you generally don’t see a lot of linear modelling. However, we do touch on a few cases fundamental topics that come up in the causal linear modeling literature. 9.4.1 Covariate adjustment example: Continuous adjustment We have been talking about thinking of causal effects as differences, what might this look like in the continuous case? \\(\\frac{d}{dx}E^{\\mathbb{M};do(X:=x)}(Y)\\) Linear case – Z in valid adjustment set Nonlinear case: Monte Carlo Sampler. Recall that. \\(\\frac{d f(x)}{dx} = \\lim_{\\delta \\rightarrow 0} \\frac{f(x + \\delta) -f(x)}{\\delta}\\) 9.4.2 Instrumental variables Consider a structural causal model with the following DAG Consider the structural assignment for Y: \\(Y := \\alpha X + \\delta Z + N_Y\\) We are interested in the causal effect \\(\\alpha\\). Let \\(\\hat{\\alpha}\\) be our least-squares estimate of \\(\\alpha\\). Here confounding shows up as a bias in the standard regression estimator \\(\\alpha\\): \\[E(\\hat{\\alpha}) = \\frac{\\text{cov}(X, Y)}{\\text{var}(Y)} = \\frac{\\alpha \\text{var}(X) + \\delta \\gamma \\text{var}(Z)}{\\text{var}(X)} = \\alpha + \\frac{\\delta \\gamma \\text{var}(Z)}{\\text{var}(X)} \\neq \\alpha \\] An instumental variable \\(I\\) for \\(X, Y\\) is one where: \\(I\\) is independent of \\(Z\\) \\(I\\) is not independent of \\(X\\) \\(I\\) affects \\(Y\\) only through \\(X\\) Two-stage least squares estimation using an instrumental variable algorithm: Regress X on Z and get \\(\\hat{\\beta}\\) estimate of \\(\\beta\\) Regress Y on the predicted values of the first regression \\(\\hat{\\beta}Z\\) The coefficient of \\(\\hat{\\beta}Z\\) becomes is a consistent estimate of \\(\\alpha\\). Statistical intuition: Looking at the stuctural assignment for \\(X\\): \\[X:= \\beta I + \\gamma Z + N_X\\] Since \\(Z\\) and \\(N_X\\) are independent of \\(I\\), then covariance between \\(Z, N_X, \\hat{\\gamma}\\) is 0. So we can treat \\(\\gamma Z + N_X\\) as a big noise term, and treat \\(\\beta Z\\) as a stand in for X. We essentially modifiy Y’s to be : \\[Y := \\alpha (\\beta Z) + (\\alpha \\gamma + \\delta)Z + N_Y\\] and fit it using least-squares. 9.5 Counterfactuals Notation Reasoning through inference algorithm with SMC: Eye disease model \\[\\begin{align} T &amp;= N_T\\\\ B &amp;= T * N_B + (1-T)*(1-N_B) \\\\ N_T ~ Ber(.5), N_B ~ Ber(.01) \\end{align}\\] Suppose patient with poor eyesight comes to the hospital and goes blind (B=1) after the doctor gives treatment (T=1). We ask “what would have happened had the doctor administered treatment T = 0?” B = T = 1 means the \\(N_B\\) was 1. Given \\(N_B\\) equals 1, we calculate the effect of \\(do(T = 0)\\) under new model \\[\\begin{align} T &amp;= 1\\\\ B &amp;= T * 1 + (1-T)*(1-1) = T \\\\ \\end{align}\\] 9.6 Bayesian counterfactual algorithm with SMCs in Pyro Condition on observed data Infer the noise terms Apply do operator Forward from noise posterior after having applied do operation. 9.7 Mediation Motivating example Consider the model: This is first case where we want to control for a mediator Use “do” to hold things constant \\(CDE = P(Y = y |do(X = x), do(M = m)) - P(Y = y |do(X = x), do(M = m))\\) Natural direct effect is defined in temrs of counterfactuals: randomize gender, and ask them to apply to the department they would have prefered \\(NDE = P(Y_{M = m} = 1|do(X = 1)) - P(Y_{M = m} = 1|do(X = 0))\\) "]
]
